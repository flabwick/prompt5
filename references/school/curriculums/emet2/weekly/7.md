**Unit 1: Multiple Regression Model (OLS Basics and Interpretation)**

**Concepts and Ideas:**

* Extension from simple to multiple regression.
* Model form:
  $Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \dots + \beta_KX_{Ki} + u_i$
* Terminology: regressors = explanatory variables = independent variables = covariates.
* Interpretation of slope coefficients: partial effects, i.e., effect of one regressor holding all others constant.
* Residuals defined as $e_i = Y_i - \hat{Y}_i$
* Error term still captures unobserved influences not included in X.
* Coefficients interpreted in the context of ceteris paribus logic.
* Sample data matrix dimensions change: X matrix has n rows and K+1 columns (including intercept).
* Residual definition unchanged, but now based on more regressors.
* Holding other regressors constant now essential for interpretation.
* Increased regressors reduce the unexplained variance (error term becomes “smaller” in role).

**Methods Required:**

* Specification of model in Python using `ols()` with multiple RHS variables.
* Interpretation of output from `summary()` of regression.
* Interpret coefficient signs and magnitudes with “holding other variables constant” qualifier.
* Residual calculation and meaning.
* Use of `.fit(cov_type='HC1')` in Python to obtain heteroskedasticity-robust standard errors.

**Off-Syllabus or Non-Required Methods:**

* No requirement to manually solve normal equations in full generality.
* Use of linear algebra for solving OLS not expected (discussed, but not taught).
* No derivation of standard error formulae required.
* Geometry of minimization not expected to be visualized in higher dimensions (not examinable).

**Visuals/Equations:**

* Diagram of multiple regression model structure.
* Residual graph: actual Y vs predicted Y.
* Equations:

  * Full model with K regressors.
  * Residual: $e_i = Y_i - \hat{Y}_i$
  * Prediction: $\hat{Y}_i = \hat{\beta}_0 + \sum_{j=1}^{K}\hat{\beta}_j X_{ji}$

---

**Unit 2: Assumptions of the Multiple Regression Model**

**Concepts and Ideas:**

* Assumption 1: Conditional Mean Independence (CMI):
  $E[u_i | X_{1i}, X_{2i}, \dots, X_{Ki}] = 0$
* Assumption 2: Random sampling from population.
* Assumption 3: No outliers (especially in regressors and dependent variable).
* Assumption 4: Heteroskedasticity allowed; variance of errors may depend on X. Robust standard errors used.
* Non-assumption: No homoskedasticity assumption.
* Optional assumption discussed but de-emphasized: No perfect multicollinearity.

**Methods Required:**

* Visual inspection for outliers (e.g., via histograms).
* Understanding implications of CMI—regressors contain no information about the mean of the error term.
* Justify assumptions qualitatively using empirical context (e.g., random sampling process).
* Use of heteroskedasticity-robust standard errors (`cov_type='HC1'`) in Python.

**Off-Syllabus or Non-Required Methods:**

* No derivation or use of matrix notation for CMI or variance expressions.
* Perfect multicollinearity condition acknowledged but not tested formally.

**Visuals/Equations:**

* Equation of CMI assumption.
* Histogram to identify outliers.
* Conceptual graph illustrating data with and without outliers affecting regression line.

---

**Unit 3: Interpretation of OLS Estimates & Change from Simple to Multiple Models**

**Concepts and Ideas:**

* Why OLS coefficient estimates change with added regressors: due to correlation between regressors.
* Interpretation of coefficient in simple regression is a mix of direct and indirect effects.
* In multiple regression, interpretation isolates direct effect.
* Holding other regressors constant isolates partial effect.
* Example: STR coefficient changes from -2.28 (simple) to -0.29 (multiple).
* Multiple regression reduces omitted variable bias (conceptually introduced).
* Expenditure per student and % English learners as additional regressors.

**Methods Required:**

* Calculate and interpret changes in coefficients between simple and multiple models.
* Interpretation: “If X increases by 1 unit, Y changes by β units, holding all other regressors constant.”
* Read and interpret T-statistics, p-values, and confidence intervals for each coefficient.
* Use `.summary()` output to perform this interpretation.
* Recognize and explain the reduction in omitted variable bias from richer models.

**Off-Syllabus or Non-Required Methods:**

* No testing or adjustment for endogeneity yet.
* No requirement to manually compute OLS estimates in general multiple regression models.

**Visuals/Equations:**

* Comparison table of regression outputs (simple vs multiple).
* Equations:

  * $\hat{\beta}_1^{(simple)} \neq \hat{\beta}_1^{(multiple)}$ if X₁ correlated with X₂.
  * Full multiple regression model.
* Diagram: Venn diagram or graph to conceptually show overlap of variable effects.

---

**Unit 4: Joint Hypothesis Testing (F-Test)**

**Concepts and Ideas:**

* Testing joint nulls: $H_0: \beta_1 = \beta_3 = 0$
* F-statistic: combines individual T statistics and adjusts for correlation between estimators.
* ## Formula:

  $$
  F = \frac{1}{q} \cdot (T_1^2 + T_3^2 - 2\rho_{13}T_1T_3) / (1 - \rho_{13}^2)
  $$
* Importance of correlation between regressors when doing joint tests.
* Individual tests vs joint test: joint test accounts for covariance between estimators.
* Overall model test: F-test for $H_0: \beta_1 = \beta_2 = \beta_3 = 0$
* Rejection of null means at least one regressor contributes explanatory power.

**Methods Required:**

* Use `reg.f_test()` in Python to run joint hypothesis tests.
* Interpret p-values from F-test: reject if < 0.05.
* Identify which coefficients are being tested jointly.
* Recognize relationship between model fit and joint hypothesis testing.

**Off-Syllabus or Non-Required Methods:**

* No need to compute F-statistic manually.
* No derivation of F-distribution critical values.

**Visuals/Equations:**

* Display of regression table with p-values for F-tests.
* Annotated regression output showing which variables are included in joint tests.
* Equation for F-statistic in two-variable case (T1 and T3) with correlation.

---

**Unit 5: Algebraic Derivation of OLS Estimates and Covariance Implications**

**Concepts and Ideas:**

* Derivation of OLS estimates for models with no intercept and two regressors.
* Objective function:

  $$
  \text{SSR}(b_1, b_2) = \sum (y_i - b_1x_{1i} - b_2x_{2i})^2
  $$
* First-order conditions:

  $$
  \sum x_{1i}(y_i - b_1x_{1i} - b_2x_{2i}) = 0  
  \quad \text{and} \quad
  \sum x_{2i}(y_i - b_1x_{1i} - b_2x_{2i}) = 0
  $$
* Solve for $\hat{\beta}_1$ in terms of data and $\hat{\beta}_2$, then substitute to fully solve.
* Special case: zero covariance between regressors ($\sum x_{1i}x_{2i} = 0$) ⇒ simple regression estimate.
* Case with intercept:

  $$
  \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}_1 - \hat{\beta}_2 \bar{x}_2
  $$
* Sample covariance drives difference between simple and multiple regression results.
* Final formula:

  $$
  \hat{\beta}_1 = \frac{\sum (x_{1i} - \bar{x}_1)(y_i - \bar{y})}{\sum (x_{1i} - \bar{x}_1)^2}
  \quad \text{if } \text{Cov}(x_1, x_2) = 0
  $$

**Methods Required:**

* Perform partial differentiation of SSR with respect to each coefficient.
* Solve system of equations for OLS estimates.
* Algebraic substitution and simplification.
* Interpretation of final formula in context of independence of regressors.

**Off-Syllabus or Non-Required Methods:**

* Linear algebra matrix solution methods.
* Solving high-dimensional models algebraically (restricted to 2-regressor models).

**Visuals/Equations:**

* Step-by-step breakdown of solving:

  * Partial derivatives of SSR.
  * Substitution of estimates.
  * Final expression for $\hat{\beta}_1$.
* Equation annotations showing where covariance cancels terms.
* Notation of inner products: $\sum x_i y_i$, $\sum x_i^2$, etc.
