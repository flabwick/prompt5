**Unit 1: Introduction to Econometrics and Course Framework**

**Concepts and Ideas:**

* Definition of econometrics: Application of economic theory and statistical techniques to analyze economic data.
* Interdisciplinary nature: Combines economics, mathematics, statistics, and coding.
* Focus on intuition and conceptual understanding over mathematics.
* Emphasis on introductory level content.
* Problem-solving and workshop integration within lectures.
* Applied econometrics using coding (Python).
* Econometrics as a tool for economic interpretation.

**Methods Expected:**

* Use of Python and Jupyter Notebooks for applied data analysis.
* Analysis of datasets in weekly computer labs.
* Basic understanding of economic context in data analysis.
* Attendance and engagement with workshops and lectures.

**Non-required/Exceptions:**

* Deep mathematical derivation not required in lectures (outsourced to workshops).
* Prior Python knowledge not required; instruction provided.
* No requirement for prior econometrics experience.

**Relevant Visuals:**

* Venn diagram showing intersection of economics, statistics, coding, and mathematics.
* Sample econometrics pipeline (economic question → model → data → estimation → interpretation).
* Annotated screenshots of a Jupyter Notebook.

---

**Unit 2: Random Variables, Sample Space, and Probability Distributions**

**Concepts and Ideas:**

* Outcomes: Mutually exclusive potential results of a random process.
* Sample space: Set of all possible outcomes.
* Events: Subsets of the sample space.
* Random variables: Numerical representation of outcomes.
* Discrete vs. continuous random variables.
* Probability distribution function (PDF) for discrete variables.
* Cumulative distribution function (CDF).

**Methods Expected:**

* Identification of sample space and events.
* Assignment of probabilities to discrete outcomes.
* Calculation of probabilities and cumulative probabilities.
* Use of notation: P(Y = y), Y for random variable, y for realization.
* Construction of probability distribution tables.
* Interpretation of histogram and CDF diagrams.

**Non-required/Exceptions:**

* No requirement to handle continuous distributions or integrate PDFs.
* No requirement to formally derive distributions from theoretical processes.

**Relevant Visuals:**

* Probability distribution table: y = {18, 19, ..., 26}, P(Y = y).
* Histogram: X-axis = age values, Y-axis = probabilities.
* Step function graph for CDF.
* Visuals showing event subsets from a sample space diagram.

---

**Unit 3: Expected Value, Moments, and Variance**

**Concepts and Ideas:**

* Expected value (E\[Y]) as a population mean.
* Not equal to the sample average (conceptual distinction).
* First four moments:

  * First moment: Expected value (mean).
  * Second moment: E\[Y²], linked to variance.
  * Third moment: Skewness.
  * Fourth moment: Kurtosis (tail heaviness).
* Variance (σ²): E\[(Y - μ)²] or Var(Y).
* Standard deviation: Square root of variance.
* Properties of expected value:

  * Linearity: E\[cY + d] = cE\[Y] + d.
  * E\[c] = c.
  * E\[Y1 + Y2] = E\[Y1] + E\[Y2].

**Methods Expected:**

* Calculation of E\[Y] using summation.
* Calculation of Var(Y) using summation.
* Application of linearity rules of expectation.
* Understanding variance behavior under transformations:

  * Var(c) = 0
  * Var(c + Y) = Var(Y)
  * Var(cY) = c²Var(Y)
* Differentiation between Var(Y1 + Y2) and Var(Y1 - Y2) formulas:

  * Includes covariance terms.

**Non-required/Exceptions:**

* No requirement to derive properties from first principles unless stated.
* No need to calculate third and fourth moments unless prompted.
* Continuous distribution integration not required.

**Relevant Visuals:**

* Table showing y, P(Y = y), y*P(Y = y), (y - μ)²*P(Y = y).
* Comparison chart: population mean vs. sample average.
* Distribution diagram showing variance (spread).
* Bar chart comparing distributions with same mean but different variances.
* Conceptual diagram showing skewness and kurtosis.

---

**Unit 4: Populations, Samples, and Sample Statistics**

**Concepts and Ideas:**

* Population: Full group of interest (e.g., all kangaroos in Tidbinbilla).
* Population size: Capital N.
* Sample: Random subset of population, size little n.
* IID sampling: Independently and identically distributed observations.
* Sample mean (Ȳ): (1/n) Σ Yi.
* Sample variance (s²): Not yet covered but contextually implied.
* Key distinction: Population values are fixed; sample statistics are random variables.
* Realizations (yi) vs. random variables (Yi).
* Sampling variability: Sample statistics vary with sample draw.

**Methods Expected:**

* Calculation of sample mean from observed data.
* Understanding that sample mean is a random variable.
* Contrast of repeated sample means across samples (illustrated with kangaroo weights).
* Acknowledgement of randomness in single-sample statistics.

**Non-required/Exceptions:**

* No requirement to determine population parameters directly from raw population data.
* Population enumeration techniques not required.

**Relevant Visuals:**

* Diagram showing population of N units and sampled subset of n units.
* Histogram of multiple sample means from repeated sampling.
* Visual showing sample mean as one outcome of a distribution of means.
* Tabulated weight values for multiple samples.

---

**Unit 5: Properties of the Sample Mean as an Estimator**

**Concepts and Ideas:**

* Sample mean as an estimator for population mean.
* Sample mean is a random variable: has its own expectation and variance.
* Expectation of sample mean: E\[Ȳ] = μ (unbiasedness).
* Variance of sample mean: Var(Ȳ) = σ²/n (precision increases with n).
* Standard deviation of sample mean (standard error): √(σ²/n).
* Implications:

  * Averaging reduces variance.
  * Larger samples yield more reliable estimates.

**Methods Expected:**

* Derivation of E\[Ȳ] from E\[(1/n)ΣYi] using linearity of expectation.
* Derivation of Var(Ȳ) from Var\[(1/n)ΣYi] assuming IID.
* Application of these to practical estimation contexts (e.g., kangaroo weights).
* Understanding implications for precision, estimator quality, and sampling strategy.

**Non-required/Exceptions:**

* No calculation of standard error unless σ² is given or assumed.
* No requirement to conduct simulations or bootstrap variance.
* No derivation using covariance if Yi are IID (Cov(Yi, Yj) = 0).

**Relevant Visuals:**

* Graph comparing distribution of population vs. sampling distribution of Ȳ.
* Histogram of sample means with center at μ and width decreasing with n.
* Formula panel summarizing:

  * E\[Ȳ] = μ
  * Var(Ȳ) = σ²/n
  * SD(Ȳ) = √(σ²/n)
* Illustration of the Law of Large Numbers in visual form.

