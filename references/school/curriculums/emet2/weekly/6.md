**UNIT 1: Regression Fit and Measures of Fit**

**Concepts and Ideas:**

* Simple linear regression model: single regressor, interpretation of slope and intercept
* Total Sum of Squares (TSS): $\sum_i (Y_i - \bar{Y})^2$
* Explained Sum of Squares (ESS): $\sum_i (\hat{Y}_i - \bar{Y})^2$
* Residual Sum of Squares (RSS): $\sum_i (Y_i - \hat{Y}_i)^2$
* R-squared: $R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$
* Standard Error of the Regression (SER): $\sqrt{\frac{1}{n - 2} \sum \hat{u}_i^2}$

**Methods Required:**

* Algebraic decomposition: show $TSS = ESS + RSS$
* Interpret R-squared in context: 0 = poor fit, 1 = perfect fit
* Recognize relationship in simple regression: $R^2 = r_{XY}^2$ (only in simple regression)
* Compare regression models using R-squared
* Calculate standard error of regression using residuals
* Know that Python computes R-squared and SER automatically

**Visuals:**

* Side-by-side scatter plots: good fit (low residuals) vs bad fit (high residuals)
* Graphical representation of vertical residuals and regression line
* Decomposition of variance diagram (TSS = ESS + RSS)

**Non-required Methods / Off-Syllabus:**

* Adjusted R-squared (only discussed later in multiple regression)
* Alternative goodness-of-fit metrics (e.g., AIC, BIC)

---

**UNIT 2: Binary Regressors and Group Differences**

**Concepts and Ideas:**

* Dummy (binary) variables: coded as 0 or 1
* Regression with binary X: interpretation changes from slope to group difference
* Interpretation of intercept: expected value for group where X=0
* Interpretation of slope: difference in means between X=1 and X=0 group
* Example: Smoking (1 = smoker, 0 = non-smoker) and birth weight

**Methods Required:**

* Derive population regression functions for X = 0 and X = 1
* Show that $\hat{\beta}_1 = \bar{Y}_1 - \bar{Y}_0$
* Use T-test for group mean difference (Python method shown)
* Estimate and interpret coefficients in regression with a dummy independent variable
* Link regression estimates to group means directly
* Build confidence intervals and test significance of group difference
* Understand benchmark group = group where dummy = 0

**Visuals:**

* Parallel lines plot: two group means, group difference = slope
* Regression output table from Python, showing intercept = non-smokers' mean, slope = difference

**Non-required Methods / Off-Syllabus:**

* Dummy variable traps (in multiple regression, not here)
* Interactions with binary variables
* Logistic regression (not discussed)

---

**UNIT 3: Properties of the OLS Estimator and the Gauss-Markov Theorem**

**Concepts and Ideas:**

* Definition of BLUE: Best Linear Unbiased Estimator
* Class of linear estimators: $\hat{\beta}_1 = \sum_i a_i Y_i$
* Unbiasedness: $E[\hat{\beta}] = \beta$
* Minimum variance within unbiased estimators
* Conditions for OLS to be BLUE:

  1. Linearity
  2. Zero conditional mean of errors: $E[u_i | X_i] = 0$
  3. Homoskedasticity: $Var(u_i | X_i) = \sigma^2$
  4. No perfect collinearity

**Methods Required:**

* Understand structure of OLS as linear in Y
* Apply definition of linear unbiased estimators
* Know conditions under which Gauss-Markov theorem holds
* Show that sample mean is BLUE in univariate case
* Interpret efficiency in context of minimum variance among unbiased estimators

**Visuals:**

* Graphical representation of residuals and line of best fit
* Weighting interpretation of OLS as weighted sum of Y\_i
* Textbook Gauss-Markov theorem box (if applicable)

**Non-required Methods / Off-Syllabus:**

* BLUE under heteroskedasticity (discussed but result not proved)
* Nonlinear estimators
* Generalized least squares (not included)

---

**UNIT 4: Heteroskedasticity and Robust Standard Errors**

**Concepts and Ideas:**

* Homoskedasticity vs Heteroskedasticity: constant vs varying error variance
* Visual identification of heteroskedasticity: fan-shaped residuals
* Heteroskedasticity-robust standard errors (HC1)
* Asymptotic variance of OLS under heteroskedasticity
* Implication for inference: standard errors, t-tests, confidence intervals change
* Python implementation: `cov_type='HC1'` in `.fit()` method

**Methods Required:**

* Use robust SEs in practice (always use unless given clear evidence of homoskedasticity)
* Replace variance of error term with sample analogs involving residuals
* Compute robust standard error manually:

  $$
  SE_{robust} = \sqrt{ \frac{1}{n} \cdot \frac{1}{S_{xx}^2} \cdot \sum (x_i - \bar{x})^2 \hat{u}_i^2 }
  $$
* Calculate confidence intervals using robust SEs
* Interpret differences between regular and robust regression output

**Visuals:**

* Four scatterplots showing homoskedasticity, increasing variance, decreasing variance, and varying variance
* Output from Python regression: compare robust vs non-robust SEs
* Regression line with spread of residuals changing along X

**Non-required Methods / Off-Syllabus:**

* White’s test, Breusch–Pagan test (not used here)
* Advanced HC2–HC4 standard errors (mentioned but not required)
* Manual proof of asymptotic variance under heteroskedasticity

---

**UNIT 5: Transformation of Variables and Functional Invariance**

**Concepts and Ideas:**

* Transformations of dependent and independent variables
* Model 2: Multiplying Y by A and X by B

  * $\tilde{Y}_i = A Y_i, \tilde{X}_i = B X_i$
* Model 3: Adding constants A and B

  * $\tilde{Y}_i = Y_i + A, \tilde{X}_i = X_i + B$
* Impact on slope and intercept:

  * Scaling affects both slope and intercept: $\hat{\gamma}_1 = \frac{A}{B} \hat{\beta}_1$, $\hat{\gamma}_0 = A \hat{\beta}_0$
  * Shifting affects only intercept: $\hat{\delta}_1 = \hat{\beta}_1$, $\hat{\delta}_0 = \hat{\beta}_0 + A - \hat{\beta}_1 B$

**Methods Required:**

* Derive transformation effects algebraically
* Apply substitution to derive transformed slope and intercept
* Use definitions:

  * $\hat{\beta}_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}$
  * $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$
* Compare models with transformed data using regression equivalences

**Visuals:**

* Side-by-side scatterplots:

  * Original vs scaled data: larger dispersion but same linear trend
  * Original vs shifted data: identical pattern but moved location
* Regression line demonstrating intercept shift but constant slope

**Non-required Methods / Off-Syllabus:**

* Logarithmic transformations (not discussed here)
* Polynomial or interaction terms (not discussed)

---

**Relevant Equations (Global Across Units):**

1. $\hat{\beta}_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}$
2. $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$
3. $TSS = ESS + RSS$
4. $R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$
5. $SER = \sqrt{ \frac{1}{n - 2} \sum \hat{u}_i^2 }$
6. $Var(\hat{\beta}_1) = \frac{\sigma^2}{\sum (X_i - \bar{X})^2}$
7. $Robust \ SE(\hat{\beta}_1) = \sqrt{ \frac{1}{n} \cdot \frac{1}{S_{xx}^2} \cdot \sum (x_i - \bar{x})^2 \hat{u}_i^2 }$

**Required Python Modifications:**

* For robust SEs:

  ```python
  model = sm.OLS(Y, X)
  results = model.fit(cov_type='HC1')
  ```
