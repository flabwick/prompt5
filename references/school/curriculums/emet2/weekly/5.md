**Unit 1: OLS Estimator – Assumptions and Statistical Properties**

**Concepts and Ideas:**

* Ordinary Least Squares (OLS) estimators: β̂₀ (intercept), β̂₁ (slope)
* Properties of estimators: expected value, variance, sampling distribution
* Random sampling and its role in estimator variability
* Conditional expectation and variance
* Sampling distribution of β̂₁
* Definition and impact of the nuisance term (∑(Xᵢ−X̄)(Uᵢ−Ū)/∑(Xᵢ−X̄)²)
* Sample covariance and sample variance
* Consistency and unbiasedness of β̂₁
* Conditions for unbiasedness: E\[Uᵢ | Xᵢ] = 0
* Importance of linearity in parameters for OLS
* Difference between errors (Uᵢ) and residuals (Ûᵢ)

**Methods Expected:**

* Derive E\[β̂₁ | X] by manipulating sums and expectations
* Apply linearity of expectation
* Use assumptions to simplify expressions (e.g., E\[Uᵢ | Xᵢ] = 0)
* Justify unbiasedness using OLS assumption 1
* Translate mathematical notation into intuitive econometric interpretation

**Relevant Equations:**

* β̂₁ = ∑(Xᵢ−X̄)(Yᵢ−Ȳ)/∑(Xᵢ−X̄)²
* E\[β̂₁ | X] = β₁ → unbiasedness
* Var(β̂₁ | X) = σ² / ∑(Xᵢ−X̄)²
* Approximate variance: Var(β̂₁) ≈ σ² / (n × Var(X))
* Sampling distribution: β̂₁ \~ N(β₁, σ² / (n × Var(X)))

**Visuals:**

* Scatter plots with regression lines
* Visual comparison of different PRFs (true regression lines) with same data points
* Diagrams showing variance around regression line
* Histogram for detecting outliers

**Non-Required Methods / Exceptions:**

* No derivation of β̂₀’s variance (optional, more complex)
* No requirement for proving consistency, only understanding unbiasedness
* Assumption 4 (homoskedasticity) treated as optional and non-critical for bias

---

**Unit 2: The Four OLS Assumptions**

**Concepts and Ideas:**

* Assumption 1: Conditional Mean Independence (E\[Uᵢ | Xᵢ] = 0)

  * Implies regressors provide no information about error term
  * Violation implies omitted variable bias
* Assumption 2: IID Sampling (Xᵢ, Yᵢ) from joint distribution

  * Required for valid expectation and variance operations
* Assumption 3: No large outliers / finite fourth moments

  * Practical implications: visually inspect histograms/scatter plots
* Assumption 4: Homoskedasticity (Var(Uᵢ | Xᵢ) = σ²)

  * Simplifies variance formula for β̂
  * Dropped in later lectures (heteroskedasticity addressed later)

**Methods Expected:**

* Identify when an assumption is invoked in derivations
* Link each step of estimator derivation to relevant assumption
* Interpret assumption implications via data examples (e.g., test scores vs. class size)

**Visuals:**

* Histograms to assess outliers and variance
* Scatter plots to detect changing variance (heteroskedasticity)
* Conceptual visuals showing conditional mean vs. conditional variance

**Relevant Equations:**

* E\[Uᵢ | Xᵢ] = 0
* Var(Uᵢ | Xᵢ) = σ²
* Fourth moment condition: E\[Xᵢ⁴] < ∞, E\[Yᵢ⁴] < ∞

**Non-Required Methods / Exceptions:**

* No need to verify assumption 4 analytically
* No obligation to handle heteroskedasticity in-depth (delayed to later weeks)

---

**Unit 3: Confidence Intervals and Hypothesis Testing for OLS Estimates**

**Concepts and Ideas:**

* Confidence interval (CI) for β̂₁: estimator ± 1.96 × standard error
* Standard error as estimate of standard deviation: SE(β̂₁) = sᵤ / sqrt(n × sₓ²)
* Hypothesis testing for β₁ = 0

  * Null hypothesis: H₀: β₁ = 0
  * Use confidence intervals, p-values, or t-statistic/Z-statistic
* Standard normal approximation under CLT
* Interpretation of CI: range of plausible values for β₁

**Methods Expected:**

* Construct confidence intervals using sample SEs
* Calculate t-statistic: β̂₁ / SE(β̂₁)
* Evaluate statistical significance using:

  * Whether CI contains 0
  * Whether p-value < 0.05
  * Whether |t| > 1.96
* Interpret statistical results in context (e.g., class size affects test scores)

**Relevant Equations:**

* CI = β̂₁ ± 1.96 × SE(β̂₁)
* SE(β̂₁) = sᵤ / sqrt(n × sₓ²)
* t = β̂₁ / SE(β̂₁)
* P-value: P(|Z| > |t|)

**Visuals:**

* Confidence interval diagram on normal distribution curve
* t-distribution curve showing rejection region
* Scatter plot with regression line and CI shading

**Non-Required Methods / Exceptions:**

* No need to use t-distribution critical values (default to standard normal)
* No manual derivation of SE formula (given directly)
* Python’s naming convention (calls t “Z”) does not affect calculations

---

**Unit 4: Estimator for β̂₀ (Intercept) and Its Bias**

**Concepts and Ideas:**

* Estimator formula: β̂₀ = Ȳ − β̂₁ X̄
* Bias analysis: E\[β̂₀ | X] = β₀ + μᵤ

  * Bias depends on mean of the error term (μᵤ)
* When μᵤ = 0, β̂₀ is unbiased
* Without loss of generality, assume E\[Uᵢ] = 0 to simplify
* Multiple PRFs can explain same data: vertical shifts reflect different β₀ values
* Tradeoff between error mean and intercept

**Methods Expected:**

* Derive E\[β̂₀ | X] from definition
* Use assumption 1 to argue E\[Uᵢ | Xᵢ] = μᵤ
* Recognize impact of μᵤ on β̂₀ bias
* Graphical interpretation: visualising PPF shift up/down

**Relevant Equations:**

* β̂₀ = Ȳ − β̂₁ X̄
* E\[β̂₀ | X] = β₀ + μᵤ
* If E\[Uᵢ] = 0 → unbiased

**Visuals:**

* Two regression lines with identical slope but different intercepts
* Residual plots with shifted intercept
* Diagrams illustrating offset between β̂₀ and true β₀

**Non-Required Methods / Exceptions:**

* No need to estimate μᵤ
* β̂₀ not a primary target for inference (focus is β̂₁)
* Bias in β̂₀ does not affect slope estimate or main conclusions

---

**Unit 5: Applied Output Interpretation in Python (Regression Summary)**

**Concepts and Ideas:**

* Python regression output: coefficients, standard errors, confidence intervals, t-statistics, p-values
* Interpreting slope coefficient (β̂₁): e.g., effect of STR on test scores
* Link between output and manual calculations
* Confidence intervals from output → test if zero is in interval
* Standard error derived from residual and regressor variance
* Relationship: coefficient / SE = t-statistic
* p-value interpretation: probability of observing t-statistic under null

**Methods Expected:**

* Identify key regression statistics in Python output
* Use standard error to compute CI
* Compare t-statistic to 1.96
* Interpret p-values < 0.05 as rejecting null
* Translate Python’s “Z” to t-statistic understanding

**Relevant Equations:**

* β̂₁ / SE(β̂₁) = t (reported as Z in Python)
* CI = \[β̂₁ − 1.96 × SE, β̂₁ + 1.96 × SE]
* P-value associated with t-statistic under N(0,1)

**Visuals:**

* Annotated Python output table (coefficients, SEs, t-stats, CIs)
* Confidence interval lines overlaid on scatter plot

**Non-Required Methods / Exceptions:**

* No coding required in exams
* No need to manipulate raw Python output – only interpret
* Z vs. t distinction ignored due to large-sample normality assumption

---
