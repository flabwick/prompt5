**UNIT 1: Foundations of Simple Linear Regression**

**Concepts and Ideas:**

* Population Regression Function (PRF): $Y_i = \beta_0 + \beta_1 X_i + u_i$
* Estimand interpretation:

  * $Y$: Dependent variable (outcome)
  * $X$: Independent variable (predictor, regressor, explanatory variable)
  * $u$: Error term, capturing unobserved influences
* Causal interpretation of $\beta_1$: conditional, with heavy caveats
* Objective: Estimate $\beta_0$ (intercept), $\beta_1$ (slope)
* Difference between population parameters and estimates: $\beta$ vs. $\hat{\beta}$
* Residual vs. Error:

  * Residual: $\hat{u}_i = Y_i - \hat{Y}_i$
  * Error: $u_i = Y_i - Y_i^{PRF}$
* Predictions: $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$

**Required Methods:**

* Constructing the PRF
* Defining and computing residuals and errors
* Interpreting intercepts and slopes within data domain
* Making in-sample predictions from the estimated model
* Estimating expected changes in $Y$ for unit changes in $X$

**Visuals and Graphs:**

* Scatter plot with:

  * True population regression line (solid)
  * Estimated regression line (dashed)
  * Errors = vertical distance from data points to population line
  * Residuals = vertical distance from data points to estimated line
* Diagram: out-of-sample extrapolation shown by linear extension of estimated line
* Plot of sample data showing minimum and maximum $X$-values (important to discuss extrapolation)

**Non-required Topics:**

* No full causal identification framework
* No treatment of omitted variable bias yet
* No time series extrapolation (mentioned only as future topic)
* No instrumental variable or non-linear models

---

**UNIT 2: Ordinary Least Squares (OLS) Estimation Process**

**Concepts and Ideas:**

* OLS criterion: minimize the sum of squared residuals

  $$
  \min_{b_0, b_1} \sum_{i=1}^n (Y_i - b_0 - b_1 X_i)^2
  $$
* Residuals: $\hat{u}_i = Y_i - \hat{Y}_i$
* First-order conditions:

  * Derivative w\.r.t. $b_0$: $\sum (Y_i - b_0 - b_1 X_i) = 0$
  * Derivative w\.r.t. $b_1$: $\sum X_i (Y_i - b_0 - b_1 X_i) = 0$
* Derivation of OLS estimates:

  * $\hat{\beta}_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}$
  * $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$
* Sample covariance and variance definitions:

  * Covariance $\operatorname{Cov}(X, Y) = \frac{1}{n} \sum (X_i - \bar{X})(Y_i - \bar{Y})$
  * Variance $\operatorname{Var}(X) = \frac{1}{n} \sum (X_i - \bar{X})^2$

**Required Methods:**

* Derive and solve the OLS first-order conditions using basic calculus
* Apply summation identities: e.g., $\sum X_i = n \bar{X}$
* Justify each algebraic step in manipulating summations
* Use summation notation correctly and handle constants inside/outside sums
* Recognize derivations are tractable due to differentiability of squared loss

**Visuals and Graphs:**

* 3D paraboloid surface graph for loss function $L(b_0, b_1)$, showing a unique minimum
* Geometry of OLS: valley-shaped contour with global minimum
* Graphical representation of parameter space and gradient descent

**Non-required Topics:**

* No numerical optimisation (e.g., gradient descent) required
* No need to handle non-differentiable estimators like LAD
* No multiple regression visualisation yet
* No matrix algebra required (only scalar calculus and summation)

---

**UNIT 3: Properties and Interpretation of the OLS Estimators**

**Concepts and Ideas:**

* Linearity: estimator is a linear function of $Y_i$
* Unbiasedness of estimators (not formally proved yet)
* Interpretation:

  * $\hat{\beta}_1$: change in predicted $Y$ for 1-unit increase in $X$
  * $\hat{\beta}_0$: predicted $Y$ when $X = 0$, often uninterpretable
* Sample prediction function:

  $$
  \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i
  $$
* Plug-in predictions and extrapolation

**Required Methods:**

* Construct point predictions using OLS coefficients
* Interpret slope and intercept from output
* Check whether $X = 0$ lies in the data range before interpreting intercept
* Determine appropriateness of extrapolation beyond observed data

**Visuals and Graphs:**

* Scatter plot showing regression line and intercept location
* Graph showing class sizes and predicted test scores
* Arrows illustrating vertical distance between predicted and actual $Y$ values

**Non-required Topics:**

* No bias-variance decomposition yet
* No formal inference or standard error calculation in this unit
* No heteroskedasticity or diagnostics
* No treatment of non-linear transformations

---

**UNIT 4: Algebraic Properties of Residuals**

**Concepts and Ideas:**

* Key OLS residual identities:

  * $\sum \hat{u}_i = 0$
  * $\sum \hat{u}_i X_i = 0$
  * $\sum \hat{Y}_i = \sum Y_i = n \bar{Y}$
* Orthogonality: residuals are orthogonal to regressors
* Implications of residual properties for model fitting
* Residuals can reconstruct actual values: $Y_i = \hat{Y}_i + \hat{u}_i$

**Required Methods:**

* Algebraically prove:

  * $\sum (Y_i - \hat{Y}_i) = 0$
  * $\sum (Y_i - \hat{Y}_i)(X_i) = 0$
* Use known identities:

  * $\sum (X_i - \bar{X}) = 0$
  * $\sum (X_i - \bar{X})(Y_i - \bar{Y})$ structure for covariance
* Multiply and manipulate sums to derive results (especially using substitution from estimated coefficients)

**Visuals and Graphs:**

* Diagram showing residuals above and below regression line
* Overlay of residual arrows on scatter plot with estimated regression line
* Visual explanation of why the line passes through the centroid ($\bar{X}, \bar{Y}$)

**Non-required Topics:**

* No formal variance of residuals or R-squared covered yet
* No residual plots required
* No residual standard error computation
* No diagnostic testing (e.g., Durbin-Watson, Breusch-Pagan)

---

**UNIT 5: Practical Computation and Implementation of OLS**

**Concepts and Ideas:**

* Data inputs: tabular data with $X_i, Y_i$
* Computation flow:

  1. Compute $\bar{X}$, $\bar{Y}$
  2. Compute $\hat{\beta}_1$ using covariance/variance
  3. Compute $\hat{\beta}_0$
  4. Predict $\hat{Y}_i$
  5. Compute residuals $\hat{u}_i = Y_i - \hat{Y}_i$

**Required Methods:**

* Summing data values to get sample means
* Manual computation of:

  * $\sum X_i Y_i$, $\sum X_i$, $\sum Y_i$, $\sum X_i^2$
* Proving that:

  * $\sum X_i = n \bar{X}$
  * $\sum X_i Y_i - n \bar{X} \bar{Y} = \sum (X_i - \bar{X})(Y_i - \bar{Y})$
* Using algebraic manipulation for transformation and simplification

**Visuals and Graphs:**

* Tabular data (implied, not visualised)
* Step-by-step calculation of slope and intercept using a hypothetical dataset
* Application of derived equations to practical examples (e.g., test scores vs. class size)

**Non-required Topics:**

* No software code or implementation required
* No matrix formulation (no need to use $X'X$ etc.)
* No scripting or loop logic (even though programming is mentioned in passing)
* No visualisation libraries or plotting tools taught
