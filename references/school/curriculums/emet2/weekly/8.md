**Unit 1: Omitted Variable Bias (OVB)**
*Concepts and Ideas:*

* Definition and nature of omitted variable bias in simple vs multiple regression.
* The oracle model as the correct data-generating process: true model includes multiple regressors.
* Incorrect estimation when omitting relevant variables (e.g., omitting X₂ when it belongs in the model).
* Error term redefinition: W = β₂X₂ + u when omitting X₂.
* Relationship between regressors and the new composite error term W.
* Bias derivation using sample covariance and variance:

  * β̂₁ = β₁ + \[Cov(X₁, W) / Var(X₁)]
  * W = β₂X₂ + u
  * If Cov(X₁, X₂) ≠ 0 and β₂ ≠ 0 → bias exists.
* General formula for bias:

  * Bias(β̂₁) = β₂ × \[Cov(X₁, X₂) / Var(X₁)]
* Sign of the bias depends on sign(β₂) and sign(Cov(X₁, X₂)).
* The OLS estimator is unbiased if omitted regressor is uncorrelated with included regressors or has zero effect (β₂ = 0).
* Conceptual framing: β̂₁ captures both the direct effect of X₁ and the indirect effect via X₂.

*Methods Expected:*

* Algebraic manipulation of regression equations.
* Identification of included vs omitted variables.
* Understanding and interpreting expectations and covariances in regression contexts.
* Application of bias formula.

*Non-required/Off-Syllabus Methods:*

* No requirement to prove unbiasedness rigorously from first principles.
* No matrix algebra expected.
* No expectation to compute bias numerically from data.

*Appropriate Questions/Visuals:*

* Graphical representation of true vs misspecified model.
* Plot showing change in coefficient estimate with/without X₂.
* Visual of omitted variable bias using arrows to show direct and indirect paths.
* Equation manipulation on slides showing step-by-step derivation of bias.

---

**Unit 2: Causal Inference & Identification in Regression**
*Concepts and Ideas:*

* Distinction between correlation and causation.
* Randomized Controlled Trials (RCTs) as the gold standard for causal inference.
* Problems with observational data: omitted variables, confounding.
* Use of regressions to approximate causal effects and when that is misleading.
* Ideal scenario: randomized assignment of treatment (e.g., instrument playing).
* Counterfactual reasoning: what would a treated group have experienced without treatment.
* Application: instrument playing on salary, education on wages.
* Treatment endogeneity: treatment (e.g., education) is a choice variable, not random.
* Importance of controlling for confounders like parental SES, aptitude, ability.
* Return to education regression and concerns over omitted ability bias.
* Conjectures about signs of bias (e.g., positive bias if ability is omitted and positively correlated with education and earnings).

*Methods Expected:*

* Formulating regression models with and without potential confounders.
* Interpreting bias direction based on sign of omitted variable's relationship with regressors and outcome.
* Understanding when and why causal claims are invalid.
* Identifying whether a regressor is exogenous or endogenous.

*Non-required/Off-Syllabus Methods:*

* No application of instrumental variables in practice (only mentioned).
* No structural equation modeling.
* No policy evaluation methods (e.g., DID, RDD).

*Appropriate Questions/Visuals:*

* Diagrams showing RCT vs observational design.
* Path diagrams for causal inference and bias from unobserved confounders.
* Regression tables with and without omitted variables to show coefficient sensitivity.
* Diagram of education choice model and ability affecting both education and earnings.

---

**Unit 3: Multiple Regression Mechanics and Interpretation**
*Concepts and Ideas:*

* Multiple regression allows for isolation of individual effects, holding others constant.
* Importance of including all relevant regressors to avoid bias.
* Inclusion of variables not necessarily for their own interest, but to reduce omitted variable bias.
* Interpretation of regression coefficients in multiple regression: ceteris paribus effect.
* Contrast with simple regression interpretation.
* Hierarchical inclusion of regressors and interpretation shifts.
* The impact of adding or omitting a regressor on coefficient estimates of other regressors.

*Methods Expected:*

* Running and interpreting multiple regression output.
* Comparing simple and multiple regression coefficients for same regressor.
* Interpreting coefficients in presence of collinearity.
* Evaluating whether adding regressors improves model validity.

*Non-required/Off-Syllabus Methods:*

* No requirement to perform stepwise regression.
* No penalized regression methods (e.g., LASSO, Ridge).
* No explicit collinearity diagnostics or VIF analysis.

*Appropriate Questions/Visuals:*

* Tables comparing regression output with different model specifications.
* Visual showing the change in slope estimate after adding controls.
* Graphical decomposition of variation in Y into parts explained by each regressor.

---

**Unit 4: The Frisch–Waugh–Lovell Theorem**
*Concepts and Ideas:*

* Three-step residual regression procedure:

  1. Regress X₁ on X₂,...,Xₖ, obtain residuals X₁\~
  2. Regress Y on X₂,...,Xₖ, obtain residuals Y\~
  3. Regress Y\~ on X₁\~ → β̂₁ = slope coefficient from multiple regression of Y on X₁,...,Xₖ
* The partial regression coefficient can be interpreted as the effect of X₁ on Y controlling for others.
* The interpretation of “holding other regressors constant” has a residual-based representation.
* Multiple regression isolates partial effects via residualization.
* Not an alternative method of estimation, but a theoretical justification of interpretation.
* Numeric equivalence of coefficient estimate from full OLS and from the residual regression.

*Methods Expected:*

* Understanding of residual generation via auxiliary regressions.
* Ability to follow conceptual derivation of equivalence.
* Apply partial regression interpretation in discussion.

*Non-required/Off-Syllabus Methods:*

* No derivation required.
* No expectation to perform this decomposition manually unless prompted.
* No formal proofs or matrix notation expected.

*Appropriate Questions/Visuals:*

* Flowchart of 3-step procedure.
* Residual plots showing Y\~ and X₁\~.
* Side-by-side comparison of β̂₁ from full model and residual regression.

---

**Unit 5: Measures of Fit in Multiple Regression**
*Concepts and Ideas:*

* Definition and role of R² and Adjusted R².
* R² = ESS / TSS = 1 − RSS / TSS.
* R² always increases or stays the same as more regressors are added.
* Adjusted R² introduces penalty for extra regressors:

  * Adj R² = 1 − \[(RSS / (n − k − 1)) / (TSS / (n − 1))]
* Adjusted R² may increase or decrease when adding a regressor.
* Purpose: avoid overfitting, adjust for degrees of freedom.
* Difference between explanatory power and predictive accuracy.
* Standard Error of the Regression (SER): square root of RSS/(n − k − 1).
* Interpretation of SER as average prediction error.

*Methods Expected:*

* Calculation and interpretation of R² and Adjusted R².
* Understanding implications of increasing model complexity.
* Differentiating between model fit and model validity.

*Non-required/Off-Syllabus Methods:*

* No AIC, BIC, or cross-validation required.
* No discussion of prediction intervals or forecast performance.

*Appropriate Questions/Visuals:*

* Plots of R² and Adjusted R² as regressors are added.
* Equations for both metrics.
* Visual comparison of predicted vs actual Y values.
* Table comparing multiple models with increasing complexity.
