SPEAKER 0
This material has been reproduced and communicated to you by or on behalf of the Australian National University, in accordance with Section 113% of the Copyright Act 1968. The material in this communication may be subject to copyright under the Act. Any further reproduction or communication of this material by you must be consistent with the provisions of the Act. Do not reproduce this material. Do not remove this notice.

SPEAKER 1
Yeah, that's why I'm My I was expecting to pay for myself. Yeah Yeah. Yeah Yeah right It's like. I think Council. That's very Yes. I. Yeah OK. I Yeah, the Yes. the Yeah Good afternoon. We 5 of this semester. The So Uh, can you guys hear me? Good. All right, so what are we doing? OK wait, before we start econometrics, there is the quiz. happening like we 5 qui You have that? Good. at another almost. 24 hours Because any any questions? This Housekeeping Oh yeah. How easy is the quiz compared to the last quiz? Is it like a? This quiz compared to the last one, yes. Who wants to both of them and then what would be like basically just insane. They are constructed in a, in a friendly way, and if you are, if you, if you attended the lectures or watch the videos, you should go with me. Mostly, that's the, that's the idea. Any other questions? OK, what are we doing? Oh yeah, we did the estimation last week, I told you, I don't know if you picked this up, I said we reached the low point of the lecture last week. Today is yet another low point. I didn't, I didn't say the low point is unique. You didn't hear me say that, but uh maybe after today it's really the low point. And Other such no point in in the subsequent pictures. We'll see. OK, but we'll, we'll get through it, we'll get through it. I think we can do it together. OK. So here, this topic sounds exciting. The 4 OLS assumptions. Let's go through that. Um, OS estimators, so those were our beta not hat and beta one hat. The estimators will intercept on the slope of the line that we're putting into the scatter plot. Uh, our statistics, so we view them now as um. Statistical or random variables. So Um, they vary with the underlying random sample. The analogy is to the sample average in the beginning of the. Went to collect 30 kangaroos. And average out their weight and if you do this another time, you get a different sample average. Each time you do this, you get a different estimate. Likewise, here, if I give you a different data set or I should say a different random draw, you will get different estimates for the intercept episode. So we want to study the statistical properties of these estimates. So the question that we answered today are, what's the expected value of the OS estimator. What's the variance and what's the sampling distribution? What's your best guess for what? Member Yeah Right. And for the expected value of the variants, we'll work that out. That's where we get to the no point, um, but it's actually know that's tricky. Um, but to work this out, we need to impose 4 assumptions. Let's have a look. The first assumption is called What the traditionally being independence CMI. The error term U is conditionally meaning independent of CMI. The regressor XI, meaning this in mathematics. So the, the meaningful equality here is the first one. This one here. This is the meaningful part of the definition of the assumption, the first equality, and then namely that states that the The expected value. is equal to the unconditional expected value in the middle here. Or like I said I think I'm saying it on the next slide, knowing the value of X is not informative. For the meaning of the error. And the second equality is just a new name. For the middle object, new sub you. So it's just the that's how that we attached to the mean. Of the character But the, the meaningful bit, the substantive bit here is really the first quality that says that. The conditional expectation is I The textbook just sets um. The two objects on the right hand side equal to 0. New is just another name for the thing in the middle, the unconditional expectation. I will just step back and put to 0. We'll see in the workshop bit of today, why this is a harmless assumption. The assumption of zero mean, zero unconditional mean is harmless. The more harmful assumption is that this one here, which is the, is the substance of conditional independence may be that. equal to the outcome That we will explore as we go along this semester. Check Different applications, whether that is actually satisfy. Um, what am I saying here? So the conditional mean independence assumption restricts the expected value of the error terms and We do know that To say differently, we do not observe the error terms. So although we don't observe the error terms, we're somehow willing to impose this restriction on the expected value, so we have to be a little bit mindful of that. We assuming something here on. random objects that we don't actually observe. And so we have to keep thinking about what that means in applications. We do applications. For now, we just go with it as a technical assumption, and I'll show you later where exactly we will need. assumption, and then even later, in the next few weeks, we'll play around with the conceptual meaning of that assumption. Um, yeah, the essential restriction is that the expected value of you is not a function of X. So when we, when we write this, and this is really the CMI condition here, we are saying that the conditional mean is not dependent on X. And instead it's just some fixed value new. And like I said, Or 0 That I don't think in the textbook you will find new just just shoot zero. All right. So that assumption says that X is not informative about the meaning of the error. And one way of thinking about this is to connect the two applications if um in a randomised controlled experiment, is randomly assigned to people. Then Did he make the case that the knowledge of access Unform took about why. What by living here and The example that we're using in the book. The effect of classroom size or student-teacher ratio on student outcomes. If you could randomly allocate students to different class sizes. And Independent variable X, then that is independent. Of the error term and then that sort of independence implies condition of being independence and then assumption one is not. So whenever you can convince yourself that your address of X is randomly assigned. Then you're safe got to assumption one. The problem is, in most of our applications, we don't view X as random sign. So there's a particular for this for this classroom example, we don't usually think that unless it is actually a randomised controlled experiment in real life, classroom size is not. So for example, more privileged or kids from more privileged backgrounds might have a possibility to select themselves into smaller classrooms because they live in fancier neighbourhoods where the schools are better story like that. That is an example of More sign where you know there could be actually a connection between X and the L2. We keep exploring the first. For the semester. Now we go with this. technical condition that we need. to establish good statistical properties for the OS estimator. So that was assumption one. Any questions on assumption? And I'll These assumptions as we go up, uh, point out again where we use them. Assumption 2 is a relatively easy one. The sample data that's given to us are independent and identically distributed. Draws from the joint population. That is basically an assumption. But we can, we will always assume that the data was Sample represent Randomly and have that assumption. Of course, many times you can come up with examples where you can't do random sampling, then that assumption is not satisfied and everything that we do are based on that assumption would be satisfied. But for our purposes. We hope to work with the random sample and. Any questions Right Repeat what I just said. Another assumption that looks technical, but it's relatively uh intuitive. Technical Technical Manifestation of the assumption is 4th moments are finite. What does that mean in words? Large outliers are rare. The 4th moment, if you remember this from first lecture, we did the first lecture. Represent how How much, how heavy or thick the tails are of the distribution, by which I mean. Came to a far away from the mean from the average events. Can they occur with a reason. High enough probability. If they can, then you're in trouble, but only really in so much as they are really large. We want data that doesn't have Out or doesn't have an outlier problem. This mathematical manifestation of this is. In these fancy, um, forgoing conditions that the. 4th of X is finite and the 4th moment of Y is finite. For practical purposes. The distributions that we are working with. that generate the data that is given to us satisfy this. To take the normal Um, you can sit down and step one level away for some uh uh graduate statistics. You can actually work this out analytically for many of the distributions that you have heard of by normal distribution of distribution, exposure distribution. So many names distributions to satisfy us. Uh For our purposes, it translates to something relatively relatable. What you do when you have a data set is you look. At the range of X and at the range of Y, which this picture here does, what we're doing here. Um, now this is not the test score data, it's just an illustration from the book. The illustration how one outlier and totally. By Your Regression line as estimate. If, if you take all the dots, including this outlier here, and you fit, you apply OS to it and you plot the line, you get the one that has this positive slope. If you take the outlier out, you get the the flat line. OK. And That shows you what. Outliers can do when relative When the relative proportion of outlayers in the sample is too large, which here is one outlier is in this case a lot because you don't have a lot of dots. If otherwise, you had 10,000 dots down here, then this outlier wouldn't hurt. So it always depends on How frequent outliers can be. In practical situations really boils down to you looking at histograms of your data. So you plot a histogram of X. And the histogram of Y and you look for the range. Of um I pay attention to the range of the data, right? And Then you can spot outliers. Quite easily. Oftentimes you take our test score example. Test score can almost by definition not have an outline. It got, it's a standardised test score, I think it was between 0 and 600. Outline would be 9 trillion. This could, this could happen if there's a data entry error. That you can detect that relatively easily. But only if you bother to look at your data also. Same with student teacher ratio that I think that varied between 12 and 25 or something. If there's a data entry error, the person who entered the data, if there was a person doing that entered the classroom size of 9000 or whatever, maybe that could spoil your estimates, but again, one large outlier might not not not even do that, OK? It depends on how many non-outliers you have, but um. I think it's a relatively relatable discussion that we're having here in in mathematics, it is expressed like this, OK. And all we mean is that we're working with a data set where outliers don't produce this. Where the line also turns up when it really should be flat. Where an outlier moves the line. So we, we go with that assumption, no outliers spoiling our estimates, OK. Any questions on that? And because of this assumption, it's always best practised before the binary version of looking at it. His two brothers. And it is Story of You can look at a scatter log to I say Instagram somewhere. Get a get a feeling for you. I know this is not a very scientific. definition, but for the data that we're dealing Rooting out the In real life, of course, you always What's surprising Estimates and the how. and to rule that out uh by looking for example at Instagrams or Sketch. The last assumption number 4, I call assumption 4 A. Next week or next week, next next. Weeks on. We're looking into assumption 4. Which is a tweak or twist on this one here. This one here you will not find in the textbook, um, and let's read through this. The error term is almost Gastic. That means it's conditional variance is not a function in X, it's just a number. So this looks similar to what assumption one. Well that's assumption one was about the conditional mean. Here we are about the conditional variants. We don't want the the spread of the error to change as exchanges. We will, we will give that assumption up next week or next week, and it's not an essential assumption. It just makes our results look a little bit cleaner, but we will move away from that assumption. The other three assumptions, 123, we will always keep. This one will soften. Um, but what this says is that the, um, well, the essential restriction here is that the variance is not a function in X, so we could say the conditional variance is equal to the unconditional variance in analogy to what that assumption one where the statement was the conditional mean is equal to the uncondition. Here we could say the conditionals is equal to the unconditioned. And just number sigma square. If would be that it's So like it's not. Yeah. like X The If it itself So that it, it's, it's not so they don't usually exclude each other. So you can have conditions being independence so for assumption one check no problems set aside, but that the variance. Changes So think of Think of the scatter plots, um. The sket Um, and, and you fit a line if the, if the scatter plots or if the spread of the dots change as X changes, and I, I'll give you a picture, maybe is it next week or week 7, I'll show you that, um, you can, you can easily easily have condition with independence, but the variance of the errors changes. Um, they just have nothing to do with each other in a way. What they have to do is that they look relatively, these two definitions, they look. Quite similar. But that's, that's all there is. It's just, is it the condition of me? That depends on X or is it the conditional variance that depends on X and All combinations can happen. What what we want, which, which is the more critical assumption is all as assumption of one, that's the one that allows us to. Uh interpret the word as estimator as a good quality estimator of beta one. The conditional of the homoplasticity assumption here is something that I'm just using as a device to keep today's results a little cleaner and then we soften that and obtain a result for the case of what's called heosity, which is basically non-homosticity, which is just a tweak on what whatever you write. All right, let's um Continue. So sampling distribution of the OS estimator. So I think we saw this before we after the expected value, the variance of the sampling distribution of the OS estimator. In what follows, I will really focus only on the slope beta one and it's estimator beta one. That's really what we're after. If we are after causal effect of X online, then Fingers crossed, what we capture that is beta one or betamo. OK. Betano, the presence of betano, as we will see later, it's just to make sure that we can shift the regression line into the middle of the scatter. And the beta one is the substantive, the meaningful thing that angles the regression line correctly. The angle of the regression line that captures the notion, if we are lucky, of a causal effect more or less. Oh, that X has a Y. Um, so Our focus is today on beta one and it's estimated beta 15. We could do similar derivations for beta, not half. Although be not had seems like a simpler estimator, the derivation is actually a little bit trickier, as you will find them in the book if you care. Um, so all the results today on the expected value and the variance in the distribution are for beta one. Because that's the really the thing that we care about. All right, so now we get closer to the low point. Let me not scare you too much. So here's just our regression model, and here in average form. So when you take sample averages on both sides, you get Y bar on the left hand side, and then you get beta not plus beta 1 X bar, plus U bar on the right hand side. So just sample averages of the three components. The sample average of beta knot is betanode. So what's the average of adding beta not end times to itself, or not? What's the average of beta 1 times XI? Well, beta 1 X5, and then the same for you, for UI. So we have basically uh an average version here of our regression model. And then I can subtract The first equation from the 2nd, and that's what I'm getting. What this what this achieves, why do we do this? We subtract it out be or not. Now, it looks ugly, but actually we've done this last week, at least the 1st, 1st line. Where it says thus, well, this is, this is our OLS estimator beta 1. We saw this last week. Now what I'm doing is I'm subbing in Y I minus Y bar from upstairs. That's what's happening here here. And then I'm, I basically have in this square brackets, I have got two terms. The X and the U turn and I'll break this up. Into these Two terms here. So you get And I want this stuff I just call this stuff and you get. Well I don't have to read it out. You can read this. Are there any issues with this derivation? Um, All I'm doing is I'm breaking up this sun here in the middle. And I've got these 2 turns here. I think I'm doing copying and pasting now from the last line, so this, this we had already on the, on the last slide. But if you look at what's going on here, these cancel each other, so you really are only left with data not here. So that's this guy here. And no change there. What have we just done? We have. Plugged in Yi minus Y bar in the definition or in our results for the low estimator and Change it to this result. So what this says is that our OLS estimator is equal to beta one. It would be great if I could stop the sentence right now. I would say. Because I just said, our OLS estimate beta one ma is equal to beta one. Unfortunately, the sentence continues plus stuff. That is non-zero. OK. So what you can see quite nicely in this representation for beta one hat, that is, you can see anchored at beta one, the thing that we are after. That's good. And then let's call this a nuisance term here, where we go, hopefully this is not a big number. Because if this nuisance term, like this whole big fraction here is a small number, then you go. So then betaha is close to beta one. Beta one that is close to beta one. If that uses of tiny, which um which we can make sure by driving and up as we will see large sample size takes you closer to the truth. All right, but we have to deal with this more carefully. And what is this nuisance to? Is what is the numerator? It is the sample covariance between X and U, right? That's, that's how you would read that. And at the bottom is just the sample variance of. So if the sample covariance between the regressor and the error was 0, we would be very happy. Of course it isn't, but uh hopefully it goes to zero. But what we want to do now is take this result and study. The expected value of hat. To do that, I'll do one more thing. I'll change. I jumped through these hoops here. I think we've done this last week, so I'll just leave you to study this. But the bottom line here is the U bar disappeared, not because your bar is zero, that is not true. You have bar is 0. We proved that last week in the workshop. If I average out the residuals, that will be zero, but if I average out the errors, that's not necessarily zero, but exploiting properties of summation here. We've done something similar in last week's workshop. This, this new bar here actually doesn't contribute anything, so we can might as well take it out. I just make my derivations on the next slide a little bit. OK. Um, So let's look at the expected value of beta hat, the conditional expected value, and the conditional variants. This looks ugly, but we can, we can do it. OK, let's let's go through it. Um, so we start by just plugging in. So we are after, maybe I'll just read left-hand side and on the right-hand side to get the big picture. The big picture is this. On the left hand side, we are after the expected value of beta 1 had given X. We do the math and what do we get? Beta one. We will do the math together, but let's just appreciate the the final result, namely that the conditional expected value of our one estimated is equal to beta one. That tells you immediately that that thing is unboxed. OK, so that's the, that's the cool result here. We have arrived at an unbiased result. The slope estimator is unbiased. But let's Let's work it out. So first line, I'm just plugging in from the previous slide the definition of beta one and that's this thing here. And I'm always conditioning on XI. And I should say, when I'm writing XI. To To clarify this, what I really mean is the entire collection of Xs that are available to you. I just don't want to write X1 X2 all the way to X and every time I'll just write X1. So what that means is that any X that pops up to the left-hand side of the vertical bar. You can do it as long. All right, OK. And there are quite a few Xs here. OK, so let's move to the second line. What's happening in the second line? Uh, I'm using this trick that the expected value of the sum is equal to the sum of the expected value. So this beta one can go out of the expected value. It's just a constant and then what happens here is you've got the, the denominator here, so this is 1 over sum of 1 squared, but I'm conditioning on X so I can treat this sum. Of XR minus XR 2 as a known number. I give you an Excel spreadsheet. And with observations on X, and I ask you to tell me the number sum of X I X plus square, you could do it. You could tell me the number. That's what that means, so you can just pull it out here. And then all you have left is the conditional expected value of the sum of Xi minus X bar times the error. What Let's continue. Let's do it. Uh, 3rd line, what happens in the 3rd line? Oh yeah, right, so I'm using that rule again. So here I've got the expected value of The sum I'm studying and terms. So I can go take the expected value inside the site. That's what's happening here, no issues. That was one of our laws for the expected value. The expected value of the sum is equal to the sum of the expected values. Here we have and terms that we. So we are in the 3rd line. Now, We can take inside the expected value anything that is dependent on X outside of the. because this is a conditional expectation with respect to us. So we can And we think that is that as no, we pull it out in the 4th line, that's what's happening here. Right. And then all we have left is the conditional expected value of the error it has. That is now where all that assumption one. Plays a role we could say. For accurately That is why we have to make that assumption one to be able to move on from this line to the next line. Namely, that this year is just a number. And this time could be 0, then. All our problems go immediately away, but I wanted to demonstrate to you, even if the expected value, the condition expected value of the error term is not zero for example of 42. You still get that result that we want. So let's assume for a second that it's not 0, so this condition expectation might not be zero, but some new, some new. That I can now put in front of here. But do you see that this thing here is 0? We have worked with some nations now. Quite a few times and I think. I want to say that many of you would see that that is 0. The sum of X1 minus X50. That makes the thing, the whole nuisance to here go away. What we are doing is we're considering the expected value of data of our hacks. So we expected value that nuisance to goes away. And we arrived at this result. A shortcut to obtaining this is by plugging in 0 here straight away, but what I just demonstrated to you that that's not necessary. Almost 6 But we can do it without cheating and get the same recover the same result. Any questions on this? Big picture, am I saying this here? Beta hat, beta one hat is an unbiased estimate of beta one. That's cool. Now, a similar calculation for variants. So it's, it's not gonna be prettier. We use that same representation or expression and Yeah No, so we're, we're starting the conditional variants now. Here, here we go. So we just Have to do Not much more difficult. So conditional variants, we just plug in here. Second line, what's happening? Um, well, we basically remove beta one. That's visually what's happening because we learned this in week one that a constant doesn't contribute to the variance, right? So you can just remove the constant beta one. Take it out. That's the 2nd line. Third line We Using the power of conditioning, so we're treating everything that is uh depending on X because we're conditioning on X as as a known value we can pull that out of the variance. When we pull constants out of variances, we square them, right? So the constant that we're taking out is in the denominator here, the sum of Xi minus X 5 squared. We have to square that. So that's happening here. Right, sweet, sweet. And all that's left is this. Now in line 4 we're going, the variance of the sum is equal to the sum of the variances. Now, of course, is to ask you. And you always do that. this to reply. Can you always say that the variance of the sum is equal to the sum on the variances, which is what we're doing here. No, but what justifies that here? Uh, the, the independence. So here you see what that assumption. 2 at So if you ask your so why do we make all these assumptions, so I've shown you now where we've used OS assumption on in the previous derivation. Here you see where we've used OS assumption 2. We actually have used OS assumption 2 for the conditional uh for the for the biasderation as well. I just haven't mentioned it. But I think I'm, I'm, I've mentioned it on the slides. But here we're using for, for the step from 93 to 4, we're using for that assumption. 2, am I saying this here? Yeah. The 3 equality holds by property 2 of the venue, sorry, the 4th equality holds by property 3 of the variants. Because of IID sampling. That is our assumption. 2 That we've been using. Um, and then next, well, we have the variance of Xi minus X times UI, the condition on X, you can take Xi minus X from the other variants, but you have to square it. That's what's happening here. And now we go. What can we do here now? Well, that explains to you now why we make that all assumption for forever and I call it. We wanted to simplify the condition variants of the error term. We just plug in a number for it. We call it sigma squared. So you Take that to the front and then you go, you can cancel one of the sums in the numerator against one of them in the denominator, and then what's left is this and, and yeah. Also, hopefully it's clear that if you look at what we have just obtained is that the variance of the estimator is the positional variants is related to the error variants and this, this big sum here but we can turn this into something. More relatable On the next slide, next, next slide. So this is what we had 2 sites earlier. If I divide both sides or upstairs and downstairs by 1 over N. You can do that. I get this result here. What do I get in the denominator here in the middle? 1 over N, sum of X minus X bar square. That is just the sample variance of X, right? So then what we have derived for the conditional variances is that is equal to 1 over N times the population variance of the errors over the sample variants of The regressive explanatory variable. So this is a relative. Easy to remember results. Namely that the conditional variance is proportional to the population variance of the error term, so it increases in the population variance of the error. And it decreases it's inversely proportional to the variance, I should say the sample variance of the regressor. And the sample size and. That hopefully was sort of expected. In analogy to what we obtained for the sample average, the variance of the sample average decreased in the sample size. The same is happening here. The more data you collect, the more precise your estimator will be. Where do you see that expressed mathematically in the one over here. So if I give you a data set with 100 observations or 10,000 observations, you see how this drives down, how how the how having 10,000 observations would drive down the barriers quite considerably. Plug in 1100 or 10,000, you would rather have 10,000 observations, and this is where you see that expressed mathematically. Now, we do a little. Approximation here to to turn this into a central limit theorem type result and it is OK. So what is a bit odd here is that we have a population variance in the numerator and the sample variance in the denominator. That's it's not wrong, but what we have done really is to we derive the conditional variance. It's like saying we know X, but oftentimes uh. To to apply the central theorem, we pretend that we don't know X and we treat it as a random variable. Then what we basically do is we go hopefully in a sample of the in a large sample, the sample variance is close to the population variance, and we just make the following little approximation, namely that using the results from the previous slide, if we sub in the population variance instead and it's just an approximation, then. That would be the result that you obtain and that is the result that the central lineage the spits out because the central limit theorem basically assumes that N is large and that the distinction between the population and the sample is. Negligible. Right. Um, so collecting results, we have learned that the OLS estimated is unbiased and we have derived an approximate barriers result. So now we want a result on the distribution, which takes this information and tells us now what is the shape of the distribution of the OLS estimator. And of course the central theorem helps us here and all we have to do is well. Take the information that we just arrived and and make the additional statement that the distribution is normal. The distribution and that's the asymptotic or proximate distribution. Of the OLS estimator. It is normal. Beta one, that's the unbiasedness result and the variants that we've just arrived. You get a result like that for beta not hacked as well. It actually is a little bit trickier to obtain, um, and like I said, we don't usually care much about beer. Any questions? Then this is the standardised version where you divide by the standard deviation and subtract of the mean, and if you have a normal random variable, if you subtract of the mean, you divide by the standard deviation, you turn it into a standard normal variant. Before we get in here. Now, what do we do? With knowledge about the distribution, we construct confidence intervals. that. So this was the bearings for the OS E to Mayor. The standard deviation is just the square root of that? Um, we've just learned that this guy has a normal distribution approximately. So therefore, We could consider the range and this was our generic result. Take the estimator. Look 1.96 times to times the standard deviation to the left and right, that covers 95%. And that's the confidence interval for beta one. That's the thing that we're after. So we take beta one hat and look 1.96 times its standard deviation to left and right. Now, if I asked you to calculate that from data, could you No, you couldn't because you don't know what the population standard deviation of the error is, and you don't know what the population standard. Deviation of X, but of course, you replace that with sample versions, and that's what's happening on the next slide. Just sub in the sample versions for sigma U, that thing is called SU and for Sigma X S X where these are the definitions. Why, so as X we have seen before, you, you basically just go X I minus X 5 square and average that out. That's the sample variance, but we want the standard deviation, to put it on the square root. Why is there no subtracting You hat. Analogy or similarly to what we're doing here is subtracting of the game. The sample average of X, why are we not subtracting off the sample average of U bar? Yeah, it's 0, it's 0. So we, we could do it, but it's redundant. We learned last week that the residuals average out to 0, and that they add up to 0. That's why you don't need to do this. You could, but they don't need to So we've defined the sample standard deviations. So if you put it differently. No, that that's actually what it is. The sample standard deviation of X and of the residual. And we just plug those into our confidence into where you saw three letters, sigmas, we're putting ass, the sample versions, OK? And that. This is exactly what we did for the sample average. You replace the population standard deviation. The standard deviation that was based on population. Moments you sub in the sample moments and I think that's called the standard error. So by by when I say think I mean. As you divided by the square root of as X, that is called the standard error. The generic result for confidence interval is always, and that's for a feasible 11 that you can actually calculate, given sample data. The estimator plus minus 1 time. 1.96 times the standard error of the estimator. That's what that is. That's exactly this expression here. If I give you the sample data, you can calculate this. So exactly what we did for the sample, so I'm just Contrasting this here and you see that there is almost will come. estimate as such a different name. The generic rule for constructive confidence in the world is the same. It is only knowing what is the distribution precisely, what is the standard error precisely, but we've just worked that out and I've given you all. And then we will see in a second how Python calculates this for for us when you, when you run Python over there. For now, we'll take a 10 minute break. wrap this up and then we do some workshop related. Right Yeah Yeah Yes So OK. Yeah And Let see here. Oh yeah Yeah Uh. He And yeah, a few, sorry, um, this is the process. OK. I I'll run through this town and like. Thank So. because there's some Python code coming. So this standard error here, let's go. This is the slope. Oh yeah, that's the standard error it uses. This formula or this formula. So, um, so as you and S X were defined. Here it computes exactly that. And um so that's where this number comes from. Oops here. The Z score, I think I'm explaining maybe now what it what it does. It takes the estimator and divides by its standard error so you can you can see this here. If you take this number and divide by this number, you get this number. You can almost see it because this is almost 0.5. You have to double this number. You get this number stats. That requires a little bit more computation in the background. When I was a student, we had these tables, statistical tables. There were no computers being used. Now computers can do this very quickly, but it uses exactly that same information and the knowledge that the distribution is almost normal to come up with this number, and the confidence interval is exactly. I remember we'd have those tables on the back of our exams and we would rip them off at the beginning of the exam to use it for. So they did last year, last year we exactly what this is. It's it's called Python speak for what we call um I call this to you here? Uh, some way I define it, I don't know, um, yeah, this thing. When you set 0, when you plug 0 in here, you get this, and that is what we call the T statistic, which interestingly Python called Z. OK. But then because it's actually another one from the distribution in finite samples, but in our samples it doesn't have a distribution. I think there is an option here somewhere use T equal faults. Python's default is to report the T statistic, which is based on the distribution which you might have done. Well, we move the push the T distribution out of the way and say no we only only deal with the normal distribution. So, uh, to tell Python that we put in, we put in the small in you didn't ask for that. I really enjoy it awesome. And then 11 other question, uh, it seems, uh, compared to the tutorials, it seemed like there's quite a consistency between them. Would it be beneficial, uh, generating other things other than what's just stated, like a, uh, a residual. so that you can see fit. Um, which give you probably a few degrees of freedom of creativity. Um, so use that. I don't know if we subtract points or people bombarding us with useless information, but if you feel like it contributes to the answer, you go beyond what's done in the tutorial. Um, I don't quite exactly remember what we are asking your assignments, but I do remember that it was very similar to to the tutorial, um, and we were expecting to use tools from the tutorial. something in stats or somewhere else that I want to add here do that. OK, so you relevant uh like a paragraph or so anything else there but not too thank you so much. Something about the. OK. Yeah. Right You Every I Yeah. right. No. All right, the mic is working. Is it too loud now? Um, Too loud. Also, it just occurred to me in conversation with one of your classmates that the assignment is due this week. Yeah, next one. Oh. But it doesn't hurt submitting it this week. You can resubmit it. I think We'll let you, we will, we will let you by resubmit, I mean within the deadline, right, not that you can resubmit at the end of the semester break now. So if you submit it now, and then you feel like, oh, I did something wrong. I want to fix this, then that should be possible. Uh, but now I want to encourage everyone to submit early. It's very similar to the week 4 tutorial, very similar. So similar. And um yeah, 3 hours, has um anyone worked on it? Are you brave enough to identify yourself, yeah. 343 to 4 hours. OK, so 3 to 4 hours, um, would you agree? I did in one hour. Um, yeah, so, and, and again, it's assessed in a friendly way, um, to submitted. Don't leave it up to the last minute. Is it due on Wednesday? Yeah, I think it's due on Wednesday of next week. OK, any questions? About the first hour, about logistics. Cool. Let's continue our journey here and it won't take much longer. Um, so this This is the result, well, here is actually the result for the confidence interval. Here I'm saying that it's really similar to what we did for the sample average in terms of its generic structure. Now let's run Python over a data set and see where we can find the things that we just discovered. All right. Let's take a look. So here I'm using data from the textbook, and the book provides it as californiaschool. CSV. It's the test score data. Um, so I'm regressing test scores on the student-teacher ratio, and I want to focus on, you know, on the calculations here, uh, and, and really focus only on the last line, the STR line here. So what that, uh, we can look at the intercept as well. So what this tells us, and those are numbers that we actually have seen before, the intercept estimate is 698 and the slope estimate is -2.28. I think we've seen these numbers before. Those are the estimates, so we can put the line in the scatter plot. Now, the standard error. You can, you can see where you would find that for the slope is 0.48. Python uses this formula here. OK, where these objects were well defined. That's where that number comes from, 0.48, by going to the data set, Python goes for you to the data. Does its calculations and spits out 0.48. All right. And there's a similar formula for the intercept. It's slightly different looking and Python applies it, it gets 9.46. Or 9.47. Um, what else? Now. Yeah, right, and we will talk about Z and P in a second, but let's move on to the confidence interval. That's the 95% confidence interval, 2.5 points in both tails, and what Python does to get this confidence interval here is it goes to this formula. Here It, it has calculated the standard error, right? You times that by 2, roughly, and look to the left and right of the estimate. So if you, if you see here, if you eyeball this. The standard error is almost 0.5, times that by 2. That is equal to 1. So you look 1 to the left and right of your point estimate. You get this Roughly. You do the exact calculation, that's what Python does for you, you get this. Right? And we do a similar calculation for the intercept is you take this number 698 plus minus 1.96 times this number here, gives you that interval here. Uh, and the conclusion for a hypothesis test based on the confidence interval for the student-teacher ratio is because the confidence interval does not include zero. Stays in the negative range. We reject the non hypothesis that the true slope, that the true beta one is 0. And that's it. That's the statistical conclusion here. The data, this data here supports the hypothesis that beta 1 is non-zero. Any questions? Yeah. He Yeah. Totally. And you will see in the, I don't know if the practise test already on, on the website. That's what I usually do this, and I do this as the first exercise and this now takes us away from this. As the first exercise in the exam, I give you something like this and test you on on that knowledge. I will not ask you to write Python code, that would make sense in a in a paper and pencil exam, but I could give you this and say, do you reject the null hypothesis that beta one is 0? That is a very low hanging fruit exercise, OK. Then I, I ask more complicated questions as well, as, as you will, as you will see, I'll, I'll give you um material to play with, but definitely something like that. But let's, um, let's work out the rest. What does this number mean? -4.7, and that number. Um, let's run through the slides. So I think we've discussed this, um, yeah, we have discussed this. Um, yeah, cast it all. Yeah, discuss that. OK, now, now I'm talking about, OK, let's go back 2 slides, 3 slides. The P value here, these two numbers that quite literally are 0 or zeros, what do they mean? You know this from stats, I'm sure stat statisticians teach P values. OK. So when we do Python even tells us that let's go slow. The first bullet point is. To run the statistical tests on whether beta 1 is 0. Which we have done using the confidence interval now, where we've rejected zeroness. We could have alternatively used the p value to to arrive at the same conclusion. Python tells us the P value of 0.000, which means a value that is less than 5%. So the decision rule is you compare the P value to 5%. If the P-value is less, then you reject. Reject what? The null watch now. The null that the truth is zero or closing the circle. OK. The p value was the smallest significance level at which the null hypothesis can be rejected. So that means it can be zero and you would reject the null, which means you have, well, to phrase it intuitively, a very um. Good estimate to conclude that the truth is not zero, OK. So using the P-value criterion, you, you essentially use the same information. And therefore, you have to arrive at the same result. The result or conclusion being that The true coefficient beta one is non-zero. There's another piece of information that Python calculates, the T statistics. So if you go back in this table, What I, what I mean is this number here. OK, it's called Z here. I'll explain that in a 2nd. -4.751. That is another number you can use to. To test the hypothesis that beta 1 is equal to 0 and how do we do, how do we use that number? So it's called. And the T statistic, that number is -4.75 in our Python results. The T statistic is defined like this. Essentially, the structure is. Beta hat minus the hypothesised beta. Over the standard era of betaha. And our hypothesised beta is 0. So therefore, the T statistic boils down to this expression here, beta 1 had over its standard error. Well If the null is true, then that has a standard model distribution, doesn't it? So then All you need to do is compare the number that you've obtained there to what? To the critical values of the standard normal distribution. And when we use the 5% level, those values are 1.96%, negative and positive. So going back to the table. This here, if you look, this number here, -4.7. How do you obtain it? You take the coefficient estimate and you divide it by the standard error, you get this number here. You can. You can see this relatively clearly here because the standard error is almost 0.5. So what you do is you double this number here. And you can almost convince yourself that you're getting this number here. If you do the calculation, the precise calculation, you get -2.28 divided by 0.48, 0.48 has to be -4.75. That is what Python does. That number or that that object has a standard normal distribution. So what you do is you compare that to uh to 1.96, in this case to negative 1.96. Is this less than -1.96? Yes, it is. So you are far out. From 1.9, from the 1.96 range, so you're rejecting the null. That you were close to 0. Same conclusion. You always reject the null, and it can't be a different conclusion. So what I'm trying to say here is there are 3 ways of arriving at the same conclusion with respect to the hypothesis test that beta 1 is 0. The first way was evaluating the confidence interval. Does it include 0? No, it doesn't. So therefore, we reject the null that beta 1 is 0. The second way is via the P value. Is it less than 5%? Yes, it is. So therefore you reject the null that the truth is zero. Same conclusion. Third way, and they're all equivalent via the T statistic that finally is called Z in Python. Python doesn't know econometrics. Um, but we can bend it to our will. Um, 4.7, you compare it to -1.96. 4.7 is a smaller number. You are sufficiently far away from zero here to conclude that you are not close to zero. Same conclusion. Yeah. Uh, can you go back, uh, go back to the slide with the P-value thing. Oh, P. Yeah, we should go forward. Yeah. Yeah. So in the remember sentence, shouldn't it be the largest? Maybe I'm just confused, but it all this should be the largest. The smallest significance level at which the null can be rejected. You want it to be largest. So it was uh 5%. You don't want it to be smaller. Um, My brain is not agile enough after one hour of bombarding you with the econometrics, I'll, I'll think about it. Um. Can be rejected. That means above it can be rejected the smallest level. Yeah, that's true. So yeah, you, you con convinced me that you must give me time to double check. If you reject it if it's below 5%. Yeah, the, the decision rule is, is your P value is smaller than 5, then you reject. So the the range where you reject is 0 to 5%. Yeah, that's right. Yeah, yeah. Mm. Yeah, so, um, oops, if, if the p value was 3%, for example, um, you would say the smallest significance level, if my significance level was 4%. I would reject. It's smallest than not correct. If it's the smallest, you should reject, that means if it's bigger than that, you you reject right? So, let's say I give you a p-value of 3, or the P-value is 3%, and my my significance level is 4, you would reject. But if my significance level was 2, you would not reject. So this would be the smallest significance level at which I think you might be reading it wrong, so the P value is the statistic you obtain, you can't reject at a significance level lower than what your value will allow you. 0.05 is the, the, like I said, it could be 3. Um, and then my significance level is my choice, but usually we say it's 5%. But if I say I can vary my significance level, if I have got a p value of 3, my significance level could be 3, and I would just be indifferent. To rejecting Anyway, um, happy to discuss afterwards. Let's not hijack the lecture. And the main thing is this, the decision rule like whether the wording here convinces you or not, the decision rule is the P value that you obtain, compare that to 5%. And if P is less than 5%, you reject. What do you reject? The null of zeroness. OK. So, in other words, the T statistic, the P value, and the confidence interval are equivalent ways of testing the same null hypothesis. OK. And they all will give you the same answers they can't. Arrive at different answers. Well this is the last slide. Are there any other questions? Not cool. All right, and then let's Dive into the workshop exercise and The IT support have helped me figure out how to use my iPad and let's see if it works. Just give me a second. Um Huh, it's there magic. All right. I think I can make this smaller. So this is the only exercise. So this is now about the OLS estimator for The intercept, which I said we don't usually care about, but, but here looking at this now. Teaches us a lesson about I guess what the role of beta knot is and beta knot had in in estimation. So let me do this. So first of all, to do this. Is this big enough still, yeah, I get. I remember from last week's lecture, can you guys see this? I think it should be OK. From last week's lecture, beta knot, hat is Y bar minus. Beta one had X-bomb. We are asked to study whether this guy is unbiased or not, so we want to consider its expected value. To get us into a good position for doing that, let's remember the Recall that our model is this year. And we had the Averaged version, we saw that today in the slides. And the, um, I guess the. The different version of this. To be so i minus Y bar is equal to. Beta one XI minus X bar plus U I minus U bar. So we saw that earlier. I hope that's acceptable. Now, just pointing out once more, that U bar is not zero. What is 0 is You had bar by construction. You might suspect you buy 0, but it isn't, OK? And you don't know what it is because you don't observe the errors. It will be close to 0, but it's not necessarily equal to 0. Is that now what I want to do? I forgot what the next step was. What do I want to do? Something like that, you know, but I don't want why I isolated. Um What am I doing here? If you want to substitute to the top and you'll get a. Yeah Ivan. Yeah, I don't, I don't need this equation that I just wrote down. So I just want this in here. Yeah yeah. Well, it's not wrong what I just wrote down, I just don't need it. But I'll leave it there in case I need it. I don't know why I did this. Let's go with, I guess, I guess both your suggestions. Is so we can go here I can go, this is probably not necessary. Um while it's not wrong, it's not contributing to the solution, so it's redundant. So I do want to plug in my bar upstairs, so maybe that's what I want here. Beta knot hat is equal to uh beta knot plus, now I can collect. Uh, yeah, X, now, let's write it this way, um. Beta 1 minus beta 1 had X bar. Plus, just you buy that, right? Yeah Everyone OK with that? Yeah. Now we're not interested in this, we're interested in the expected value of that. No. Uh The conditional expectation. Is this And Yeah, but that, I want a result for that. And if this thing is unbiased, this should be equal to be or not. But we just plug in and now we can use uh. This quite nicely. OK, that is And again, when I write conditional on XI. I mean all the Xs that are in your data set. So what do I get? They are not Plus X bar times the condition expected value. Of beta 1 minus beta 1 had given an XI. Plus the conditional expected value of U bar. So all I'm doing is breaking up the expected value. Everyone OK? No, what happens next? So this first Be not we want there and we would like the rest to be 0. We would like To stop here Now, can you tell me why the, the second term, so this guy here. would be 0. We want this to be 0. Is it 0? What is this? Equal to, it's safe to say it is. 0, but why? Yeah, because the slope estimator is unbiased, right? So by unbiased list of beta one had, this is 0. OK. Mm, so. Well, What do I do? Uh 0 because beta one had. Unbiased OK. So let me move this here down. And then I go, I still have this guy here. Oops, different colour. What's that equal to? Let's work it out. Can I, can I do it like this? Do you need this, uh, I guess, so we get, let's go slow. Be not plus the expected value of U bar. What is U bar? It must be the sample average of the errors. So, I have, so I can take the one over and out. And move the expected value in, right? We've done this now a few times. Hopefully everyone is happy to say this year. Is, would you be agreeable that this can be done? What is that equal to? 1 over N. So what is this year equal to? Uh Well, generically, we call it in the lecture, new you. The placeholder for whatever. Oh, actually, let me go, let me be slower. Sorry, I have to be, I have to be one step slower. Let me erase this. It is in fact this year. Now, I am, this is an N, not a U. Sorry, this is an N. It is of. You I Why? by OLS assumption one. Uh, one, OK. Well that's assumption one. Now this thing is. This is me, you. Wait, you, you as a placeholder for whatever the mean of you is. I say the textbook just says it's 0. But we said maybe it's not. All right. So then the end result is this. They are not, plus you you. Right? O No. Yeah, you're averaging, sorry, it's new. So if I tell you new you, the error mean is 42. Then the right hand side is equal to beta not plus 42. Is then, what is the, what is the left hand side? Is the statement that they are not hat is unbiased, correct? No. Unless you tell me that new you is 0. So how do we reconcile all this confusing Information Or is, is it not confusing? So First, before we reconcile this or try to make sense of this, I would say, Um, Beta. Beta hat, not is. Not Unbiased And then we go Unless You oops. Uh, yeah. clumsy with my pen knew you. It's 0. Now I'll show you why without loss of generality, we can assume that, and I'll show this to you in the picture. Let's try it. Let's look at. So I'm trying to make this point now. Can I move this out of the way? So, um, what do I do now? Let's draw a scatter plot. What do I do? Here With very big dots on This is my data. Now, I'll give you The PIF The population regression function. Uh, let me try this, um, Oops, I didn't want to do that. Didn't want to be that thick, but they want in that location. Yeah, yeah. If I tell you this is the PAF this that has generated this data. What's the mean of the errors? Is it 0? No, is it positive or negative? All dots are above the line. So all errors are positive. So this would be A regression line, a PIF. That has some slow beta one. Um, uh, this is a, this is not a good one. Let me, let me move this all up. Luckily, I can do this here. Yeah, and let me extend this here. This is Beta knot. If this is the population regression function, not the estimated one, that is consistent. This is a beta knot that's consistent with the errors that average out to something positive, not to zero. Does that make sense? You, I can draw a similar picture. I can give you a different, you can put the line on top of the dots, and I get all negative errors, right? What we are saying by, so let me, let me just write this down. So If this is my PRF. And the This PRF Is uh consistent. With This story here. OK. You, you can imagine how um a negative mean looks like and the PIF sits on top of the scatter plot. What we are saying without loss of generality. Betano is the value that puts this thing in the middle of the dots so that we get a zero mean. OK. So we go, why would we think of the PRF like so? Why wouldn't it be? OK, I'm gonna duplicate this picture. Why wouldn't it be? Here And this PIF is consistent, oops. With the zero mean of the errors, and the beta, of course, that the beta not that corresponds to this. As this one here. Now, if you look at those two pictures together, is that still, is that still all OK? It's the same dots, right? You can say it's either generated by the top picture, in which case you must interpret the errors as being positive or averaging out. To a positive number, or by the picture at the bottom where beta not has a different value and it has a larger value. It sits, oh it's, the value it has makes it so that the errors average out to zero. So yes, you could be in the above picture. If that, if that is The model Then be not hat. Is a biassed estimator for beta knot, because beta knot had independent of which picture you're in, beta knot hat is the estimated PFF. Let me put that in the picture now. The estimated PFF. Whatever the situation is, it, it tries to put a line in the middle. So for example, and I'm putting it with at a slightly different slope. This is the estimated PRF. Can you, is it too bright? I guess you can see that. So that is PRF hat, which is Beta not hat plus Beta one hat. XI, right? OLS will choose beta not hat to put the thing in the middle. But then when I tell you, but the, the true data was generated like the above picture, then beta not had. Betano you can read off here, is here. Beta not hat is not close to beta knot. But when I tell you, OK, that's a silly idea, why would our, all our errors sit on top of the line, um, why not consider the, the bottom picture and I'm using the same estimated PFF here. OK, I want it to be looking a little bit different. In the bottom picture, I'm getting my My beta knot hat, being close to beta knot. And on average, the same. Can you guys follow me what's going on here? So it, it really depends on how we think the original or the population data is being generated. In both pictures, the dots are the same. And there's two distinct models that generate the same dots. And what, what makes them different, these two models is the true value of beta or not. But in a way, what is, what this, what this is saying is There's two parameters that you're playing around with that um you're calibrating to produce the same result. Beta not and the error, the mean of the errors. They together But um fix sort of the, the scatter plot, OK? And then what I'm trying to say here is, why wouldn't we just think that the bottom picture represents The true model, OK. In, in which case, um, the, the errors average out to zero, in which case beta not have is a unbiased estimator of beta not. Does that make sense? All right, so, but our formula and the most conservative or cautious statement is That beta not had Is is is biassed. What am I saying here? Yeah, it's not unbiased, OK? Then you go the book just assumes that the errors average out to zero. So what the book does if you have the choice between these two scenarios. The book says, just pick the one at the bottom. And why can we do this? Because we usually don't care about the value of beta knot. Beta knot is the thing that puts the PIF into the middle of the scatter plot. In that case, you can interpret the error mean to be zero and then betanote is an unbiased estimator. OK, I think I've made my point here. OK. It's sort of a technical curiosity. In the end, we really only care about beta1 and beta 1 hat, and in both pictures, the slopes are the same. So it's un it doesn't matter for the slope, OK? So this is just an academic discussion for the constant term. And yeah, that's, that's all. Are there any questions on that? Cool. And remember your quizzes this week, and the assignments are all to do this week. OK. All right, see you guys next week. uh. Uh I. to get. I don't, I don't really get what you so in this situation, this is a situation where it just so happens that the sample you've collected is just like coincidentally offset compared to the true population beta not by a positive amount, right? Mm, no, this, these dots, sorry, my microphone. These dots represent the population. So I'm just saying these are the dots. Then why would the data be? it can be, you know, that's the whole point. You can generate these dots with a line that runs through the middle or with a line that runs under. But doesn't the way that you calculate minimise the knot is not being calculated. I'm giving you a number, 42, and you add errors to it. They all have positive value. Then those are the dots. It's like, um, it's like you start with a line first. What They go, I give you this line. And now I'm adding errors to it that all are positive with these guys. but with work. Right. You can say, well, alternatively, I can take these same dots now we're not exactly the same one you get the idea. I can say, well, these dots could have been generated by this line here as well. Here the errors are positive, here the errors on average are 0. And then you can also put a line here. There's infinitely many lines that generate the same dots. And what, what these lines, what makes these lines different is the intercept why are we generating the dots instead of generating the lines from the dots? Yeah, so we're sort of reversing our viewpoint here. Are we just Thinking about What line are these dots consistent with? And actually infinitely many. And, but what pins it down is like, what is this, why would it be this like just pick the one line that averages the dots out to 0. I mean that line is. Putting beta not here. You go, well, it could have been this line, but, um, textbooks say since we don't really care about the value of beta not just pick the one that runs the whole line through the middle of the day. So you're saying that um you could like you could choose any beta knot and it would not ruin the whole system. So therefore you just choose the best ones. And we ask what line can we fit? All I'm saying is we can fit a lot of lines here. The, the simple intuition is, if I ask a primary school student, well I'll just draw the one through the middle. That's this picture here. And that's, that's and that comes the way I'm saying, yeah, this is really how we see the world. But what we just learned is, well, this could have been generated by this line also. OK. And um what I'm, what I'm saying is that um well the the the only difference is, and, and the slope is fixed, the only difference is the intercept and and and then. The errors are just the vertical distances, OK? And if you shift this line up or down, then the errors change and here the errors are are positive. Um, so then why wouldn't like we include the condition inside of the algorithm that generates be not had that it has to be that it has to minimise. No, so that's that's in fact what the book does so assumption one. I say very careful one is is this here. Where I go this year doesn't is not necessarily you, so it could be this situation it could be that situation. The book says no, it's the situation. And the reason for this is, uh, this is the intuitive way of looking at things and but strictly speaking, what's the whole that's the whole point of today's discussion that if you don't have this, then they are not hat is not not unbi's biassed. Because you don't know, are you here or are you there, um, but then we go without loss of generality, we are in this situation because we don't usually care what this value is. All I want to know is what, what this value is the slope. The slopes are all everywhere the same. that make sense? Yeah, I think that makes sense. I know it's a bit confusing and to try to. Make that point today and that's why you see in the book, oftentimes you don't even see this. You just see exact immediately that they say yes, and I just wanted to offer this in addition to what the textbook tells you. OK, so this part is the strict definition and this is just a further assumption, yeah, but then I say this, so we go, this is without loss of generality for the purpose of estimating data one, which is our. Go really, yeah, yeah. OK. Yeah, digest this. It's a bit tricky, but if, if your goal is to estimate the true be or not, actually, there's nothing you can do because you never know, are you here or are you there? So another way of looking at this is if I told you your objective is to estimate data or not and you're not allowed to make this assumption here, then you can say sorry I can't pin this down because I don't know, is this my is this reality or is that reality. But luckily we don't care about the reason why we don't know whether this is the reality or this is the reality because we only have a sample and we don't know whether the sample just happens to be offset by coincidence. No, because this is really the picture that you have, you know, whether it is sample or population data, I ask you which line. Could be consistent with this picture and that's why I mean you don't, you don't know which line. I mean we have learned we can fit one slope to this here and 11 regression line. The last line is the one that runs through the middle like we we explored that. But what the population line that runs through here could be this or that and in fact if you think about there's infinitely many. And what changes with each line is the error. But what what you get is the error mean. Plus beta not. Gets you into the middle, yeah, yeah. That's the only constant thing. Yeah, yeah, that's fine. I know it's a bit. Who's next? We just had a couple of quick questions about the assign. Um, is that brief. Are we gonna be penalised? Is that, is that not brief enough? No, that's fine. OK. Come back. You told me you would be. pages. Our goal is
