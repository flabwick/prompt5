SPEAKER 0
This material has been reproduced and communicated to you by or on behalf of the Australian National University, in accordance with Section 113% of the Copyright Act 1968. The material in this communication may be subject to copyright under the Act. Any further reproduction or communication of this material by you must be consistent with the provisions of the Act. Do not reproduce this material. Do not remove this notice.

SPEAKER 1
Yes, that's right.

SPEAKER 2
OK So what's happening?

SPEAKER 1
Not much. I'm thinking about consuming some multiple regression.

SPEAKER 3
You want me to unfortunate.

SPEAKER 1
Well, actually, I mean, I've heard of instances where tutors have given lectures, for example, in the.

SPEAKER 3
Must have been approved before I would not want you to take unnecessary risks on my account.

SPEAKER 1
Besides, you have practised this much more than me, so you still have comparative mental.

SPEAKER 2
I've done it like 10 times, yes, that's right nonetheless comparative advantage nonetheless, that is why you do this 10

SPEAKER 1
times, right?

SPEAKER 2
Yeah, that's right lets you give a lecture. Maybe I'll duck out halfway through. and Yes. OK. Yes More speed I And that's OK. Yeah.

SPEAKER 3
OK, what time is it? Should we get started? Oh yes, we should. Good afternoon everyone. Nice to see you, um. You guys have a good break? How were the exams? Not not this exam, so so. OK. All right, let's uh continue. 6 more weeks of econometrics. We start the multiple regression model today. Um, straightforward extension of, of the simple regression model. Uh, we, we have 3 lectures, I think on the multiple regression model. And then in week 10 we start with time series analysis, which is an application. for us at least an application of the regression model. The models that we will study when we do time series are called auto regressive models, um, so there are particular type of regressions. Are there any questions before we move on? We've posted. Participation marks, assignment marks, that's really all we've posted, and I've written what messages or announcements explaining how you can seek feedback on the assignment if this time today I'll run broadly through the first assignment there's a question.

SPEAKER 2
I received a grade of HML part but I like a little squiggly line to the Python. or is it just the grogginess.

SPEAKER 3
Um, I guess it depends from Tudor to Tudor, but, uh, as long as there's one mark, that's, that's the one that applies. Um, the way it works from there and they, they would have marked in your case the HTML file, and there's no point in writing the same mark or attaching the same mark to the uh was it Jupiter notebook Python notebook file. So most of you will have received a mark on either of these files. Any other questions? Cool, um, the. What is it called, the portable, the, yeah, the little mic wasn't charged, so I'll just stand here for the first half hour and give the mic some time to charge and then I can walk around again. Um, OK, so let's do, let's do econometrics, multiple regressions. So today there's actually not a lot of new stuff. We'll pace or run through it at a relatively fast pace. Only at the end when we talk about the F test, is there something that is conceptually new. So let's do it. And OK. The model is this now. What's different here is that you have more than one explanatory variable, and it's convention to call the number or to give the number of explanatory variables, the letter K. So here we have K regressors in addition to a constant. So we've got K + 1 Greek letters here. Um, the constant is indexed by the uh by the number 0, so be not, beta knot. Um, what else? So in this case, we don't have any observations on ordered pairs X and Y, but uh a bigger twople here of dimension K + 1 K regresses and the dependent variable. OK. um. Naming is still the same. The regressors, X1 through XK are the independent variables or explanatory variables or regressors. Another word that you sometimes see is covariate. Um, still have only one error term. Um, an intercept and case lope coefficients. What is conceptually different here is that But it's not really different, but you just have to be careful that. The generic statement that the error term captures everything, everything that explains the dependent variable that is not a regressor is still true, just that you have more regressors, OK. So you could think of it compared to the simple regression model as Uh, the error term playing a smaller role, but it still plays an important role. So if you, if you add more regressors to the model, you're trying to explain more of the variation in the dependent variable, uh, explain more based on. Observable data, but there is still always an unexplained component that's the error term. And Uh let's continue here. The interpretation of the slope coefficients here is a little bit different and requires us at least in this lecture to be very careful. Um, and that is How it works here So we focus on beta one, which we have focused on in the past, the coefficient that belongs to the first regressor. So that is the effect on the dependent variable Y of a unit change in. Regress saw 1, X1, and now a statement that you wouldn't have seen before holding X2 X3, all the way to XK constant. We didn't need to say this before because there were no X2 X3, and so on, OK? Now, We need this holding something constant condition. Whenever we make a statement on interpretation on a coefficient, OK. Um, when we write out here in the middle of the slides, the population regression functions where the only difference. The only difference is. So I've got two equations really. I, here at the top, I'm considering the um PRF. Fixing all the regressors at particular values. So for example, the first regressor I fix at some value little X1. So little X1 is a placeholder for some number. So, uh, number 5, OK, X2, you fix at another number. X3, you fix it yet another number and so on. And then, You're right. Well, you evaluate the PIF uh by just sort of plugging in those little numbers, X1, little X2 all the way to little XK. So you write that out here. That is the expected value of the dependent variable when the regressors are fixed at these values for the little axes. If you then perturb or change the value of X1 by 1 unit, but not the other values of X X2 X3, and so on. That's what happens in the next line here, so all we do is we add one. To little X1. So we increase the value of X1 by one unit, holding the values of the other regress regressors fixed. Um, you, you write out the PF, so we're writing it out here. All that changes is you have X1 + 1 here. If you subtract the two of each other, you're basically subtracting out that happens mechanically because the model is linear, you're subtracting out all the other regressors. And that makes evident that what what what is left here is beta one, makes evident that Beta one As the expected effect on the dependent variable of changing regressor one. By one unit While fixing the other regressors or keeping the other regressors fixed at some other values. I could then do the same exercise changing the value of regressor 7, for example, by 1 unit, but the idea is always you don't change two regressors or more than one regressor at the same time. You only change one regressor, holding all the other regressors fixed. Any questions on that? So the holding everything else constant is important. And we will emphasise this as we As we continue here. What else? Yeah, and so we, we call that the partial effect on Y of X1. OK. And likewise, all the other regresses. Or coefficients can be interpreted as partial effects as well. How would you define the OLS estimator in the extended model? It's fairly easy. You take your objective function. Um, that we had earlier where we just had one regressor, but now we have clear regressors. So we read this now as a function in K + 1 variables. I always say plus 1 because the constant is included as well. So straightforward extension, it's still a sum of squared. Errors or residuals, whichever way you want to interpret it, you're solving it. For the beers or the bees here on the right hand side, the bees, and in such a way to minimise that objective function. So we look at this whole thing as a function in the bees, be not B1 all the way to EK. It's a it's the big, big picture here is it's a quadratic function. OK, here, power of 2. We find the values of the bees that minimise this functions. The values that minimise that function are called the solution, and we call the solution or give that a particular labels, Greek letters with hats on them. Those are the estimators, OK? And I showed you this picture 3, not 3 weeks ago, but you know, in, in week 3 or week 4. Then that was the picture that. does apply to the simple regression model. And now if you go, how would the picture look like in the multiple regression model where you have K regressors, then I'm saying this here, you can't plot this. OK, we don't have enough visual dimensions to Um, to imagine to uh to make. To represent it visually, OK? So. This unfortunately is the best we can do if we have 42 dimensions, for example, then we just. Can't visualise it, but we don't need to. Mathematically it's a well behaved problem. We've got K +1. Unknowns and we will have K +1 1st order conditions which we can solve. We won't do this paper and pencil, but we'll let Python to it for us. So what am I saying here? In the multiple regression model, the geometry of the minimization problem can't be shown or illustrated in two dimensions. Um How would you obtain the solution, the values for the betas that minimise the objective function, what am I saying here? I don't even bother. I mean, how would you do it? You would sit down and take all partial derivatives that you can take. So let's say there's. 10 coefficients in there plus the constant, then you would take 11 derivatives that gives you 11 equations for 11 unknowns and it can be shown that this problem is well behaved that you will have exactly the right it will be. Solvable. OK, you can solve it. And well, actually, you can't because it's super tedious. OK. So the way to do it is ask Python to do it. Or if you wanted to do it is to learn a little bit more linear algebra and to do it compactly using matrices and vectors. OK, we will not do it. We will do it in the 2nd hour of today for 2 regressess. Where I'm throwing out the constant, so you're already seeing that for this to be feasible, I need to take shortcuts. The most I can do, I think I could do beta not beta, beta 1 and beta 2. That's 3 equations and 3 unknowns, but then it really gets boring after that. In the second hour we'll do two gen genuine slopes, beta 1 and beta 2. But I said beta not equal to 0 to avoid that complication. So we'll, we'll see that in the second hour. In general, in higher dimensions, we let let Python solve this problem for us, OK? Um So this slide we've seen before, it's a straightforward extension. The predicted value here for the dependent variable now relies or is based on your K regresses. So you don't just have beta not hat plus beta 1 had X1, but you actually continue beta 2 hat X2 all the way to beta KXK. The definition of the residual is literally the same as before. There's no change. It's the same formula, but conceptually the difference is hidden in white hat. My hat was defined upstairs to be based on your care regresses now. But the residual, the generic definition is dependent variable minus predicted dependent variable. That's the residual. So nothing, nothing surprising here, I would, I would say. Please stop me if you've got a question. I'll just continue. Well that was that. OK, we'll quickly run through our 4 assumptions that that we needed to understand the distribution of the OLS estimator. And what we'll do is we'll just Discover that these assumptions are straightforward extensions of what they were in the simple model. So Let's do this or that assumption one that was the conditional mean independence assumption. So the only thing that that is new here. Literally or visually is the conditioning set here includes not only X1, but all yours. X1 through XK. Um, and the statement is, So the, the meaningful statement here is the first equality. This one here, which says that to the left, the conditional expectation is equal to the middle thing, the unconditional expectation. And then we had this discussion before for the unconditional expectation we can just without loss of much generality assume that it's zero that we discovered that in one of our workshop sessions that that was really the role of the constant term to make this happen. So the statement is. Given knowledge of your care regressors. Does that knowledge help you forming an expectation about the error term? It shouldn't. If it doesn't, then this assumption you hold or is justified. So I said this quite awkwardly now, so let me try to do it again. In you, you're thinking about the expected value of the error term and the question is does information about the regressors. Help you understand that expected value. This assumption here says it shouldn't. There's no information in the regressors that helps you shape an opinion about the expected value of the error term. That would be justified if you can convince yourself that the error term. is genuinely random, then knowing values of regressors will be useless. OK. That's the CMI assumption, and it's it's conceptually the same interpretation as before. It's just now you have more regressives and quite possibly there could be a bunch of regressives that are now indeed informative about the error term that would undermine this assumption, but we will explore this as we go along. OK, I think I've mentioned this. Well as assumption 2, we need our sample data. To be truly random, OK. Um, so this is more stringent here because we have more data, we have more variables. Um, but if the sampling process is, is random, then we are happy to live and accept this assumption here. OK? We need the Outlier assumption or no outlier assumption, OK. So that applies now individually to the, to the regressors. Um, where we have this picture here? No, I don't have this picture that the regression once with an outlier, when you run it with an outlier. It might tilt the regression line. Um, what you want is to have so much data um and the presence of outliers to be limited, that the regression line isn't affected. Affected by um by the presence of Few outliers. Um, like I said, it's often really. A problem in practise when you work with uh survey data where the data has been entered incorrectly. So let's say you worked in your homework assignment with the With weight people's weight in kilogrammes or pounds, let's say the data was entered as somebody's entered weight as minus 9000 plus 9000 some large number that could influence. Mechanically, the regression estimate. Um, so here we're saying you have made sure that outliers are not. Present in your data and then you can run your regression and outliers applies to all your regressors and the dependent variable. Meaning That you don't have outliers. Relatively easy to deal with in practise. You just look at histograms of your data. OK. So there's nothing new here, really. This here is Not an assumption as we learned last time. So I'm not Going with Homo scare here. I'm going immediately with heterosketa City, which we learned is not really an assumption. And what I'm trying to say here with this slide with this assumption is that the variance of the errors is not a function of the values of your regressors. And we've learned this last time when we had one regressor, now we have more regressors and Um, so that what I was just describing was homo galasticity. Here we actually allow this to be dependent on Uh, the values of the regressors. OK, the formulas, um, That we provide and the way we estimate this in Python allows for the variance of the error term to be a function in the regressors. OK, let me say that again because I said it incorrectly initially. Here we allow for heterosk city, so we allow for the variance of the errors to change in the value of the regressors. We do not have to have a exact understanding of that variance, but our Standard errors allow for this possibility. Um Another assumption that you might see in some textbooks is this the assumption, is this actually in our textbook? Yes, the Stock and Watson book has it and Woolwich probably as well that the regressors are not perfectly colinear. Um Or Another way of calling this is no perfect multicolinearity. I'm saying that I'm not a big fan of this assumption. Um, it's a bit of a A digression, it's a little bit um. Similar to the discussion on outliers, you can make sure relatively easily that you don't run into this problem. So for example, you would have perfect coinarity in your regression, in multiple regression, if you include a regressor twice. But why would you do that? If you do this in data, data is so kind to tell you, oops, you, you included um education twice in your regression. I've thrown it out once for you. OK. Python, I think might run an error. Um, but then you'll detect it very quickly. Um, so literally, when you repeat a regressor on the right-hand side, then you have multicolinearity problem, but it's, you wouldn't do that. Then there's a slightly more complicated situation where one regressor could be Uh Could be constructed as a linear combination of the other regressors, then you would also have multilinearity.

SPEAKER 4
Yeah.

SPEAKER 3
Mm. No. So that that is not a weighted. Model, so you're not putting more weight on it. Um, I don't actually know what happens in Python when you do that. Um Can you can try this uh tomorrow in your computer sessions if it throws an error or if if the stats models package is um savvy enough to tell you why have you done this, um, please exclude one of the repeated regressors. But what I was just trying to explain now as well, which is not so easy to detect, let's say you have 10 regressors, it could be that regressor number 7 is a perfect linear combination of the other. 9 regressors. Um, but your software will tell you that. So, long story short, let's not waste energy on this theoretically possible situation, OK? I wanted to point it out because you will find it in some textbooks and then you might wonder why wasn't he talking about it. It's not typically a practical problem. Now, these assumptions, before OLS assumptions we had to run through to understand the sampling distribution of the OLS estimator. Um, so that's what I'm saying here. We are interested in the distribution for the slope coefficients, not really for beta knot. Beta knot is just a vehicle to move the regression line into the centre of the scatter plot. We are interested in the slopes. There are K slope coefficients and K estimates. Um Now, the distribution result is actually quite complicated, OK, for the, for the Kres model. So I will not give you an explicit formula. If you want to see that, you can take my graduate class where, where you derive it using linear algebra. Um, but then you have to take a semester's worth of linear algebra. Maybe Jose taught you this. I don't know. Um, it can, you can write it down quite neatly using matrices, but we're not doing this. So I'm giving you a relatively hollow or empty result, which basically just says there are standard errors. But I'm not telling you. How they look. I don't, I'm not sure you will even find them in the book. Uh, only if you If you're willing to, to read um linear algebra and if if the book is willing to use linear algebra, it can provide you with explicit standard errors or distribution results. So let's, let's run through this. One result that is relatively easy to show even in this in the multiple regression model is that the OLS estimators are still unbiased. That is that is true, and that's not that difficult to show. So we've got this nice result unbiased. The slope coefficients are unbiased. I should say the slope coefficient. Estimates or the estimators are unbiased, that's great. Yeah, this is what I mean by this is not very useful. All I'm saying here. is that there are variances and there are covariances. I'm not offering any content. I'm not. Offering any Informative results here. All I'm saying is, I don't know, it's a bit tautological. The variance of beta hat given X is some number. OK. And I'm not telling you the number because like I said, the number requires some heavy lifting in terms of derivations, maths. Unfortunately Python and any any statistical software package will use linear algebra in the background to compute the estimates for these guys, but the good result is this. Conceptually, the CLT, the central limit theorem, still applies. And therefore, our OLS estimators for the, for the slopes have a normal distribution. Around The population coefficient beta j. And with the. Particular variants. With, with a particular variants where I can't provide you the exact formula unless I'm using linear algebra. OK. The estimator for this placeholder sigma squared here. is what Python will calculate for you. And we'll see that in a second. So what am I saying here? So instead of, so this, this here would be the basis for deriving standard errors and confidence intervals. And Since I'm not giving you an explicit results, we, we will not talk about how to obtain standard errors. Instead, we will rely on Python to do the hard work for us in the background, OK, using um. Using the linear algebra that's required to obtain those. So let me run you through an example, and we, we use the example from the textbook again where Until now we had two variables. The outcome variable was test score of primary school. So I think it was secondary school students in in California. And the main dependent explanatory variable was the student-teacher ratio, classroom size. Um, Now, we add 2 additional regressors because the topic is multiple regression, so we need more regressors. So instead of just regressing test score and STR, we regress it in uh in addition on. The two regresses that I'm giving you at the bottom, and they are, so this is ELPCT, the English learner percentage, just some background. These were schools, secondary schools in in California, where many students are. Migrants or have some Um Some background um with parents might be migrants, OK? Um, but in, in this case, many students are actually, um, Newcomers to to the to the US and then the question is, what is the percentage of non-native English speakers in the school district? Remember the. The unit of observation here was the school district, so you might have A very diverse school district with a lot of um I guess Spanish native speakers and by a lot I mean relatively. Speaking, um, and then you might have school district if I think about Beverly Hills, for example, I don't really know too much, but I'm guessing there isn't a large English learner percentage in Beverly Hills, and they are priced out of that neighbourhood. um and that that is how I would be thinking about this and. So we want to control for that because we would think that that explains. Test scores and the story is it English learner percentage affects test scores because you don't understand so well the instructions, OK, so not because they are worse people, but they are at a disadvantage in English instructions. So they would think that the English learner percentage affects test scores. Because of lack of understanding, slower learning because of language barriers. So not because they're worse people, OK? And The second regressor is the total annual expenditure per students in the school district. So going with my example of Beverly Hills. I don't really know how it works, but let's say that's a wealthy school district. There's more money and that should. I can't see how that hurts the students, OK? So it probably benefits students. And then take a school district from downtown LA. Um, relative to Beverly Hills, you have a measure of how much the school district can spend per student, and you want to include that in your regression as well. OK, that's the setting. And then Two regressions here, one that we have run before, the simple regression. And the result is here at the top. So we've run this regression before. This is now reminder already heterosketasticity robust. So what are we doing? Where do we do this here we fix the covariant structure using the heteroskeleticity robust. Uh, covariance estimators, um, which gives us the correct standard errors. The point estimate we have seen probably in week 2 or 3 of the semester for the student-teacher ratio was -2.28. So That seemed reasonable. We expected the effect to be negative. OK, larger classroom, more students per teacher, not a, not necessarily a good thing, so that's the negative sign, and the number could be of interest as well, but I don't want to interpret too much now. This number I want to show you now how you run the multiple regression in Python, and all you do is you change the formula so we've got the simple regression formula is up here. And the multiple regression formula is, is there. So all you do is you add the other two variables using pluses. Anything else different? Not really, and you still make it heterosketasticity robust. And now lessons learned, you look at the output table. You get a similar looking output table. You have 2 more lines because you've got 2 more regressess, OK. And maybe the first. The first lesson could be the intercept doesn't have to be the same. These are the estimates, right? It's, it's a different subset or combination of regressors. So just because you intercept upstairs was 698 doesn't mean it has to be that downstairs. So that's also that's different. The slope estimate on STR changes. OK. So it was 2.28 negative up here, and now it's negative, so it's still negative, same sign, 0.29. Quite, quite a different number. Um, so. It's a very broad lesson. As soon as you add regressors. The coefficient estimate on the one that was already included can change and mostly does change. And why is that? Anyone want to guess? So this will have to do, oh yeah. Yeah. Yeah. Yeah. So in the model upstairs, the one above, where you did not include the other two variables, the student-teacher ratio indirectly captures. The effects of the other two variables. So for example, Um, let's say you have a high student teacher ratio. That to me is uh uh it's the opposite of privileged this unprivileged like a disadvantaged school district which is probably also a higher English learner percentage so you can see that uh these these regressors are correlated with each other and to the effect that they are correlated, the regressor in the in the simple regression model above. Soaks up these indirect effects as well. OK? So as you throw in more regressors, which we've done here at the bottom, you could argue and hope that you, you get your approach. A clearer direct effect of student teacher ratio on um. Test scores. OK. We will have that discussion when we talk about a minutelapias next week, but that was sort of the first preview and that was the perfect explanation. Um So, generically speaking, the numbers change, so the estimate on SDR changes between the simple model and the multiple model because STR is correlated with English learner percentage and the expenditure per student. Um, in that table, you can also read off these coefficient estimates of these two new variables in their own right. It could be of interest, OK? And I would, I would think they have the expected sign, and I don't want to interpret too much about whether the sizes. As expected or not, but mechanically, what else do I want to do? Yeah, mechanically, you still, and you see this highlighted here. I don't know why I highlighted it. You still, you get your standard errors and how you map from coefficient estimate and standard errors to the T statistic to the P value and the confidence interval is exactly the same. So for example, How do you obtain this number here? 2.447, the T statistic for the expenditure per student, that's exactly as 3 weeks ago, you take the coefficient estimate and divide it by the standard error, you obtain this number here. And then likewise, you know how to construct a confidence interval. That is exactly the same. All right. This is the same regression it's just. Copy and paste. Now what I want to emphasise, and it's good that I have a slide for it because otherwise I would have forgotten, the interpretation of course is now of the coefficient estimates we have to be a little bit more careful. We have to use this holding everything else constant qualifier. So the interpretation is, so let's go back to the previous table where we have the simple model. The interpretation is if. If student-teacher ratio goes up by 1, we expect test scores to go up by 2 to go down by 2.2. 8 I do not have to say, holding everything else constant because there is no everything else. But here in the multiple regression model, the interpretation is if STR goes up by one unit, then to test score. Is expected to decrease by 0.2864 units, holding everything else constant, um, so in particular here holding the English learner percentage and the expenditure per student constant. OK. So that that is important. And then likewise, you read um the coefficient estimate on the expenditure per student. If expenditure per student goes up by 1, I think it's $1 then you expect test score to go up by 0.039. Test score units holding everything else constant. Ah, and then Oh yeah, but what you also see is between these two models, the simple model and the multiple regression model, um, the coefficient estimate and the first one suggests a statistically significant effect. While it doesn't in the multiple regression model. Um, because here you've got a. TSA of 0.594, which is too close to zero to distinguish it from zero. That was not true in the in the simple model. But I would, I would trust this one here more by that story that in the simple model, STR is doing the job of the other variables as well. So if we regard the multiple regression model as a richer specification, then we could conclude that the simple regression model overstates an absolute value, the association between test scores and the student-teacher ratio. So it seems to ascribe too important a role to the student-teacher ratio. When we control for other factors, the importance of the student-teacher ratio goes down. Yeah. I think I've mentioned all of that. Any questions? OK, and then. One more random topic now that we've discussed the multiple regression model, we can talk about so-called tests of what I call it joint hypothesis. So here's the regression table again. So you do know how to test a simple hypothesis that is one coefficient. So if you wanted to test whether the beta 3 is equal to 0, you can just read that off the TTE that belongs to beta 3, which was 2.447. You can, you conclude that 2.447, because it's larger than 1.96, is sufficiently far away from zero. To make that call that it's probably the underlying population coefficient is non-zero. OK? That's a, what do I call it, a simple hypothesis test. But what we want to study and that that works in these. Tables in these multiple regressions just the same. You do it individually each coefficient. Now what if you wanted to test a joint hypothesis and here you have 3 regressors, 3 slopes. How about you're interested in beta 1 and beta 3, and you hypothesise that both of them are 0. You could just hypothesise that. I mean, there's many hypotheses that you could have, but let's do this one. Now you can go based on the table. Here, based on this table here, if you go beta 1 is 0 and beta 3 is 0. Do you agree with that? What's your, what's your best guess? Well, you look, you look at, well, we just discovered that beta 3 is probably not 0 individually, so you're guessing. That you would reject that hypothesis. The hypothesis that both beta 1 and beta 3 are 0. It's just a sort of. Was that a reasonable view of of this regression output OK, but it's a little bit more complicated than individually checking the statistics. Let's investigate. So first off repeating what I just said. You noticed that the TSA for the expenditure per student is larger than 1.96% so you would. Conjecture or guess that you would reject the hypothesis that both beta 1 and beta 3 are 0, just based on the conjecture that well beta 3 is not 0. But what you're forgetting in this approach is that the two estimators for the betas are correlated. And you need to factor that in as well. So Checking the individual T statistics. As a good first guess, but it's not quite correct because it forgets the correlation between the two statistics or the coefficient estimates. Let's try to make that visible. And what we will study instead is a so-called F statistic, which is in, which is defined here, so. Uh just sort of, I just offer this to you here to, to give you some sort of intuition for why the individual statistics are not enough here. This F statistic Combines the two individual T statistics, T1 and T3, and squares them. But it also takes Um The correlation between the two key statistics. Why are the T statistics correlated? Well, the T statistics are based on the coefficient estimates. The coefficient estimates could be correlated. OK. And that's reflected in this formula. Now, if somebody told you, don't worry, I know they are not correlated because I'm an oracle and I know that. That formula reduces to. To this one here. So what I'm trying to say is if coincidentally the T statistics were not correlated so that row had guy here is 0. In this formula, this formula. Collapses or reduces or simplifies to this one here. So, if the two T statistics are not correlated, happen to be uncorrelated, then the idea of the F is, yeah, take the individual T statistics, square them, and average them. OK? That is, that aligns with our initial intuition of to test. The joint hypothesis that both beta 1 and beta 3 are 0, do individual T tests. That is fine. If the T's are indeed uncorrelated. If they are not, then you have this complication here. Luckily, Python knows what raw hat is, OK? Um, So We can, we can compute this F. Um, let me see what else am I saying here. Yeah, so how do you, how do you do, how do you conduct this joint test in Python? It's as easy as that after running your multiple regression model. You use the um Fest. Method or function, whatever you want to call it, where, so you, you specified your regression model under reg one, and you, you define an object F test which takes your regression model and applies. The test method and What you're testing here is that STR is equal to the expenditure per student is equal to 0. That's how you tell Python test that beta 1 and beta 3 are equal to 0. What it then does, it calculates this number here. It knows the statistical distribution of that number. It's a chi square distribution. We don't need to know this, but luckily Python just reports the P value that's associated with that number, and all we need to now remember then is how to interpret or how to make the call on whether you reject or accept if P is smaller than 5%, then you reject. And you just have to remember, what am I rejecting here? You're rejecting the null hypothesis, but both of them are 0. Which here We do, we do reject. So the p value here is equal to 0.0047, which is smaller than 5%. So we reject the null that both beta 1 and beta 3 are zero. So long story short, your initial intuition that well the English learner percentage is a non-zero number. Sorry, was the expenditure per student is a non-linear number that actually dominates here and factoring in the covariance or correlation between the two Ts didn't change that story. Most of the times it doesn't from experience, but you will have to do that, OK. And then One F test that Python and many software packages report with every standard regression output is this one here. For the model where we included 3 slopes. You test all slopes for zeroness. And what that basically is is a test of whether your model makes sense or not makes any sense at all. You want to reject this here. If you can't reject this, then you go, what what are you doing? What are your regressors doing if if none of your regressors is uh is relevant in this sense, OK, um. How you, how do you do this in Python? Um, well, You, you do after running the regression, you use F test again, and in this case, now you, you specify all three regressess equals 0, and the P value here is even smaller. And so this is a Simple test that you can run after every multiple regression where you regress, uh sorry, where you test all your coefficients for zeroness jointly. Which is a justification that there's at least, if, if you can reject it, which hopefully most of the times you are able to do, is a justification that uh you're not completely wasting your time, OK? That would be good. Are there any questions? I see the time is, time is up, so I'll give you 8 minutes and then uh we'll continue.

SPEAKER 4
I'm taking, uh, uh, out of interest, I was taking like a finance course and I somehow realised this. This equation looks very similar to. This, have you ever seen this before?

SPEAKER 3
Yeah This I just realised that like, you know, at

SPEAKER 4
least the top line of this is the same as this. So are they using similar concepts? I'm sure they are, but like I was just wondering like if you knew anything about portfolio risk management.

SPEAKER 3
If you don't like immediately know it then probably it's I can't. Other than what you're doing with the pictures you're taking two random very well uh in our case, T knot and T3, have a correlation and and just studying the squares but this seems to be what's going on there.

SPEAKER 4
Really like if there were any sort of like variant minimization thingy here, there's no minimization going on. OK, OK, OK, sorry I can't, yeah, I like maybe that would be cool.

SPEAKER 3
Sorry about that.

SPEAKER 2
I'm with you because it's just because you can't your

SPEAKER 3
usual Friday you can't go tomorrow or today? so many things from Friday have been moved so I've had like I do STEM, so there's too many contacting me. Let me, there's no way you can attend any of

SPEAKER 2
our tutorials because we let you attend any right't have to be the one that I organised. I could show you my timetable just two weeks. I'm not totally against it.

SPEAKER 3
It's just my initial instincts to attend, but it's always the same two weeks, but we're always struggling of the semester. So let me, let me have a think and then if, if I do it, I'll I'll I'll post it

SPEAKER 2
because I'm sure a lot of people.

SPEAKER 3
That's true. I, because I feel like if I was to miss one, I know. Yeah, I know. Fair point, um. Yeah, let me think about it. I'm I'm not, I'm in favour of it some time to to go through the, the phases of what is it, denial and the negotiations. I forgot what they were like one I'll skip the

SPEAKER 2
I'm still. No, thanks for raising it. It Yeah, so Definitely the one that we intended initially. I feel like in this this case. People Yeah. Before Council Before Also covers for the other two investments. And you know it soaks up. It's all effect effect of detail, but it soaks up. the Of the two reversals because it is correlated with reversals, so I'm gonna send that.

SPEAKER 3
On the simple question, it's it's inaccurate.

SPEAKER 2
It's, let's call the bottom.

SPEAKER 3
And the ones that you then have because you have in the first place.

SPEAKER 2
So To the question is always what other interests should 3 dogs. Uh. That's, that's the question that I. Always what are we not? No. Look. Yeah. Yes, definitely I would say we've got a more accurate, um. A teacher. decreased quite a lot and turned quite significant. Um, but I didn't say, I should say in general it's not necessary alone it becomes insignificant is the case here. that's. Yeah In this case, I wouldn't think of it as

SPEAKER 3
a good thing. I feel like there should be a effect, but now

SPEAKER 2
we've thrown in the other two reversals. The effect becomes quite small and it's when we count from zero. observations the unit of observation for the school or the school district. to enrich your data. probably not this situation. I. duration. Yes. I wouldn't call it if it is what it is, the data speaks and uh. For the most honest way that. Sorry, So Uh Mhm Yeah. learner percentage for the expenditure of the students. Yeah, so what that mean. I'm thinking like a yeah. The significance of this goal is in this individual coefficient significance. Yeah.

SPEAKER 3
The. To detective that statistics.

SPEAKER 2
Yeah. That's not, that's that's straight as we can. for the day. To Meaning. But it's always just like test score and to the ratio and test score to the. Yeah, so we, we always, that's what I mean holding everything else in conversation, I forgot to mention it, but it's always. Determine the effect of the students vary two things at the same time. T In OK, it's too much to jobs. You know you don't want to deal with Changing two

SPEAKER 3
things at the same time if you could.

SPEAKER 2
I think Oops, um, we're back.

SPEAKER 3
Woohoo. Should we do this? OK, let's quickly run, very quickly run through the assignment. You'll be very disappointed at the level of detail that I'm offering, because I also don't really know anything, um, but you will help me. So this was very straightforward. Almost everyone got 10 points. OK, this is not true, but, you know, 89 or 10 points. Let's quickly run through this. This was an investigation of weight on. On earnings True. So, in the, in the Ts in computer labs, we did heights on earnings. Um, here, so nothing interesting in exercise one. what is interesting in exercise 2? Who wants to tell me? Like this, it was marginally interesting, the whole thing, but exercise too had some interesting component who wants to say it, there's no right or wrong. What was interesting in the weight distribution.

SPEAKER 2
Yeah. There were lots of very uh. Like there are a couple of really odd things.

SPEAKER 3
There was bunching, so this sounds so judgmental. So there was, there was bunching at large numbers, and I think in the raw data it was actually at 501 pounds, which you can then map to 250 kg or something like that, whatever the number was. So that was odd Uh, and that would be appreciated if people write that, but the way I know my tutors, they, they didn't mind if you didn't write this. OK, they, they, your tutors are the best. I had to tell them to subtract, that's not true, but uh they were very gentle with you guys, OK? Um. That was true. So you observed that. I was there was like a jump. Like there was almost wasn't a continuous distribution because it's discrete data, but there was this big jump. There was A bunch of people weighing all of a sudden 5001, and you, and you go, what happened there? It looked like they just combined in this one weight category which made the distribution look funny. What you can do is So from how you explained this, it sounds like you see them as outliers, right, and they could be. So an option for you would be to remove them from the data and continue your analysis, but it wasn't. We didn't require you to do that. You, you were free to do that and we definitely wouldn't punish you, but we also don't punish you for, for not taking them out of the data. So that was interesting in terms of the weight distribution. Anything else? I don't, I think that was, that was it. Um, Then you create this variable weight above. Uh, yeah, OK, that you did that. OK. And then you go, is there an association between being above, what is it? Was that the mean weight? No, that was the median. Above the median weight, is that a good thing or a bad thing for your earnings? That is exercise for now. Who knows? Who earns more above average? People or below average weight people, above average weight people, OK. Good. And I can tell you the numbers now, but I don't think I need to. Um, I just waste our time. Um, you can detect this with a T test or an exercise 5. You detect that with a regression. I have it OK. Alright. So you run this regression somewhere? What do you get for a point estimate? What are you doing? You regress. Uh, earnings on weight and the slope was 22. Positive. So that confirms your finding in the earlier exercise where you use this crude split. Um, between above median and below median, um, that. Uh, a larger weight is good for your earnings, OK? This is all very similar to what we did for heights. That's why I also feel like I can race through this. Um, So that was exercise 5. Is it, is it significant? Yeah. It's significant. OK. Good. So, Exercise, are there any questions? Did everyone find that? No Exercise 6, you, you create these predictions, you can all do that because I think we did this in the computer labs as well. Exercise 7. Now, that is where stuff gets interesting. What have you learned? So it's an open question, and I have to write, give a thoughtful discussion, um, which sort of give us, gives us the ability to withdraw. Withhold points if it was considered thoughtful. But I think again, the tutors are quite, quite generous. What is a thoughtful discussion here? What was the question? What have you learned about the effect of weight on earnings? So And superficially, Right, it's not even superficial. You can say the effect is positive. Now, let me just start this conversation. So, does, does the, do these findings mean that you should go home and eat a lot in the expectation of earning more? And if that is not the case, what is going on here? Why do we have this effect? Yeah.

SPEAKER 2
Like maybe people who can afford to like eat a lot. Like A better life like Have what?

SPEAKER 3
Like if you can like.

SPEAKER 2
Um, in terms of like the effect of weight on earnings, like if you have a higher weight, there's a chance like you're eating a lot. means like you probably have a like a high salary.

SPEAKER 3
Yeah, so wealthier people. I, I doubt that I would detect that in the data, but I think that would explain something like that. Up there. You have to fight with each other. You guys have the same answer? Yeah, OK, I like that one. So if you are young, you weigh less, which is a big. Big if. Like if you're very old, you also don't weigh a lot. But then you also don't have earnings, yeah. And I, I, I mean, I would be happy to reward such an answer. What, what is your line of thought? Uh, I said there's probably some, uh, correlation between gender

SPEAKER 2
earnings, and there's probably a strong.

SPEAKER 3
Yeah, so this is how this was constructed. I go, this comes out of your exercise from week 4, you go, you found that males. Uh, are positively, I wanted to say are positively disadvantaged. advantage, that's what I mean. They are, for some reason, males, we know that males earn more. From the truths, but males weigh more as well. So this could be an indirect male effect. This, this links up quite well with today's lecture also, right? So, so that, that's what I would have hoped, like that would be the 88 star answer the, the tutor, the tutors that will mark yours um are probably. Not even looking For all of that, but what I have in mind here is this discussion that relates to what you've learned in your computer labs that males earn more on average, and this year could be the indirect weight effect or the indirect, sorry, the indirect effect that gender has on earnings as captured through weight. Because it's uh it seems silly that we should have a positive effect. I, I think I can't, I can't come up with a causal story. I could come up with a causal story for why males earn more and the story is discrimination in the labour market, but I could. We could dig deeper, but definitely not wait. OK. Any other questions? Yeah. Sorry, I don't hear you. Is there anything to be said about a wall of

SPEAKER 2
data falling below the estimated PRF?

SPEAKER 3
Oh, the, the, the data is a bit funny, right? Um, it's, it's uh discreet in both directions. It's not continuous. And we just ignore that. But you can, you can write about it, yeah. Uh we, we, yeah, the, the, if I look at the scatter plot here, yeah, you, you've got these, you've got few realisations in, in the, in the horizon in the vertical direction in the horizontal. And then a lot of the data falls below the estimated PAF, which is uh an artefact of, of this particular data. And you, yeah, you can throw in a discussion on that as well. OK, that's, that's really all I wanted to mention. That's the, that's the general feedback. If you want more specific feedback on your own solution, then talk to the, to your tutor, OK? Now, just give me a minute or 10. To hook up the iPad to uh. Um To the Wi Fi. If I remember what to do. Oh, it works. right Again. Again This is not efficient Equations. Today is ugly. It's it's going to be a bloodbath. That's the mathematical term, but we, we can do it. I'll just have to find my pen, OK. What is going on here? Can you guys see? Yeah, I assume you can. So here we have multiple regression. Oh, why is it not? Here we go. Multiple regression, no constant, no beta knot. We've got 2 regressors. OK. Write down the least squares objective function. Let's do it. Part A. Uh, did I call it SSR or RSS don't really matter. You can call it whatever you want. Um, but I want to acknowledge that there is a function in two variables. Call them. B1 and B2. And then it's just this. And you must please help me when I make mistakes. OK That's, that's the objective function. No constant here. Obtain all partial derivatives, and then get the first order conditions. I think we're doing B and C combined. Um So All right. So I'll do, I'll do the first order condition. So I take the derivative with respect to B1, and since we've done this before, we go. -2 Some And then if this is the derivative with respect to B1, I have to multiply the thing by. X1 I and an aesthetic equal to 0. That's what I call the bees beta hats, so I can change that straight away. Oops. There you go, so. I don't want that. And now I just Divide both sides by -2. And now I copy and paste. I duplicate this, so let me move. So it's And then I want to shape this second one into the first order condition that comes from the 2nd partial derivative. What do I have to change? Here it is an X2. Here Is that all? True. I sense don't. You're not touting? OK, good. Let's call that by majority vote. If you don't stop me, that's majority vote. We'll continue. Um Does that part B and C? Yeah, there are no other first order conditions. So part D is a bit weirdly formulated. What I want to say, and this, this is the ugly exercise, is solve for beta one hat and completely fully solve out. It's clear that we can write beta 1 hat as a function beta 2 hat, and that's how we will start. But then we want to plug beta2 head in as well so that we only have uh X's and Y's. In there. So let's do it. So we're taking, so this is part. We had, what we're doing? We're taking the 1st, 1st order condition and we isolate beta one hat. And we've done this plenty of times before, so we do this in, in one big step. So I'll go. So let me write Uh, it's called this actually. Because this year 1 and this year 2. From From one we get. That beta one hat is equal to. So I I take, what do I take? The XI, sorry, the Yi term that multiplies X1I, so I get. Some of X1IYI. So what I've I've been dealing with is. This guy here that I circled in green. And now I have to deal with uh this guy here now, the other guy in green. And what are the signs? They're all positive, right? Yeah, no, I say, this would be negative. Minus beta 2 had sum of X1 X2I, and I have to divide by something. Some of X when I squid. Is this correct? Just we can do like a little reality check. If this was a simple regression model, beta 2 had was 0, it wouldn't be there, then this formula would collapse to something that looks all right, namely, This bit here. OK. Now, the complication stems from the presence of the second rea, but we've handled that. All right, I believe. Please tell me if I'm making a mistake, because sometimes I do make mistakes. All right. So now what do you do? I think in anticipation of what's ahead, I will. Um, actually rewrite it like so. How do I move this? Same, right? And now Exercise is not done because we need to plug in beta 2. Where does beta 2 come from? From our 2nd 1st order condition. By symmetry. Um, you can copy and paste this one here. And fiddle around with it to make it right. OK. This is supposed to be for beta 2. So beta 2 on the left hand side. So we've got a 2 here and a 2 there, a beta 1 here. And To there. I'm just using symmetry as an argument. Why would it be different? Any questions? Now we have to plug. Unfortunately we have to plug this one. In here, oops. So that would be Annoying A good way of doing this? Well, let's do it. All right. Then They don't want hat. Becomes first, this guy here. And then I'll just go for the 2nd term that I. Circle in green here. I'll start a new line. And I go. That is Beta 2. So I've just copied beta 2. And this was minus beta 2, so there's a minus here. not in green, maybe, minus. What I have written here. Is beta 2 And beta 2 multiplies. This thing here. Does that all make sense? Yeah. I need some reassurance. I'm sweating here. It's just uh Uh, it's just tedious. Hopefully not too difficult. Um, we gotta be careful with our sign. So what do we do next? No. What do we do next? So we go. Mm-hmm Let's let's separate on the right hand side terms that belong to beta one hand and terms that don't. So we go. That Is equal So we still have this guy here, that's, that's safe. And then we have This guy and, and that guy here. Combined So let's add to that. And that is a negative sign. So So we have oops. Most luckily I can do most of this by just copying and pasting. So that was a negative sign. Why is this not this colour? Times. This guy here. And then the terms that belong to beta one, that is minus minus so it's a plus. Peter on hat. And This guy here. Terms Oops. That go there. This has a, an easier. Looking version, this factor here, because in the numerator, you have the same thing, right? So it's the numerator squared. So I can rewrite this as. This thing squared if I erase this year. And at the bottom, you have Yeah, where's my pencil. It doesn't look ugly now, but it is correct, OK. To make it look a little bit better. And then we put parentheses around it, just 2. To be safe All right. Now what, what do we have to do, we have a beta one hat on the left hand side and on the right hand side. So we move the right hand bit over to the left hand side. That's what I would do. And so we get on the left hand side. We get Beta hat. 1 minus. Now let me write it to make it look better. So we've got some of X1I. X2 I squared. Over Sum of X sorry, it's the wrong parenthesis. Some of X1 I squared, some of X2 I squared. That's on the left hand side. That's a very big parenthesis. And on the right hand side we have still this. Can we simplify that? For the right-hand side, let's put this on a common denominator. So what I'm circling in green now. Let's give that a common denominator, so that's the right hand side is equal to. So the denominator that I'm looking for. Is Some of X1. Uh great. So these squids upstairs. Uh, here So we've got some of XY squad. X2 I squared. That's what I want on the right hand side. So what do we need to do with the first bit? We need to multiply the first one by X2 I squared. So we get Sum of X2 I squared here and here sum of. X 1 IYI and we subtract of minus. Um Sum of X1IX2I. Sum of X2IYI. That correct? Deep breath So we can put parenthesis here just. Because now. What's What's still missing? On the, on the left hand side, we can make a common denominator. As well. By going, so let me just pull this apart up here. By going So This Oh sorry, and there was. was their mors Was it a 1 minus? Yeah. Did you see what I just did? There was a 1 minus there. And I gave it the same denominator. Why am I doing this? Because now I'm seeing on the left and on the right, we're dividing by the same thing. Is that true? So we go These guys here. Kill these guys there. And all of a sudden, It collapses, simplifies to something. Relatively easy, not, not easy, OK. So, I'll go, continue, continue, continue. Um, copy and paste. So this has, this has gone here. But what Actually that's what I want, I want to know, so therefore. Beta one hat equivalent to beta one hat is equal to. The numerator of the of the right hand side divided by the numerator from the left hand side, which is this going here. This Is it? Are there any mistakes? First of all, Does everyone buy this? Hm I think it works. OK, let's tentatively assume that this is correct. Was uh was ugly to deal with. On the right hand side, there's just data X1, X2, and Y. So that's it, that's the final result, unless there are mistakes when then we would have to think harder. Um So Yeah. top end of exercise. Uh let me scroll up. Yeah. So we have now derived beta one hat, and now comes an interesting exercise. What if The sum of X1 X2 is 0. In our formula you'll see. A lot will go away. So that part. Some of X1 X2I is 0. Implies that beta one. If you have a look, so if I Let me just copy the equation. What happens here is that This year is 0. This year is 0. And then this guy here in different colour. Drops out And that gives the result in the stated in Part E. It is just the sum of X1 YI over the sum of X1 squared. Well, what has happened here, big picture, I mean, it's a Annoying exercise. The hypothesis here part is Some of X1, X2 is 0. Conceptually That It's close to the idea of them having. A zero sample covariance. If the two regressors have no covariances in the sample, The lesson learned here is that the coefficient estimate of the first regressor. Is equal to? The simple regression. Uh, estimate. Cause, so So let me just write it, so this is sum of X1IYI over sum of X1I2 which looks like. Looks Like Simple regression. Estimated that's the simple regression estimator that you obtain in the simple regression where the constant is zero. So the big picture here is And relating it to the lecture is, um, why do the coefficient estimates change when you move from a simple model to the multiple model, because the regressors are correlated with each other in the sample. So the coefficient estimate on the student teacher ratio changed when we added. The the other two regressors. That change happened because the regressors are correlated with each other. If it so happened that they weren't correlated with each other. Then the coefficient estimate on the student teacher ratio would not change. This is what we discovered here. Does that make sense? Um Almost surely they will change, OK, um numerically, when you move from a simple model to a multiple model. Because Any two variables will always have some correlation with each other, and in most regressions that you're running, they actually will have a meaningful correlation with each other. So as soon as you expand or extend your model, you include more regressors, you add regressors, the regressors that were included initially their coefficient estimates will change because regressors are always correlated with each other, and that is. One way to to show this mathematically, OK? Any questions on that? That's a conceptually an important lesson. That um Um, why does the coefficient estimate change when you add a regressor? Because the regressor is correlated. Or the regressors are. OK, we have more to do, unfortunately. So this was part E, part F. So this is a different model now where you have an intercept, show that the intercept. estimate is this this is a relatively easy exercise. It's sort of a warm-up exercise for part G. Let's quickly do this. So, oops, I don't want to do that. So, go back, go back, doesn't want to go back. OK. Um, where are we? Part E, are we part F maybe? So all of a sudden now, our objective function has Be not as well? And it looks like this. I minus minus B1 X1 minus B2X2i squared. Let's take the derivative with respect to the constant. So all of a sudden I'm adding a constant. Take the first order condition with respect to the constant. Gives you So we don't need the negative 2 because we know that drops out, so we get um. That's all. And now, Solving for beta not hat. Or what you can also do is you divide both sides by n. We want sample averages. It follows that. Beta not had is equal to Y minus beta 1 had X1 minus beta 2 had X2 bar. Does everyone see that? Mhm How do I get this? I take the first order of condition and I isolate beta not hat. And I divide it by 1 over N here, everywhere. Um, just get sample averages. So it was an easy one. The last one, Now, in the last exercise. Um, I'm asked to derive an expression. Forbade our one hat when again, Oops. And this is really the sample covariance between X1 and X2 is 0. OK, this is just a more general result than the one that we obtained in part E. So let's get the first order condition for beta 1. And plug in the zero covariance condition. So the first order condition. With respect to beta one had. Is well it's really. Almost this one here. Duplicate that Let me move the one over and away. And the first order condition, oops. We had that before, but we didn't because now we have a constant as well. Beta 1 I is equal to 0. Now what we do is We plug in beta not hat. Sorry for racing through this. It's a bit too much, isn't it? We get Why I minus Yar. Minus 1 head, X1 I minus X1. Minus beta 22 hat, X2I minus X. To No space I need some space. Yeah, here we go. The whole thing was. Multiplied by X1 I. But I can also do this one bar. Why is that? Because everything in brackets is 0. OK. So If I isolate Sorry to rush this. Be one hat. We get We get some of X1I minus X1 bar. Why I minus Y bar. Over some of X1 minus X1 bar where I already factored in that. The covariance, the sample covariance between X2 and X1, by hypothesis in the question was 0, so that drops out. So that is slightly more general result than the one we obtained 2 or 3 questions earlier, where we didn't have a constant. But the bottom line, the lesson is still the same, that. In the multiple regression model, When here with two regressors included, when the two regressors have a zero sample covariance, then you might as well run. Two simple regression models. The regression of Y on X1 will give you exactly the same estimate for beta 1 hat. Or the regression of Y on X2 will give you exactly the same estimate for beta 2. OK. Or thinking about it from the other direction. Why when you move from a simple regression to a multiple regression model, does the coefficient estimate change? In general, it does change unless the regressor that you're adding has a zero sample covariance with the one that you had already included. OK? In general, so the the condition going back to the very top, the condition here. Let me highlight it. This is the last message. The condition here I'm I'm highlighting in green. So if I divide it in by end in front of it, that really means the sample covariance is 0. For any two regressors, that is almost certainly never the case. Two regressive Could have An almost zero sample covariance, but probably non-zero. As soon as you have it non-zero, then as you move from the simple regression to the multiple regression model, you get different coefficient estimates. They will and the change depends on. The sample covariance between your two regressors. OK. Said enough, sorry for racing and pacing um through this. I hope it makes sense, um, to you guys. Um, yeah, that's, that's all for now. Um, have a good Easter break and I see everyone again next Wednesday. Sorry, brain freeze. So, see you, see you next week, hopefully.
