SPEAKER 0
This material has been reproduced and communicated to you by or on behalf of the Australian National University, in accordance with Section 113% of the Copyright Act 1968. The material in this communication may be subject to copyright under the Act. Any further reproduction or communication of this material by you must be consistent with the provisions of the Act. Do not reproduce this material. Do not remove this notice.

SPEAKER 1
OK, so we. Should we get started? Week 6. Welcome to week 6. How is everybody feeling? We sixish. Who has not submitted their assignment? Who's brave enough to admit it? OK, good, good on you, good on you. Um, how was it? Doable, yeah. Good. Was, was it very similar to the week for computer lab? Yeah, it, it looked, I just looked at and it looks like the overwhelming majority has submitted already. I think there's almost 3 more hours left, so. Anyway, um, we'll mark them in the break and return them. Return the marks to you at the beginning of week 7, so in 3 weeks, um, and then you can negotiate with your tutors. Not with me. OK, talk to your tutors. Um, we'll also, um, put together indicative participation marks, OK, for your tutorial participation so far. That's a number between 0 and 5. I think it's very difficult to get 0, OK, so we'll post those early next week and then we can talk about them in week 7. Are there any questions about course? Logistics at this point. Yeah. Who's going first? I think it's So I don't hear you. Oh, really? What have you done? When, when was this? I don't think so. So you've, you've been going to your tutorial and you've been going to the same one. who cares? I mean, yeah, the tutor knows you. Hopefully And the enrollment in the beginning is just to stream students so that not everyone goes. To, I don't know which one is the best time slot. Thursday 11, I don't know if you know if that is a time slot, but it's just to stream kids, not kids, you guys in, in the beginning, and after that. There's not a lot of shuffling, so uh uh formal enrollment is not really. Uh, required. Was that the same question or? Oh Have a, um, holding the sheet. Do we get to make one No, I'm very old fashioned. I'm sorry. Um, yeah, I know. So the, the instruction for the exam is very simple. No permitted material. That includes a calculator. Nothing. I mean, a pen, right? But even that, I think they can give it to you. I mean, we give you paper. What is it all a script book, maybe cried paper, but other than that. Scary I'd say any of the derivations we do here. Our fair game Wow. Scary, I know. I, I tell my graduate students the same thing and it's like 3 times as much material, so. It doesn't really help you, I know. But, um, but as we approach the, the end of the semester, which we're not currently, um, I'll fill you in on that. There's a practise test that we work, work, walk through, and And Yeah, but I, but I, I don't think really that is. Um, well, it's, it is a restriction, but it's a sensible one. I want you guys to be able to derive stuff. But things like, so I wrote down the, the formula for the OLS estimator here, because, um, some of your classmates asked me a question at the beginning, so I just wrote this down. So that that's a formula that you should internalise, that is memorise. If you can't, then you have to be able to derive it. What is easier to remember. I mean this is the shortcut version, but so there's a few formulas that over the semester you recognise you just have. To have available, but then there's some derivations as well, but it's, yeah. It's modelled on what we do here. Yeah. OK What we do in the workshop. Yeah, so that's what I mean. Everything that we do here is fair game and here is um the lecture material and the workshop derivations. I mean, there could be. Questions straight out of the workshop. And I would consider that low hanging fruit. And In in the sense that I mean you should prepare for it. I, I understand you have competing interests, so you have to study for 3 or 4 exams at the same time, you have trade-offs, but if I ask you, what did we do last week? Did we, did we prove or or not prove that the estimate of the constant is unbiased or biassed, there was a bit of a nuance there. I could ask that in the final exam. And then hopefully for 60 or 70% of you, that's like, OK, I, I recognise this I can do this, and 30% maybe struggle for whatever reason, but um. But yeah, you'll, you'll see. I mean, I'll, I'll, I'll show you and it's. It's a combination of easy exercises, I hope, from your point of view, and then there's there have to be a few challenging ones in there to create a distribution that um reflects. Students on different understanding of them, differing and vary varying understanding of the course material. But it'll be much clearer. We're in the middle of this semester now. Let's uh maybe Uh, not relaxed, but you know. We're taking a break soon, but uh for now, we'll work through this material, and we'll worry about the final exam. At that stage, any other questions? OK, so let's do it. Week 6, we're in the middle of the simple regression model. So that's the one where we have one regressor. After the semester break, we throw in. Extra regressives and augment the model. For now, we just run through a few selected. Topics that are not sort of, Building on top of each other, but the sort of random topics that that I want you to be aware of. The first is measures of fit. So there are two statistics that people look at that explain how well the regression line fits the data. Um, one is, and that's the more prominent one, R squared, and the other one is the standard error of the regression. The idea is roughly, it's, these are, uh, is it the same dashed line maybe? But if your dots are the ones on the left-hand side, you would say your regression line explains the data better. I mean, it's just intuitively, and better fit in the left picture, not so good in the right picture. And If you think about what, what creates that difference is really how, how far the dots are away from, from the line. And the vertical distances are the residuals. So it will have to do, it has to be somehow related to the residuals and let's have a look. So let's quantify or let's make this more objective rather than looking at pictures. So the concept of the regression R squared, the idea there is To study the The variance or the sample variation of the dependent variable and compare that. To how much is explained by variation in the independent variable in X. So in formulas, you define these, these three objects. One, the first one is called the total sum of squares that compares YI to its mean and squares it. So that here If I had a division by N here, which I don't, that captures the sample variance of Y. If I, if I divided this by N, then the thing would be the sample variance of Y, OK. But it's not divided, but it doesn't matter, it captures the notion of variation in the dependent variable. Then This thing in the middle is called the explained sum of square, it's mathematically defined as Y hat minus Y bar. So that is like saying if your dependent va wasn't Y but why hat. What's, what's that object's variance? If we had a division by N here, I'm just trying to give you an intuitive notion of what that is. So if my hat is the variable of interest, you compare it to its mean, you square that. And sum that out. If you then divide it by N, which we're not doing here, but we could in our heads, and that's roughly the sample variants of Y hat. And but it's called, the object is called the explained sum of squared. The intuition is what variation is explained. By the regressor because white hat is basically derived of your regressor. It's the predicted value of the dependent variable. And what is the prediction based on? Basically on your estimated regression function which comes from the regressor. And then what's left between Y and Y had the residual. So the third concept is the residual sum of squares, which is quite literally. In the parenthesis here, the residual, right? Y minus Y hat is called UI hat. I should, I should have written this here maybe, but I can, I'm telling you now, that is, of course. Squaring all residuals and summing them up. And what's missing here, it's not missing, but it's never done in these definitions, division by N wouldn't be wrong, but the way these things are called, like the top thing is called the total sum of squares. It's not saying the average sum of squares, so that's why it's not divided by N. Nothing here is divided by N. What you can show is you can decompose the total variation in the dependent variable, the TSS, into a variation that comes from the explanatory and variation that comes from the residuals. We'll do that in the 2nd hour of today. It's not that difficult. So we can show that TSS is equal to ESS plus RSS. Now, with these concepts defined and sort of understood, We define our squared as the ESS over the TSS and the idea being Well, the ESS captures how much of the variation of the dependent variable is explained by the independent variable. And then we just make it a ratio of The total variation on the dependent variable. So we divide by the TSS. And with the result, uh, from the previous slide where you can decompose the TSS as the ESS plus the ISS you get also this result here. So that the definition is upstairs, and then this is just a rewriting using the decomposition of the TSS. So, so, and what, what values can you have or what values are interesting to study here so. If R squad is equal to 1, going back to the definition, it must mean that the ESS sorry. Let me restart. If the R squared is 0. It must mean by the definition that the ESS is zero. So, that would be a case where the regressor does not explain any of the variation in the dependent variable. Bad regressor. OK. It's almost impossible, like even if you have a city regressor and there's always gonna be some non-zero. Uh, ESS. But um, just playing around with the sort of corner values that the R square can take on, R square of zero would mean ESS of 0. In intuition of that is your explanatory variable is bad. R square of 1 in the definition must mean that the ESS is equal to the TSS. So you explain everything with your regressor. Which seems odd, like using the dependent variable as the regressor. You can't do that or you can, but then you sort of chasing your own tail and it doesn't, doesn't make sense, right? But that would be that that extreme value for the Rsquare. Um, so in that, in that case, you would be explaining everything, but that is just, um, Um, A theoretical consideration. So we, we see from these two extremes what that R squared is bounded between 0 and 1, so that's convenient. You can From Different regression models and compare their squares and then say you can go model shopping and go this model here has a higher square, so I prefer this model or something like that. And this gets more interesting when we do multiple regression and the question turns on or turns to which subset of regressors should we be including, um, so we'll revisit this later in the semester. And then there's a random fact for the simple regression model, which we're dealing with right now. So regressions with a single regressor, are square. Happens to be the square of the sample correlation coefficient between X and Y. That that can be shown, but it's not true in general, so it's just uh. A random nugget here, which is happens to be mathematically true. And when we do multiple regression, then that is not true anymore. And Python routinely spits out an R square according to this formula up here. And then a related concept which Um, is related to the RSS from the here's the, the link to from the previous slide is, well, why not just look at one of these concepts and One of these concepts is the RSS. The idea based on the pictures that I showed you, let me go back. Was that what, what, what explains the difference here? I said, well, it's really the vertical differences to the, to the line. So what captures that? That's the residual. If you think about, if you think about it, that's really implicitly done in the definition of the um of these guys here, TSS, ESS RSS and how we define our squared. But another way of looking at this is by defining a simpler statistic. Which basically, which formula should we look at, which basically tells takes your residuals and squares them up. You divide by N or something like that and you put a square root over it. That is If you link that back to concepts from 2 slides ago, 2 slides earlier, that is the RSS over some division by N. N minus 12, or I just and who cares, but um. So it's not an entirely unrelated concept to R squared. It uses. One of the ingredients, which is the RSS. The more common one to choose between those two, R squad for the standard error of the regression, is, I believe, the Rsquared. I guess people like it that it's normalised between 0 and 1. But they, they both captured the same idea. How far do the dots spread away from their estimated regression line. All right. OK, I think that's all good. Any questions on measures of fit? We'll revisit this when we talk about the multiple regression model. Another selected topic, topic binary. Regressors. Quite often, explanatory variables are binary. So for example, um, so the, the model of the, the example that we used so far was always. Number of students in a classroom, and the number was 2021, 22 students, something like that. But it could also be more coarsely defined as large classroom, small classroom, what am I saying here? So I could define a binary variable that is equal to 1 if the classroom is small, and 0 otherwise. And then the question is, where's the cutoff, but. Uh, I don't know, could define. It would be small if it's less than 18 or something like that. I'm just saying you could have a, an independent variable that is binary, taking on value 1 or 0. Or um. It's equal to 1 if someone identifies as female and 0 otherwise. Or which we look at now. is equal to 1 if a person reports to be a smoker or otherwise. In all of these examples, the question is always like there's an extensive and intensive margin. Like what do you mean smokes, smokes 1 cigarette a day, 40 cigarettes a day. Implicitly in all of this is a choice there. OK. This could be smokes at all, meaning. Smokes more than 0 cigarettes a day. But it's, it depends on the survey and how it's run, OK? Um. Sometimes in the household day they ask people, are you married or not? Maybe it's a bit clearer, I'm not sure, unless there's. There's variation in how one interprets to be married or not have children or not. Uh, I think that should be relatively easy. Everyone here could say 1 or 0. Do you have children? I would have to say one, and then most of you probably say 0. OK, but that's a binary. I mean this is different to asking how many children have you got for you that's almost the same as the binary, your binary outcome for me it's not, OK. So, but you get the point in many applications and you'll be seeing them too, you have explanatory variables that are binary. And the convention is to make them 01 binary, not 2 and 45, or you could do that, but there's no point in doing that. I mean there's no gain from doing so or it's no loss of generality picking your numbers and 01 is is the easiest to deal with. How should you interpret a regression when the regressor Is not continuous but binary. Why I'm asking this question is, so far our interpretation of B1 or beta beta one or its estimator was, Uh, what was the interpretation of a slope. OK. Implicit in this is the differentiability of the PRF. You go, well, the continuity of X, but Here it's not continuous, it's 01. So does that make sense? Mechanically you can run a regression like that, but let's take a look at how you interpret a regression like that when X is binary. So you can still write down your simple regression up here, but be mindful that X now takes on two values only 0 or 1, so. For those of who, if it's, do you have children or not, for those of you who don't have children, your regression function is the top one. Y is equal to beta not plus UI. For me, my regression function is Yi is beta not plus beta 1 + UI. So I'm getting and beta 1 extra. Beta one could be negative so it really depends. Um, so. It splits the population in two. That's what I'm trying to explain here. Your value of X determines which population you belong to. If X is not, do you have children, yes or no, but it is, do you smoke? yes or no, then it splits the population in two. Are you a smoker or are you not a smoker? OK. And that's the example that we look at in a second. And that's the same here for the, so upstairs is the uh what is this, the linear model or the, here the population regression functions. Um, so when we studied beta one in the past, we understood this to be the derivative of the PRF. Now it's the group difference. OK, that's the only um change here. So, beta one captures the expected value for the people who are in the first group, minus the expected value for the people who are in the second group. If we're talking about people, OK. So it does make sense to use binary regressors, just be mindful that the interpretation is not that of a slope, but of a group difference. So here we're using our T test function. Um, that you would have seen, I think in the week 4 Computer lab. And I'm using a data, have you used this data set? I don't think you have the birth weight data set, so let me quickly explain the data set. We have Um, moms in there who had babies, and their babies' birth weights were measured. And mom's smoking status was recorded at binary. Don't ask me exactly uh what the cutoff was, I'm guessing it was, uh, the question was, um, so there's a variable called. In the data frame, smoker. Smoker, which must mean you smoke more than 0 per day. And this is all self-reported, but let's ignore strategic reporting, stuff like that, but let's just take it at face value. So there's smokers in there and non-smokers. And so we're feeding the Test function, the birth weights of the smokers, so the moms who smoked while they were pregnant. And moms who didn't smoke while they were pregnant and we compare the group averages. So let's just using the Test function on the next slide, I'm giving you the regression. So but the bottom line is this you've got two group means for the For the moms who reported. To be smoking, the average birth weight. For babies born to them is 3,178 grammes, and for moms who didn't smoke, it is 3,432 grammes, and the difference is 253 grammes. Um, at this stage, I'd like to say this is like one of the few times. In life when higher weight is considered to be a good thing. Um, although, I mean, I'm just wanted to sort of frame your thinking. The high weight is an indicator here, higher weight of health, of of better health status, OK, of a proxy. And, and then there's a confidence interval. Um, what I want you to I more here is the point estimate for the difference in mean is 253. When I run a regression, I'll get exactly the same point estimate. Let's, let's interpret what happens in this regression. Um, what we're doing is we, we're using, uh, what are we, what are we using? The same data set, we use the dependent variable birth weight on smoker status, and smoker is Coded as being equal to one if you're a smoker, that's the convention. And, and the regression, anything else funny going on there? Not really, it's relatively straightforward. It's a simple regression of birth rate on smoker, and I don't know why I highlighted all of this. What we're getting is. An intercept estimate and a slope estimate that isn't a slope and the interpretation is this If smoker is the dependent variable that defines the benchmark group. Um, let me see. The benchmark group being Actually, yeah, let me slow down here. The benchmark group is sort of the, the converse or the, the opposite of what the Dummy variable is coded as. So if the dummy variable is called smoker, you can read off the intercept. Or you can read the intercept as the birth weight that applies to the non-smokers, OK? And that's exactly that the number that we found from the T test. So what you see, the intercept captures the average birth weight of babies born to moms who reported not to be smoking. And the beta one estimate is a negative 253. That's the adjustment you have to make relative to that number, to take them to the level of the other group, which is the smokers. OK. So that's what we saw if I go back to slides. Beta one is the adjustment that you make. When you are in the X equal to one group. Here X is you are a smoker. So the adjustment that I have to make here is subtract or decrease the number by 253. OK. Um, Anything else that was all that in the. Any questions on that? So, also comparing the regression to the T test or for that matter. You open up Excel and you look at group averages. The what I'm trying to say here is with the regression, you can do exactly the same thing. Capture exactly the same information. And then do all your regression-based analysis that is based on standard errors. So you can do a confidence interval, a T test, you can use the P-value, whatever, to make a judgement on whether an effect is significant or not here. The effect is sufficiently so if we take the T test -9.396 is sufficiently far away from 1.96 to conclude that this difference here, this group difference is statistically significant. So, um, if you report to be a smoker, you can expect a lower birth weight, at least that's the statistical implication here. All right. Next Selected topics, next chapter. So we're moving away from an empirical example, we're going. To study, um, well, we go revisit the Gauss-Markov theorem that we have seen for um the sample average in the univariate. Case in week 1 or week 2 was week 2 I think. And there's a G Markov result. For the OLS estimator. Let's have a look. OLS estimators, uh OLS estimator is not the only estimator of the PIF. Like I say, for any estimation problem, there's infinitely many alternative ways. So, um, we could. P An objective function like so where P is whatever number you want to choose. And go and solve that it's differentiable hopefully and you get you get a number, but you could also go, well, the idea of running a regression was to put the line so that the residuals, the sum of square residuals is minimised. Why sum of squared residuals? Why not absolute values? That's what the middle estimator does. It's called the least absolute deviation estimator. It's also not a bad estimator. It's analytically not so easy to, to deal with, but people use that one as well, and software packages such as Python or data can estimate this one for you as well. But the point that I'm making here is really, Anything goes in terms of estimators, like here's my favourite again. So you just put a line in there that is flat at 42. That's an estimator for my PIF. It's not a good one, right? Um, so there's infinitely many estimators, and the question is which one is good. And we define goodness by these two properties again. So we've seen this before as well. We use bias and variants. This is a reminder from week 2. We have an unbiased estimator when the expected value of the estimator is equal to the thing that we are trying to estimate, and minimum variance. Is uh efficiency is defined to be an estimator that is minimum variance in the class of unbiased estimators, and we're saying this here. Um, I'm not saying it's. OK, if I don't say here, I think I must combine that in the Gauss-Markov result. And so going back to my slide here, why, why I just hesitated for a moment here it's easy to choose the minimum variance estimator, that's this one here, OK, that has zero variance. Um, so it only gets interesting if you're restricted, if you make it a horse race that is fair, OK. So this guy has variant zero, but it's very biassed, OK. So we need to combine these two notions and that's in fact, um. An accomplishment of the OLS estimator that in the class of unbiased estimators it will have the minimum variance and that's exactly what we're looking at now. Just one detour is we're looking at. Estimators that are linear in Y and our estimator is of that shape. So here Beta one hat, luckily I wrote it on the whiteboard, is linear in Y. And then the, the weights, well it's called AI upstairs there, is, is this funky function with the summation of the Xs. But it basically takes the YIs. And multiplies them by some weights, AI. The AIs are weights that come off the. Um, regressor. OK, so this is indeed the structure of our OLS estimator, as you can see on the whiteboard. And then we call an estimator blue if it's linear, unbiased, and has minimal variance. So when we did univariate statistics, we discovered that the sample average was indeed blue. Currently we're doing bivariate statistics, so we've got two random variables that we relate to each other, X and Y. We use OLS in particular beta one hat. And the result is um this estimator that we defined. Like so 2 weeks ago is blue. Um For beta 1 hat, hopefully that's not too surprising. For beta zero hat or beta not hat. Just be mindful of what we learned last week. In the workshop session where we go, uh. We want to assume that the errors have zero mean. In that case, beta hat is indeed unbiased. OK, so that's how this is meant. OK. So this theorem provides a good justification for using OLS rather than the other options that are offered. Um, This holds only for a subset of estimators that are linear in Y, which doesn't have to be bad, but there might be nonlinear estimators that are better, but that's for a more advanced econometrics course to to look into it. This is already a pretty, pretty powerful result. This was under our 4 assumptions. In particular, the homo skelasticity assumption for a that we now, I think. Are we moving away from it now? Yes. So any questions on this result? So Yeah. But if we've got a plastic thing. Is it still like This all I it is, and I'll show you this now, but this result does not hold anymore. But there is a Gauss-M Markov version for the Hetruskilaty case, but we, we won't um show that. Um, but yes, sorry. So. I'll give you a guideline for what you do under heteroskeleticity. Um, unfortunately, strictly speaking, Gauz Markov doesn't hold anymore, but there are no, still no better alternatives available. So let's talk about homo skelasticity versus heteroskelasticity. So OS assumption for one was this assumption where we said conditional on X, the variance of the error is. Just a fixed number. It's not, it doesn't vary in X, that's the point. In a picture I show you now, situations where that doesn't hold. Here it does hold just visually, the vertical variation of the dots doesn't change as X changes. In this picture here, Uh, where is it? It does change. Do you see that? The variation increases as exchanges. Here, the, the variation, the variance of, um, I don't know, is this the PRF or the estimated PF? Whatever the vertigo differences are, the residuals or the errors, the variation increases here as X goes up. But it could be the other way around. I mean this is a poor picture, but here's the other way around. Here you've got higher variation at the beginning. This is overall a good fit, but you see the variation, the variance of the errors or whatever the residuals is high up at the beginning for low values of X, so low values of X, but it becomes smaller and smaller as X increases. But it could also be varying. Here I'm suggesting that you have high variance at first, in the middle, not so much lower variance, and then it spreads out again. In general, we want to be. Able to cover any of these situations. I want to say homeless get well. Put differently, Homo klaicity doesn't allow any of these three pictures. This one here, that one or that one? Um, so we want to Uh, going to go away from this and These 3 cases are Called or what could be defined as non-homoscetastic, but there's a different word for it. It's called heteroscetastic, OK. And um so let's modify our results to allow for changing error variants. What do we need to do? So first, as a writing exercise, it looks innocuous enough, um we want to let The errors, we want to allow the errors to be heterostastic, so this is my clumsy way of saying the variance of the errors is now allowed to be to be a function in X, that's what I'm trying to say here with the right hand side. Um, not a very, uh, not formulating that well, but I, I think you get the idea on the right-hand side. It's not just a constant, it's a function. The function is called sigma squared, and that function can vary in X. And it could be any flexible function in X on the right hand side. And then if you go, if you think about it, that's not really an assumption, that's like a non-restriction. So it's a misnomer to call it an assumption, but I call it an assumption anyway OK. And What was an assumption was what we did last week when we said almost getasticity. That is an assumption when we go, the error variation is. Unchanging, it's fixed. Here, it's not really a restriction. OK. Any questions? Now. The mechanical implications of this have to do with how we Derive the distribution of the OS estimator. Let's have a look. While we assumed Thomasketasticity is, it makes the asymptotic distribution result where we invoke the central limit theorem. Much easier to derive. And we had obtained this clean looking result. Last week, OK? Under heterosky, I'll be Trickier and where will it become trickier? You don't just have a constant error variance up here, and that that really all there is. I could, I could change this slide and just say, just put in a more complicated error variance here, and that's what the next 2 or 3 slides will do. Um, so let's, let's, here I'm saying it's tedious, but it's actually not tedious because in a way, by making The setup more general, the, the math becomes actually so much more tedious that I'm not exposing you to it. I'm just offering you the result. So let's, let's have a look. And why did we use, so this is from last week, so don't, don't get a fright. Um, we've worked through this already. I just wanted to remind you, where was the Homoketicity assumption useful last week. Uh, it was useful here where we could just, as we move from line 1 to line 2, we can just plug in sigma squared, sigma Usquared. And then everything else just goes down to the, to the result that you saw on the previous slide for the asymptotic variants of the slope estimator. So in a way, What I'm trying to say now in the next few slides is, well, you can't do the move from line 1 to line 2. You have to stop in line one, that's, that's really all there is. And allow this variance to be A flexible function in X. So, that's what I'm saying. So we were here with line one and so this is all a little bit sort of um. Approximation sort of something in intuitively what we would expect. And all there is, all this does is, well, you have the, the variant of the errors given X, but we said we can now not assume that they are just a constant. So let them just be what they are, or, let that variance just be what it is. And all I'm writing here, I just. Try to be consistently in line with what the book does, as well, I can't do any simplifications here, um. So what I'm doing, uh, I'm doing an approximation first year for the Uh, for the denominator here in. Inside this the outer parenthesis looks like the Sample variants. Do we have it on the whiteboard? No, yeah, here, down here on the whiteboard. Can you guys see this? But one bit is missing, there's a division of N that's not there. Right? So, what I'm doing is I divide by N and I multiply by N and I replace the sample variants with the population variants. That's this guy here on the whiteboard. It comes in handy. Um, but I just call it sigma X squared. And you've got the multiplication by N because Here I divided by N and multiplied by N and then I subbed in. I moved from the sample concept to the population concept. That's why I use the approximate equality here. So everyone sort of OK with what's going on here? Yeah. So I, I took care of the of the division by sum of Xi minus X 5 square. And for this long summation. Of variances there. All I'm saying is we don't have a prior on what this variance is, how it changes in X, so let's just let it just be the variance of Xi minus mu x times UI, where this is basically a placeholder for. Um, The variance of the error term as a function in X, OK? Um, and then what's happening in the 3rd line? Nothing really. I just, um, yeah, just uh. Multiplying out this Parenthesis here, and then that's what we get. The, so if we. Yeah. And then I collect the results. So the central limit theorem now under heteroscatasticity, so invoking assumption for B, which wasn't really an assumption, allows the variance. To be flexible and in a way, the asymptotic variance then becomes slightly more complicated looking, OK, because the error variance here is not explicitly assumed to be some constant. OK, and, and that's what we're getting. Can you calculate this? No, because it's based on population moments, and so to create standard errors of this formula, you need to define. Sample versions of the, of these objects that are in here. But it's actually relatively easy to do. Let's take a look. Um, well, first, this slide says, if you did, let me go back, if you did know this variance here. And what you need to do for a confidence interval is look 1.96 times. That variance square root to left and right, and they go, but you can't evaluate that. This is all based on population moments and so just replace things by their sample equivalents. So let's start with sigma X first. That's the population standard deviation of the regressor. You just replace that by the sample version. We did that on the whiteboard here. OK. And for this variance here, well, what you would do is. Um, you replace. Mu X, that's the mean of X with its sample average, and an error you're replaced by a residual. And a variance you replace by one over of the sum. The structure of a variance is um the random variable minus its mean squared. The mean here is 0 because it's based off the residual. So that's all there is here. So if I give you a data set with values on X and Y, you could actually calculate this number. And then you can use previous slide. This version where you sub in the sample moments from this slide. To obtain your confidence in all. So these SUX and XSX were defined on the previous slide. This is the standard error. That is now called heteroskelasticity robust. Now, you never have to sit down and calculate this for yourselves. Python will do it for you, right? So Python will use this formula, um, and the implicit definitions of SUX and SX squared. Um, yeah, right, so here I'm saying Python does this for you. Um, What do we do here? Uh, yeah, this is the California school data. Again, I believe this must be, there's no heteroskelasticity adjustment here. So this is the old way of doing things. OK. And here's the new way of doing things. What do you do in the future? Um If you want to account for hetero elasticity, meaning you want the variance of the errors to, to change in X, what you need to do is to um Amend or you change your Python code to account for a different covariance calculation, and this is how you do it in Python here. So this is what you have to tell Python. Um, so you, Python has, has this um. Option where you can choose how the standard errors are calculated. If you don't choose that option, it defaults to homo scalasticity. That's why we never have to talk about it. But if you want it to be heterosity robust, you have the choice from a menu of heteroscatasticity robust estimators. One of which we have learned or met today, and that happens to be called HC1 in Python language. There's a menu that's HC243, 4, and there, there's small differences between them, like degrees of freedom correction and underlying asymptotic approximation. The one that is in line with what I've taught you and what the textbook teaches you is called HC1, OK. And now, Do we have a moment? Um, what's the difference here? Oh, we don't have a moment. Um, in 9 minutes, we'll continue. OK, so 9 minute break. I so here we're just assuming this. No. Um, so What we're not doing Yes What this variants. Can can Don't OK, so Yes. We don't. So you don't look at the data and say um the variance changes. or or linearly or whatever. this use this vas. The rights of this That And in the past, we have just said that's. That is done. has thrown off the wall by the. Here the same variance is the variance of X times. estimate Some estimate. So we don't need to form about it. XX. and Yeah. Yeah, you have too much time. He doesn't have questions. Um, just about the the the phone. for the. First because That Question. For. Ta. I know this. I It's very Hi Um, Can you see that could work out. For this So I just My I He said the tuition is maybe. I think there's some rate of. it's like a. When the trailer was the trailer was the I I. For No, Oh Yeah and uh I Uh, Well, because I've seen This. So You found any more errors in this lines error. I mean small tiny mistakes. Yeah, but that's because I don't correct. I mean this I made clear to my students when I was teaching. I don't know what this is. Possible. Well, I mean, it's a rabbit hole shop, but I think it's findable. You probably have to go to GitHub and look at the coat. I just have to try to to. The Yeah. But I, I think that's the only difference, right, there's no actual errors in it. It's just different, different implementation. it's a lot of error. 6. Yeah, but this is fun because unable to connect. It's terrible. So you're starting with this can't connect the cable do you have? It's Well, but I haven't done that it doesn't work. Yes, that's not good. I've always used this thing to, so it's uh. Did you visit the 101321362? I have the address? Mm. disconnect. Dunbar. very uh secure. Yeah, yeah, no, I mean, I guess that's the idea because if you come up here, then you can also plug in the computer. I can just come up here on the computer. There you go. Oh I leave you to it. I think the break is. Oh yes, you just. OK Yeah. Oh, you know, I mean, now you're doing what, workshop questions? I mean, I'm just doing my assignment at the back. But this, this lecture material is a good refresher because this part I never actually got. It's like cheating. Yes, no, but you, you, I think you explain, I think the explanation on how you are cheating has improved or maybe my ability to process this explanation is improved. Where were we? Welcome back. OK. We we here already? Any questions? So we're running the heteroskelasticity robust regression. Which you can run like this in Python by doing the model fit allowing for this particular covariance type. That's just how you have to enter it into Python. What changes now if you compare this table, which is shortened to that table here, and I'm just being the bottom part. If you look at the point estimates, so this is the table where you Use homo skets, homo scalastic standard errors. Um, look at the point estimates here. And there. Same, OK. So first lesson. By making robust to heteroski ethicity, nothing changes on the point estimates, or with the point estimates, say point estimates. The only thing that changes are the standard errors and everything related to the standard errors, which is really the entire rest of the bottom table. OK. So going back to going back to the standard errors under the Homosketastic case, let's just focus on the standard error for the slope. It's 0.48 and here it's 0.519. Different standard error and then therefore the T will be slightly different. The P value is not looking different because it's still 0 because this year is highly significant, and then the confidence interval would be slightly different as well. But the construction of the, um, let's say let's go with the T statistic is still the same. The T statistic, if you remember from last week, this number here, -4.389. How do you obtain that? You take the point estimate and divide by the standard error that gives you this T statistic. The point estimate is the same, but the standard error is different, so this number will be slightly different. And likewise for the confidence interval, you take the point estimate. And then you go plus minus 1 times 96. 1.96 times the standard error gives you this confidence and. And then What should you do in practise now? And that's the guideline for the, for the rest of the semester. Um, Homogelastic standard errors are only correct if OLS assumption for A is satisfied, because that, that isn't a restrictive assumption where you go, I'm assuming that the error variance is constant. If it's not, then your standard errors are wrong. So conversely, He heteroski standard errors are in a sense always correct because it's not a restriction. So what I'm saying here is, they are correct under both assumptions for A and assumption 4 B. It sounds a bit odd, but because assumption 4 B is not really an assumption. I'll just call it that for some weird reason, OK? So what I'm trying to say here is, Always use Hatressticity robust standard errors unless somebody tells you convincingly that you're under a homostastic or in a homoscelastic case which nobody will ever do OK. So, let's read through these last 3 bullet points. So if you know for sure that your error terms are homosktastic, that means constant variances, yeah, then go ahead, just use Python's OLSI, which by default will Not make it robust to heterosity. If you know for sure that the error terms are heterosktastic, use this option here, the heterosk. To robust standard error option. And the situation that you find yourself in, in real life is, you never know for sure, then go with the one that isn't actually restrictive, just use the heteroscaleticity robust standard errors, meaning. Use the HC one adjustment. Does that make sense? So in the future, maybe in the next computer lab this week, I, I, I might explicitly tell you, make the point very explicitly to use the, the heteroskeleticity adjustment, but after the semester break, I might not tell you anymore. Um, so I want you to be mindful of this and always call heteroskilasticity robust. OK, so the new default will be that. Um, so this, this week, because we've just learned it, I'll tell you explicitly in the computer lab and in week 7 in your computer lab, I will not point out to you explicitly to make your estimation robust to heterosca ticity I expected of you, OK? Um, because it's, it's the more general framework. Any questions? Good. Too easy. This, this was the end of the slides. Then it's workshop some. A little bit I just um. Get ready Where are we Uh, over it In there now. No. Oh, OK. Yeah, different number. I think it'll work. There we go. Smooth, um. Woohoo, cool. So this is the exercise, exercise one. Regression R2 I wrote down as a reminder, what the definition of TSS, ESS, and RSS was. And the first question asked, um, show that TSS is equal to ESS plus RSS. Let's have a look. So TSS. I, well, since I wrote it there. OK, I've, I've written it at the top right. Let me make the first change already, that takes us in the right direction. I want to transform it somehow. To the ESS or related somehow to ESS and RSS, the ESS has a white hat in it. So why not add and subtract that? I mean, I'm sort of reverse engineering because I know what has to happen, right? Um, it's always unsatisfying like that. So I go, I can do this, but I now have to do that. Um, it's exciting stuff. OK. Nothing Strange has happened. Oops, can I add a page here. I need more space. So now what can we do? Let's put more parenthesis here, here and here and be mindful that then we have one big one here. OK. So We rewrite like so and then you can sort of see. How that has helped us. So, I mean we're not done with this line, but this year is the RSS. Right? But we're not done, so we need, let's say. We need. How slow should they go? Um, let me make a separate sum, because I feel like you guys will be fine with that. What's missing The cross product 2 times, right? If the cross product wasn't there, we still have to write that. What have we got whatever I've just written here. The first term is the RSS and the second one is the ESS and we, we would, we would, I'd prefer not to write a cross product now, but we have to, right. So let's write it. So next line. Plus 2 times the sum of YI minus Y I. Hat Is that right? I trust you. You're responsible. OK. What's next? So What I didn't know how to pitch. We, we want this cross product to be zero because then we are there, then we've arrived at the goal. Because And let me use all my powerful tools. Because um. This is the RSS. And this is the Yes, yes. Unfortunately, there's this cross products and we make more space. Please stay there, Paige. Now, studying this cross product. Our suspicion is that'll work out to 0. Right? Our hope is, and let me rewrite this as UI hat. Why I hat minus Y bar. OK. Not wrong. Does it, does it go in the right direction? I never know. I, I'm always cross my fingers. So what would you do next? Yeah. Oh Yeah. The errors sum out to zero. But you have this bit here, so you can't just say this is 0, there's, there's more, unfortunately more thinking required. Um, but how can we get in the right direction? Let's sub in. Why I had here. Because I suspect that the Y bar won't be a problem because of what you said. The Y bar multiplied by You are hat, that will be 0. By this feature of my head. Um, the troublemaker here is why hat I because it has an I subscript, but um, I think there's going to be a simplification. Let's have a look. So I'm, I'm subbing in. For why I had is equal to They are not had Plus beta one had XI. And then we have our white bar here. Do you see now how this could be 0? Let's multiply out. We have 3 terms. We have Beta not had. Some of you I had. We have Beta one hat, some of XI UI hat, and we have Y bar some of UI hat, right? You yeah. So Are all of them equal to 0? The lucky answer is yes. Why is that? This year Is 0. We learned that 2 weeks ago, I think. So for that same reason, this is 0. But why is this year 0? We also learned that was it 2 weeks ago or last week? Um, no, it was 2 weeks ago. The, the residuals are constructed that way to be orthogonal to X. So done So it follows that. TSS is ESS. Plus RSS. Any questions? So, now, yup. Like, give me a run, but could we not see if that last step. you've got some of you had I and then everything times higher. So if we know some of you had I 0. I mean not just No, then, then, so with this argument, we could have done that here as well. Yeah. So it, it doesn't work. So let me make a site. Point in here. So if you have, so you, you know, so we do know this from last week. So that implies, as a general rule, that some constant times you I had. is also equal to 0. Why? Because you can take the constant out front. So let me rewrite it. So you could You could write this as C times. You are hat. And that is 0 because you are hat sums out to zero. But now, this is A different thing is if you have. CI You I had, by which I'm saying you're basically, it's not a constant, it's changing with an I subscript. So the key distinction here is This, this thing that multiplies UI hat has an I subscript. You cannot now do this. That is not possible. And then you then you can't do that. So this is like saying individually the UI some out to zero. But each of them carries a weight here CI. That does not necessarily Somewhat to 0. Yeah. And we, we've sort of been, being careful with this whenever we use summations. Um, when. When Yeah, when, when this sort of situation happens, we, we can't unfortunately, take that shortcut. It's tempting, but it's, it, we can't do it. And so, What what plays the role of this constant here. So this constant oops, sorry. This constant C here that doesn't have an I sub script in, so I'm putting a box now. Around this Uh, or actually one level. Up higher, the beta not hat is such a constant. And the why bar is such a constant. OK. Um, but this XI here, oops. Um, I don't want to do. This XI here is a, is not a constant. It has an I subscript. That, that's why we can't do this. OK, enough said. I think you've, you guys, um, trust me. Um, so this works nicely. Um, was there any other value? Oh yeah, what I wanted to say, at this stage, now that I've shown you, demonstrated to you how to drive this. I feel like I could ask this in the final exam, and you go, that's fair game. If I hadn't shown you this, then I would say that's a difficult question for an exam, because you can go off in many directions. And oftentimes I find myself teaching you going, oh, what's the next step here, even though I've prepared it. I go, I actually don't remember this anymore. Um, and luckily I. Uh, then somehow get it right, but in an exam pressure situation, I try not to put too many original proofs. OK. More like Regurgitation or reproduction of stuff that you would have been that you would have seen before. Alright, so, but there was another part. Oh yeah. A simple linear regression model gives you a zero slope. Estimate show that R2 is 0. All right, let's add a peach. So the slope is here on the, on the whiteboard, but I'll write it down anyway. So part B. So Apparently We have this result here. That's the information that's given to us. How do we relate that to our squared now? Um, yeah, how do we do it? How do we relate this to our screen? So first of all, this seems to imply that the numerator is 0. Would you agree? So, can I cut that out? Yeah. We also know that passwords. Explain something We don't know that R square is here. I think we want to show that our square. That But we try to end up with a condition where. ESS 0 Oh yeah, right. That's what I want, so this. This guy is 0. Uh, I think I've started. From the wrong point. Yeah. OK. Let me reorganise my thoughts. So this is again not wrong. But what we want to show is that Beta 1 hat equal to 0 implies R2 equal to 0. So How can Ask where B 0 and I said that in the lecture by the definition of R square, ESS has to be equal to 0, right? Now we have to relate the ESS to beta one hat. I think what I, what I wrote there upstairs is not really. Helpful, but it's not again not wrong, but an example of how in an exam you can just go off in different directions. um, but so what was the ESS? That's the ESS, OK, uh, writing. Mm Let's see how we can get there. Let's plug in again. Why hat. Um I think this will work now. But we were given the information that beta 1 is 0. So By assumption Or hypothesis, so we get The ESA, when the slope estimator is 0, boils down to this. We still want to show that this is 0. Where is that hidden? What is the extra step? What is beta not hat? In the simple regression model. So I, I've memorised beta one had, what is beta not had. Two weeks ago we saw beta not hat in general is. My bar mine is beta 1 hat. XB So what I've just written is so. This year, Is beta not hat? But beta 1 had was 0, so they are not hat when beta 1 hat. 0, beta not hat is just Yang. So we have Y minus Y is equals 0. OK. So the way I started was not useful at all. But luckily I realised this is going nowhere. We need a different approach when you told me reminded me of the ESS and then we got there. Any questions on this? Cool. Good. Next. Exercise. So we've got 3 versions. Of a regression model. The first version, so 123. 12. 3, it's just how it always looked. And what I'm trying to say with model 2 is. What if I transform the data? I take the dependent variable Y I just multiplied by a number A. And I take the the independent variable multiplied by number B. What happens in a regression of these Change or transform variables. So instead of regressing Y on X, you regress A times Y on B times X. This is what I'm asking him In part A if you run the middle regression. What, what do you get? For the slope estimator. So let's let's frame this. So what I'm trying to say in part A. As you have transformed. Transformed Variables. You have a A new independent variable, let's call it Ytilda, which is the original. So dependent variable times A. So for example, if the dependent variable is Uh, what could it be Test scores, I say, instead of the test scores, take 42 times test score for everyone. So multiply the whole thing by 42. And do the same now for the student teacher ratio, multiply that by some other number. So, the same for XI define a new independent variable to be B times XI. So what you then get for the average versions. What is the, uh, let's call it YI Tilda, this is ugly. Bar. The average version is then A times Y bar. And for X I til the bar is equal to B times X bar. OK. We need these now to set up the OLS estimator gamma 1. Gamma 1 in the model that you can just see at the very top is the slope in the regression of Y tilde on X tilde. To slope in the regression of whiter and extilda. The estimated slope is gamma one hat. So The question here is what is Gamma one hat? Gamma one hat is you use this formula. But you don't use X and Y, you use the transformed versions. So I'm writing down a copy of this formula. Using the tilde. So what I want to do, yeah. So first, I just write it down. So you get XI tilde minus XI til the bar. Y I Tilda minus Y I til the bow over. The sum of XI. Til the minus X I till the bar. And now, we plug in, we link it, relate it back to the original X and Y. So what I've written down here, Is that's the regression. That you would run in model number 2. where you regress the transformed variables. And you obtain the slope. The slope, gamma one hat is what I've just written down at the at the bottom. And that's, let's work that out. With our definitions, that's equal to. When it says XI, I put B, so when I say X I Tilda upstairs, I put in B times XI. When I say X I til the bar, that is. BXR. So I'm linking this back to the original variables of the model. And here, uh, A Y I minus Ay bar. And here BXI minus BX bar squared. Right? So what I've done is I have now Linked it back to the original axon Y variables, the untransformed variables, and I've made explicit where the B's and the A's enter that formula. Now I can clean up. And now, now I can use this trick again, these A's and B's, they don't have an I subscript. I can take them out of the sun. So, you can convince yourself that that's equal to A, B, sum of XR minus X bar. Yi minus Y bar. Over B squared. Sum of XI minus X bar squared. And that Is Equal to um A or B. Beethoven had. So comparing the left hand side to the right hand side, we've just shown that gamma 1 hat is equal to A over B beta 1 hat. So you could run the regression of the transformed variables, obtain gamma one hat, or you run the regression, the original regression of Y on X. You obtain beta on hat, multiply that by A and divide by B. Backs out the gamma one hat. That's how you can relate the regressions to each other. So what this is saying that if you are given a spreadsheet where I tell you. The dependent variable was times by A, and the independent variable was times by B. How does this change your result? This contains the answer. OK. Any questions on that? So, transforming the variables does change the slope. That's, that's one message here. Um. There's no estimate, um, but in a controllable way, right? If you know what A and B are, you can Um, back out either gamma one, gamma one hat or the other way around, beta one hat. Now, the question is, what is beta, sorry, gamma not hat. In that regression, so the constant estimate. Should be Um, What should it be The transformed variable, so first of all, the generic result is The sample average of the dependent va, which here is why I tell the bar. Minus slope estimator times the sample average of the regressor, which is X I til the bar. So, this shouldn't have a subscript, shouldn't have an IE subscript. Did I use I subscripts up here? I did. Sorry. The sample averages shouldn't have ice subscripts, so I'm I'm removing these. Um So Xtilde bar and Y tilde bar don't have I sub scripts. So this here is the, the generic formula that I've just written at the bottom for a, for the estimate of the intercept, it's Sample average of the dependent variable minus slope estimate times sample average of the independent variable. And now we just work that out. So that is A times I minus gamma 1 hat, which we, which you can see up there is A over B. Beta 1 had times X bar is B times. So X to the bar is B times X bar, OK? So cleaning this up, you go, this guy. Cause that guy And we're getting 8 times. Why bar minus beta 1 had X bar, but that is 8 times? They are not ha. So the intercept estimate. Is only a times beta not hat. It doesn't change or it doesn't contain the little B here. So the intercept is unaffected by the multiplication of the independent variable. Fair enough. All right. Any questions on that? OK, then we do a similar exercise in part B, where what you, how do you transform the data in part B? That's for the 3rd model. Instead of multiplying the dependent variable, you, you add a constant to it, and for the independent variable, you add another constant. And how does that change things? So apparently the slope is not affected by that, but there's something happening. To the intercept. So, um, Let's take This top bid In part B Part B It Now, the change is this, it's A + B +, not times. And that's the same then for the averages. And this formula, OK, this thing is not called gamma here, but it's, I think it's called Delta in the 3rd model. But this formula, this is a delta, this formula is still correct. How is to plug in the meaning of Extilda and white tilda. Any questions? Can go with us. So we get. So we get A plus. Sorry, not A B plus XI minus B. Plus eggs bar. Times A plus. Y I minus A. Plus wide bar. And at the bottom B + XI minus B + X bar. But you see how they all neutralise each other, these A's and the Bs. I think you're missing a square. Where? Oh, here. Is that right? Did I miss that earlier as well? Oh yeah, yeah. Thanks. I think you can see relatively easily that that is beta one hat. The bees get subtracted out, the A's and the bees in the the B in the numerator and the denominator and the A's in the. Uh Maria as well. OK. OK. Sounds good. Does it sound good? I mean, it's better things, but, you know, what can you do? And then for delta, not. The generic formula, I can just copy from upstairs. Copy Paste, that's still true, but the definition of the transformed variables is now different. So this is A plus Y minus gamma 1 hat is beta 1 hat. We just discovered that we want to relate it back to the original model, and X bar is B+. X So what do we get? That is A plus. Beta one had B plus. My bar minus beta 1 had X bar, I'm just reordering, and that is equal to. A plus. Data one had B+. Better not. Hat Is that right? Let me just cheat and see if that's what we wanted. Yeah, that is what we wanted. There was a, there must be a minus sign. Here. Where did I make the mistake? Yeah Ah, here, this minus here, right. Thanks. Um, yeah, I can put parenthesis around it to make it look prettier, but it doesn't. But, so you have beta not hats. And you've got this other term added to it. So the, the lesson there is If you just add constants to your data, the slope is not affected, but the intercept is. OK. One last thing That, that you can show in a in a picture nicely and easily. Let's have a look at a picture. So I'm drawing. Um, a few dots in there. OK. Now I'm fitting a regression line. Oops, not like that. OK, looks like a convincing line, does it? Now, I shift. My data, we should do a copy of this. So, same dots, right? Let me remove the regression line. And now, I'm adding A and B. To my dots. What that means is adding B. To the to the regressor, moves the dots to the right. Adding A to the dependent variable moves them up. Right? That, that's what I've done in this picture. Does that make sense? I mean, I could have shifted them to the left and down. Do you see that the slope is not affected by this? It's the same composition of the dots. I would fit. I wouldn't fit a different slope, like just because I've moved the dots around, I will now not make this slope here, right? The slope is the same. So I could, if I'm good. With notability could take this this line here and go, I must fit the same line here, right? But what is different, which is, which I highlight even more now by moving this up higher, is. The intercept, right? So. The intercept, oops. is affected. Different intercept. And the precise relocation of the intercept is worked out by if, if this is They are not had. Then we've got beta not had here, and this year has to be a minus beta 1 B. OK. It takes into account. The fact that we have added A constant to the regressive and a different constant to the dependent variable. So it's relatively intuitive. This picture shows nicely, I hope, convincingly that you wouldn't change the slope. Now, I couldn't draw a picture like this for the other case, because then the, the dots will disperse if you multiply the dots. And then there's no intrusion, almost, I mean. It's tricky to offer intuition there. Are there any questions? Then that's it. All right, have a good break. Good luck if you have midterms. Um, yeah, and I hope you can.
