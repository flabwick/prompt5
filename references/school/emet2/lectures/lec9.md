SPEAKER 0
This material has been reproduced and communicated to you by or on behalf of the Australian National University, in accordance with Section 113% of the Copyright Act 1968. The material in this communication may be subject to copyright under the Act. Any further reproduction or communication of this material by you must be consistent with the provisions of the Act. Do not reproduce this material. Do not remove this notice.

SPEAKER 1
And Yeah OK, good afternoon, everyone.

SPEAKER 3
Week 9. OK. Um, how is everybody travelling? Good. Have people attempted the quiz? No. Yes. Or was it Bad? OK, that's an important distinction, like the other ones I hope. And then there's another quiz in week 12. Is that right? I posted the assignment. Did people see that? Did, have you finished it already? Submit it. Um, I think, last week I said after this week, you should be able to fully solve that assignment. It covers everything up to lecture, uh sorry, week 9. So that includes, I guess, your computer labs as well. Are there any questions before we continue? No. Excellent. So we'll, we'll wrap up the multiple regression model today and start with time series next week. Which I've said it 2 or 3 times already, it's just an application of this. To data that has a time subscript rather than um An individual or an entity subscript. All right. Um, but let's, let's wrap this up. So The sort of headline for today is functional form considerations and I'm. Illustrating today that the linear model can be quite nonlinear. And that's a good thing. So it offers you a lot of flexibility. It's linear in the betas, but the Xs can be unlinear. That makes sense. We'll we'll have a look. But before we talk about that, there's a flashy subsection called the dummy variable trap. OK. Let's have a look. So, what's this? Let's say you have G G dummy variables, so that's the number of dummies that you have. And what we do with dummies, they, they split the the data of the population and. Into subpopulations and they do it exhaustively. What am I saying? So they are mutually exclusive and an exhaustive division of the subpopulation. The simplest example is when G is equal to 2, and you've seen this before. When the dummy variable is called smoker, you've got two groups. The smokers and the non-smokers. Um, So you can Define Two dummy variables then. Smoker equal to 1, if a person is a smoker, 0 otherwise. And then you can define a dummy called non-smoker, which is equal to 1 if the person is not a smoker, and 0 otherwise. Now, you can run so we ran these birth weight regressions. You can run birth weight on smoker. I think we've done that in the lecture, um, but you could run birth weight on non-smoker also, or you could run, you could try run birth weight on smoker and non-smoker at the same time, and that's the dummy. Variable trap. I don't know why it's called trap. That won't work. Why not? That's the that's the whole point of this discussion. And because. Having included, so if if I write it like this, having included smoker on the right hand side, adding non-smoker doesn't give you any new information. Does that make sense? Because there's only two groups. They're mutually exclusive and exhaustive. So once you include one group here, you don't need to add the other group. And so here I'm just saying, if you do this, I actually don't know what happens when you do that in Python. I do know in Sea will just tell you, you've included one too many. I'm dropping one out for you. I don't think Python will do that, but um, I'm guessing there will be some sort of error message when you try to run this last regression. And just to convince you that you can run this regression either with the smoker dummy or the non-smoker dummy, you get. Qualitatively the same results, like visually not, but the information in these regression outputs is the same. Let's read. So in the first regression, we regress birth weight on smoker. We get an intercept of 3,432. That Is the average birth weight of babies that are born to moms who report to be non-smokers. Then In the regression at the bottom. We reverse birth weight on non-smokers, then the intercept captures the average birth weight of babies born to moms who report to be smokers. So what we learn from just from inspection here, the intercept covers. The group that is sort of the inverse of the dummy that you're including. Inverse converse reverse, I don't know, um, makes, makes sense if you, if you look at the top, the, the slope, which is not a slope we've discussed this earlier, it's just the adjustment for the group here is -253 is the. The change in average birth weight, if the dummy for you is equal to one. Here, the dummy is the smoker dummy. And the intuition here, of course, was that smoking is not Beneficial, well, it's not having a positive on positive effect on birth weight, but a negative effect. So that's the negative 253. Or conversely, in the, in the regression at the bottom. The non-smoker coefficient estimate is 253. That's the The adjustment that you make on top of the intercept. To move you from the intercept to, to move you from a smoker to a non-smoker. OK. Um, the way I'm phrasing this is that the, um, The group that is represented by 0 is the so-called benchmark or default group. So if the dummy is called smoker. Like upstairs, the zero group are the non-smokers. And the intercept applies to you then, if you're a non-smoker. At the bottom. The dummy is non-smoker. So a dummy, a value of 0 means you are a smoker. So the intercept captures. The predicted effect for that group. OK, but if you think about it, although the intercepts look different here, the slopes are not the same. One is positive, one is negative, but in absolute value they are the same. But these tables contain the same information. You can back out average birth weights for both groups. You just have to remind yourself. Who is, who's who's, which number applies to what group here? OK. Any questions on that? If you try to include both dummies on the right-hand side, Python might throw an error. Uh definitely, the inclusion of the second dummy doesn't add any information, given that you have already included the other dummy. Now let's generalise this. To Um, to include a dummy that slices the population into 4 subpopulations. So the smoker one was relatively easy because there's only 2 values. Two subpopulation, smokers and non-smokers. Here, same data set, we had these 4 dummies. That indicated When The mom went to see a doctor for the first time during their pregnancy, OK? And then There's 4 options given. So the first is You didn't go to see the doctor at all, so this tummy is, uh, try pre-zero, whatever you wanna call it. So no visits to the doctor, prenatal visits. To the doctor that I'm, I'm suggesting here is It's not a a good indication, as we also see in the data that is problematic. It doesn't mean that you have a very healthy pregnancy, you don't need to see the doctor. The default is, just from my own experience, as soon as you find out you're pregnant. Um, most people, and you'll see that in the data, see the doctor immediately, just to get a health check and. Assessment Um, you know, that's that's the default. So the, the group that is most common in the data, or, uh, the most common in the data is actually the, the group, the group represented by the second dummier called Trire1 is equal to 1 if the first prenatal health visit was in the 1st trimester of the pregnancy. That's usually, well, you learn, you're pregnant, you go see the doctor straight away. You typically, that still happens in the first trimester for most of the cases. That is true in this data set. Um, then there's 2 other dummies. If you went in the 2nd trimester for the 1st time to see a doctor, or in the 3rd trimester, I would say the most. Common group, just to formulate this empirically, is Trire 1. OK. The group where you don't see the doctor at all, as we will also see in the data, Trire zero is the most sort of suspicious group. So It's relatively uncommon to not see a doctor, and it's not usually a good sign if a person doesn't see the doctor during the pregnancy. I'm just formulating this based on the results that we will see here and so on. So now, as a technical thing, we have a division of the whole population into 4 groups. And they are exhaustive and mutually exclusive. You can't be in in two categories at the same time. So, if you run this birth weight regression now, um, which one do you include? What am I saying here? Yeah. Which one do you include? We have 4 dummies. Here, I'm saying we just learned, learned, you only need to include 3 dummies. Which one should you include? It doesn't matter. Once you include 3, The 4th 1 doesn't add any information. So, it's just sort of frames the interpretation of the results or the angle that it offers, but the, the results that you're getting are are not substantively different depending on the subset of regressors that you include. As long as you include G minus 1. If you decide to include only in this case, one or two dummies. Then that's a different story, but the rule is you include All dummies, but one, and then it doesn't matter which subset you're including. Let's have a look what I mean here. Um, and here I'm saying the unused tummy, because you're not using one of these, implicitly defines the benchmark group, the benchmark group that tells you what the intercept stands for. Let's have a look. There's many combinations that I could try. Here I'm just showing you two. In the first regression, so I'm I'm always also including smoke and alcohol here now. Um, but I want to pay attention to most, most of the attention is about the dummies. In the first regression, I am. If I just look at which dummy I'm not including, I'm not including Tri PR one, which My story was it the most. Uh, common group there is, and what you see is that the coefficient estimates of the other three dummies are all relative to that excluded group. So if you are a mum who went to see a doctor in the first trimester, then the intercept term applies to you. Then the average birth weight of your baby is 3450. 4, that's the expected value, right? And then we're controlling for other stuff. Then, Once you tell me, but I didn't see the doctor in the first trimester, I saw the doctor in the second trimester, then I go, now I have to adjust my prediction down by 100.84. OK. From 3,454, which applies to moms, average birth weights of babies that are born to moms who went to see the doctor in the first trimester. Once you give me the information, well, I didn't go in the first trimester. My first prenatal visit was in the second trimester, then I have to adjust this down by 100. And what you see is that all, all of these 3 are negative. Which indicates that health out and, you know, birth weight is a health outcome. High birth weight of baby is an indicator for good health. OK? So what I'm seeing here is that The best health outcomes are to kids, babies that are born. Two moms who went to see the doctors. In, in their first trimesters. OK? Because all of these signs here are negative now. I could run this regression with A different set of 3 dummies. And what I'm excluding here at the bottom is Trire zero. So moms who didn't go to any prenatal visits. So if you are a mom that didn't see the doctor at all, the expected birth weight is 2,756. Then you tell me, but I did go in the second trimester, then I go, then I add to that 597. That's the expected birth weight of a baby born to a mom who went in the 2nd trimester. That number is the same. So if I take the intercept down here, 2000. 756 and I add 597, that should be the same as taking the intercept upstairs, 3,454 and subtracting of 100. This is, like I said earlier. These tables have to be consistent with each other, mechanically. There is no different information included in both regressions. You must arrive at the same prediction. OK. It's just The excluded group in in either regression defines the benchmark, but what the intercept stands for, and then the coefficient estimates on the dummies tell you how this benchmark has to change relatively if you fall into any of the other categories, OK. I think it all makes sense. I hope I'm not making this more confusing than necessary. Also, what I want to say, I did include smoking and alcohol to convince you that these regressions really are the same because the coefficient estimates of the slopes. Of smoking and alcohol are the same. And they are not just close to each other, they are numerically identical. In these two regressions. They have to be, because you're not doing anything differently by including different subsets of the dummies, as long as you include G minus 1. If you go G minus 2, then it will be different. G-1 won't be different. The excluded dummy doesn't cause any harm. OK. Are there any questions on that? Is there anything else interesting going on there? No. All right. That was the dummy variable trap. So, the trap is really maybe if you have G dummies. Or the rulers include all but one. The trap is that you forget to, or that you include. Fewer than that. OK? If you have 4 dummies that you only include 2, that would be a problem. It's not so much a problem that if you have 4 dummies, that you include all of them because your software will tell you that won't work, OK? But it will throw an error or data will just politely say, if you were using this data, um, sorry, one of these dummies is redundant. But if you include, if you have 4 dummies, you only include 2, then your software package will not tell you there's an error. It will not know. But the results will be different. You are leaving out information in that case, OK? So that I think is the, the trap there, the the only danger. And it's a a form of admitted variance bias, yeah, and. Not really. So it's a bit different because it's a dummy variable, and it sort of slices and dices the population into subgroups, but technically, you can discuss it within the framework of OVB. You're making a variable. All right. Now, functional forms. First part is polynomials in the regresses. So here, I'm writing down a linear model. And it's linear, in the Greek stuff, OK, but the regressors are non-linear. So like that, we can cover or we can Deal with some form of nonlinearity, the one in the regressor. And so this is really a multiple regression model and where your regressors are polynomials. Um, so powers of X. This is easy to run in in Python or any software. Um, you just create these higher order regressors and run a regression with these guys. But why, why would one do that? Using the data on test scores and average income, so I think, I don't know if we have used average income before. It's the test score data and students in was it secondary school, school district I. Average income is the average income in the school district. So we are not talking here about causality, yada yada. That's not the point. The discussion I want to have here is about mechanically, the functional form, OK, so if I throw in average income as a regressor on the right hand side, I could include it also. As a power of 2. The quadratic, the square of average income. Mechanically, I can do that. I can also throw in to the power of 3. Cubic specification now. First, to demonstrate to you that mechanically this is not a problem, and how you would do it in Python is since you have average income in the data, you could either hard code a new variable appended or add it to your data frame called average income squared, or use Python shorthand. Like, so, you see this here in this little yellow bubble. Um, I, and then average income 2, so the double star is squared. That works, so you don't actually need to add the quadratic version of average income to the data set. This can be done, it won't be wrong, but Python doesn't need you to do that. You can just tell Python like that if you have a variable in the data. You can include its higher power by just writing this. In the formula for the regression. And then it adds a new variable here which Is the square of average income. OK. And This is just a standard multiple regression table. So moving from the simple regression where you just include average income. To a multiple regression model where you also include its square. Now comes the question, why would you do that? Well, if the picture looks like this, this is from the textbook, just visually, you could go, maybe the quadratic form of version is a better reflection of the data. OK, and you, by visual inspection that looks quite convincing, but The reason why you would really want to do this is because you have some sort of model in your head, in your mind that says that. The change of the regressor is changing itself. You are going after sort of a flexible second derivative. And what I mean by that? So the picture is convincing, but you could also think about the estimated PIF in the quadratic model. Um, here I'm just plugging in my estimates, so if you if you go back two slides earlier, the intercept was 6007. Slope on the linear term 3.85 and slope on the quadratic term negative 0.042. Just plug that in here. So that's your estimated PIF and what you can now do is Plug in values and do this analysis when when the regressor changes by one unit. What's the prediction for the dependent variable? Now here, I haven't told you this, but our average income was the average income in the school district. And it was reported in thousands of dollars. So if you are interested in studying. The change, the predicted change in the outcome variable for a change in average income from 5 to $6000 per capita. That's on like a low average income, but let's just go with this. You can just plug that in. And what you have to stick in the way it was coded is then average income moves from 5 to 6 because it was reported it it was in in the data 5000 was represented by the number 5, 6000 was represented by the number 6, so you just stick in. Up here in your estimated PFF 6 here and 6 squared there. That's what I'm writing here. And then likewise, change it from 6 to 5, so 5 and 5 squad. Work this out, and what you get is 3.4. So you get The change in the dependent variable when X changes by 1 unit. Now, What happens here is because you have a quadratic specification, the change in the dependent variable. Depends on Where you start with your change in average income. So what I'm trying to say here is this. All of these changes that I'm studying here are $1000 changes. Moving average income from $5000 to $6000. We saw this on the previous slide, changed test scores, predicted test scores by 3.4%. This is just plugging into the estimated PIF. Doing the same increment, $1000 but now, from 25 to $26,000 if you plug this. Into this equation here works out to 1.7. $1000 change from 45 to $46,000 works out to 0. The change is changing, OK? And so what I'm saying here is the effect of changing average income on test score is decreasing in average income. In this case, it's decreasing. Because The quadratic term is estimated to be negative. OK. So the second derivative. Is negative. That's what I'm saying here. Um, and then you go, if you, if you continued, if I, if I investigated 45, sorry, 55 to $56,000 I'm just guessing I'm getting, gonna get a negative number, where you go, making the school district richer by $1000 now has a negative effect on test scores. But that's again only mechanically, you wouldn't. Buy this necessarily. I don't see how it can do any harm to test scores, increasing incomes. It's just an artefact of this particular functional form. In the picture you see that somewhere turns negative, but then would you You have a model in mind that justifies that this thing would go down again, probably not. OK. So the quadratic specification can only do so much. It offers more flexibility, OK? It allows the change of the dependent variable to To change at different rates, OK, or to decrease in this case versus if you have the linear specification, it's always the same, OK, but that also means for the quadratic specification you the change could be negative, but then you have to just give it a reality check and go, I don't buy it. I don't see why an increase in income should have a negative effect on Tesco, so that's just the limitation of this particular model. Does that make sense? Is that a question?

SPEAKER 2
especially It's just, you know.

SPEAKER 3
But the, the interpretation, so. You could have, so you couldn't, if you don't look at this picture, uh, that's what, that's what I recommend is you don't look at this picture, and you go, I want to model the effect of average income on test scores, and I would like the marginal, not going to microeconomics, I don't know if you guys would have taken microeconomics. You go, I want the marginal effect to vary. In Xs. As soon as you want that, you need a higher order term. And aquatic term oftentimes will do it. Um, I think I do it, I, I will include a cubic term as well, but the quadratic terms allow for more flexibility and arguably, it's not costly to include it. It's just a little bit more of typing in Python. And am I doing this next here? Uh, sorry, I will do this in a second, but you can. You can also raise my regression, um. Yeah, you can just throw this in and ask yourself, has it improved my regression? And you could look at the significance of this thing and go, this is convincing. I should be including this. OK. You could also look at adjusted R squad, but ideally you don't have a sort of model or data driven. You have sort of a model in mind where you go, I want average income to affect test scores in a more flexible way, not just in a linear way. And then the quadratic. Term is often like a good first go at it.

SPEAKER 1
So when is a function of the reversal.

SPEAKER 3
Yes, so which, which here is the case because you have a second order term. With linear, the marginal effect is constant. It can't depend on X. If you want it to depend on X, you have to allow for a non-linear term. And just from practical experience, oftentimes the quadratic term is already a good, a good way of addressing that. Because as you know from microeconomics. Oftentimes you go, what's, you're just, you know, you're interested in the second derivative and here you're allowing for a second derivative. OK. Now, mechanically, I can include the Third order term or the cubic term like so, and then here it appears. And then Um, so this is just to convince you that you get another multiple regression and I don't want to think too deeply about this now, but you could use our tools from last week and ask yourself, Has, has this improved things, including these higher order terms. So we can play around with our F test. So we could test the null hypothesis of linearity against the one of nonlinearity. And um to be more precise. Against the alternative that it's quadratic and or cubic. That is, it's a polynomial of degree up to 3. So what you would do in the previous regression. You ask yourself, so going back to that regression, are these two. Coefficient estimates suggestive of population coefficients that are zero. So I'm just asking for the higher order terms. So average income squared and average income to the power of 3. Are the, are both coefficients zero? And how would you test that? It's a joint test that, uh, I think beta 2 and beta 3 are equal to 0. So how do you do this in Python? You go, well, this is how we did it last week. That was the idea. You tell Python which coefficients you're testing here for the quaratic term and the cubic term, that they're both. Not relevant, but they're both 0. And here, So this is the, this is the null hypothesis in Python. So both of them are 0, that is, beta 2 and beta 3 are both 0, and the P-value is tiny. So it looks like it's large, 9.something, but it's, it's, uh, it's actually tiny. OK. Go 0.000 and so on. So you're rejecting the null. And this is not really surprising coming from this table here, because you see individually they are. Close to being relevant, the quadratic term for sure and the cubic term. Just It's just an indication, but the joint tests confirms that relative to the linear model, uh, I guess what we do, what we've done here is we tested that both of these are zero. Another way of formulating this is that the true model is linear against the alternative that is nonlinear of order 2 or 3, and we reject the linearity hypothesis. Does that make sense? That's what we have done here, so. What do I say here? Yeah. The hypothesis that the PIF is linear is rejected. So it's, it's not linear. But the alternative was, it's either of order 2 or 3. We know, we don't know what. But it doesn't hurt, including up to all the 3. Uh, right. Any questions on that? Yeah, for you?

SPEAKER 1
Can we say that since the p value for the cubic regressor is close to 0.75, that the cubic influence is dubious compared to Uh, dubious is a strong statement.

SPEAKER 3
So if you, if you are forced, like somebody forces you to make a choice between the two, like, say, which one do you want, then I would go with the one, right? Based on that. And it's all. Yeah, I would go with that. But you could also Um, so plug in some numbers here if you just look at the coefficients. I mean, this hits a cubic term, so it gets amplified, but I, and I haven't done sort of a plausibility check, but it looks to me like the quadratic term is also more meaningful in terms of how it hits its coefficient estimates and that it has a larger effect. But um, I would go with your intuition that um. Um, it seems more significant. It seems further away from zero, the, the quadratic term. Was there a question?

SPEAKER 2
Um, the 3 that's what we did, right?

SPEAKER 3
So an F test on beta 3 and beta 2. Just like to check it and see if I can

SPEAKER 2
model it.

SPEAKER 3
It's just Oh, yeah, so, um. Yeah. So that gets into this discussion of model specification. So you, you could run the model with just, well, we've done the model with just beta 2 included, you can do the model with just beta 3. How do you choose that, that, that's a bigger topic that we're not really touching on, cause it gets dangerous when people choose their models, um. The best I can offer is when you do your own research project. You don't usually start from zero. You build on people's work and you do what they have done, and you could tweak that a little. But I would say if you include a quadratic. So, if you, if you include a cubic term, you should always include the lower order terms as well. So you don't just jump, but it's a bit of experience, best practises in the field, and, and in this case we have a hard test for saying linearity versus non-linearity, but then how do you choose between these two? Is there definitely the need to do so? Sometimes you have an explicit econometric model that tells you there should be a quadratic term included, then you have to include it. You can do as much testing as you want if the structural model that your work is based on says there should be a quadratic term in there, then you have to include it. Long story short, it's a little bit unfortunately in art. This is where our science becomes. Very non-science is a problem.

SPEAKER 1
Yeah. I could say that it's probably not good.

SPEAKER 3
Yeah, that's right. Yeah. You can always, you feel like you can throw in a lot of higher order terms. And then the question is always of model specification. That's where is in, there's this discussion between what, what's called structural econometricians who start from a model that is, comes from written on paper and pencil, rather than just running a regression, closing my eyes and just running a regression, throwing in everything that I have. So you have mathematically worked out. A system of equations for which you then go to the data and estimate that and then you don't don't actually have these choices but let me get started in that direction, but you're pointing out, I guess a sore or weak weak spot in in regression modelling and specification. Um, in, in how we approach it in this introductory lecture, everything is relatively softly formulated. Um, if you have a data set, which subset of regressors do you include, which higher order terms do you include? Um, we're not really touching on that, um, but, uh, there's some things that we can do like. We always have a linear term, but then the higher order terms, we can test like suggested in this test here, and then everything else is informed by experience, by previous work in the literature, and Um, Yeah. That's that's all I can say. Now, Uh Next topic is still functional form. Logarithmic functions of X or Y. As I look at what happens when instead of using your original data, X and Y, you include logarithms of them, either on the left or on the right, or simultaneously on the left and right. So we are using still our test score data and average income in the school district. So this is the regression in levels and the question is now, instead of running this regression of test scores on average income, how about I regress. The natural log of test scores on income or income on the natural log of uh Sorry. Test scores on the natural log of income, or the logs of both on each other. What happens? People do that, and why do they do that? So just a quick refresher of what the natural logarithm does. When I say logarithm everywhere, I mean the natural one. So Take two values for the regressor X1 and X0. So those are placeholders for. Income in this case. Um, and look at the change as you move from Xonau to X1. So that's delta X. Define the natural log of X1 and X, call them X1 tilde and X not tilde. So this is just reminding you that if you, if you have logged your data and you take the differences in logs, that approximates percentage change. Have you guys done finance? But not that you are required. Like continuous growth or even in macroeconomics, if GDP grows at a continuous rate, then you take the the natural log, the beginning of the end of the period gives you the continuous growth rate. OK. So the reminder here is just that if you take the differences of two numbers that are logged, natural logged. That is a good approximation sometimes for their percentage change. Let's have a look. Let's say my original numbers are 50 and 52. So one person is 50 years old and another person is 52 years old. What's the percentage difference between them? The absolute difference is 2. What's the percentage difference? I hope you wouldn't need a calculator to work out, it's 4%, OK? Now, for the next calculation, you need a calculator. If you take the the natural log of 50, you get 3.91. Just believe me, take out your phone. The natural log of 52 is 3.95. What's the difference between these two numbers? I hope you don't need a calculator for that. That is 0.04. 0.04 is 4%, because percent means divided by 100, OK? So you see these numbers are the same? Yeah. OK. That wasn't easy. Easy way to convince you. So you might either take the original numbers and levels. And ask yourself, what's the percentage change? All the original numbers in natural logs and just different them. So, likewise, if I just tell you, so you don't have the information upstairs, I just tell you the difference in logs is 0.07. That means whatever these numbers were originally, the percentage difference is 7%. And now what you will see very soon is we actually get the difference in locks to be equal to 1. What does that mean? Well, that's a 100% change, at least. Uh, mechanically extrapolating, but what I'm also reminding everyone of is that this approximation works for small percentage changes only. OK. So, everyone, that was a quick refresher. Everyone OK with that? Let's see now. How this works out, what this does for us in the linear regression model, um, where the, the oops, the data is given in levels. What was it? An average income and Test scores. And I just, I just log them. Oopsie. I just logged the data, or did I break it? Oh, I'm so sorry. OK. Um, and what you create is two new variables. One is called Xtilda, which is the log of average income, and Y tilde is the log of Um Who were the test scores. Now, consider the following. The following 4 models or regressions that you could run, the original one. OK, actually, we won't be running that. We've run this before, but you could run the linear log one where Y is the original one, the original variable, but you regress it on the log version of X. And when I say log version, I always mean natural log, right? Don't take um log based 10 or something like that. Um, log linear is you take the dependent variable in logs and you regresses on regress it on the original independent variable and then the other option is both are logs. So in the generic interpretation in all these models for the for the slope is and this is a very. Cautious formulation, the slope coefficient beta one as always, by how much does the dependent variable change on average? The on average one is always an important qualifier. When the independent variable changes by 1 unit. So let's make sense of that statement in these 44 models. So the first model we are quite familiar with, where beta one is it's always the change in the dependent variable over the change in the independent variable. Here they are the original data. So if I change X, if the change in X is one. If you just plug in one here in the denominator, you get that beta 1 is the change in Y. So, the short formulation is X up by 1, Y up by beta 1 units. So no, no surprises here. Now, if I tell you, I'm, so this is now the, the model what I'm doing here, where you regress Y on the log of X. You go, if I change my independent variable by one unit. So my independent variable here is Xtilda, the logged data. So if the change in Xtilda is one, I plug that down here in the denominator. Well, then beta 1 is equal to. The change in why? That's why I'm writing here. But the interpretation, taking this or mapping this back to the original data, X is a, a one unit change in, in X tilde translates to a 100% increase in X, right? This is because if, if the log change is one, that means I have Changed 1 by 100%. Here. So if X goes up, so this maps it to the original X. If X goes up by 100%, then Y goes up by beta 1 units. OK. Now, I've also told you that the log approximation doesn't really work for 100% changes, but that's the mechanical interpretation, and we will actually break this down to a smaller percentage change in a second. But let's work through the other examples. The other example is the third one is, uh, the original regressor, but the logs dependent variable. So, Beta is, if you change the, the regressor by one unit, here the regressor actually being the original regressor, then plug that in here, in the denominator, you get that beta one is equal to the change in Y tilda. So, beta one. Captures the change in the logged dependent variable, so mapping this back to the original dependent variable, if X goes up by 1 unit, the original dependent variable goes up by 100 times, beta 1. percent And yeah, I don't think I have to read through this because we sort of have to take a break soon. And the other option is if X goes up by 100% and Y goes up by 100 times beta 1%. I'll show. I'll, I'll, I'll go through all of this in actual um estimations here, as you can see, but let's take a 10-minute break, and then we will go through that and wrap up, um, this lecture notes. So 10 minutes. Break That's I if they're both exponential to the same

SPEAKER 1
thing. And this one gonna be equal to.

SPEAKER 3
Well, You take your original data it's not you log that. These two will be different models. So, or I guess the same on both sides of the great so what have you got in mind do you want to, oh, I see what you're saying. I, you take the original data and you. Explanation, yeah, yeah, yeah, yeah, that that would be the

SPEAKER 1
same so the one would be good. And I was thinking like there are ways to pseudo linear linearize the. right. I know.

SPEAKER 2
I could, for example, take transformation of Y Y I

SPEAKER 1
is able to X2 do the linear react. So then I would just, I could just make this

SPEAKER 2
whole thing I would have the actual linear regression through linear regression. Yeah Instead of instead of regressing with respect to graduate,

SPEAKER 1
I would just takeation why why is equals 2 and then I regress on that. I send a linear aggression and then at the end I just have to square my resolve to get the

SPEAKER 2
interpretation. You I'm just taking, I'm just taking a trans like

SPEAKER 1
instead of plugging investing in Python, I would transform this data frame into this data frame where why is it well actually I'll just take it I actually get off. I'll just take them. And Then I'll get the you are taking your level and. I just try to like just do a change of varis then I would, you know. This would be. Oh, I see what you're, yeah, so I could do that and I was thinking like if it's quadratic roots can take a log, you know, I take the outside. I was wondering, are there any types of relationships where you cannot linear linearize.

SPEAKER 2
Oh. Well, From polynomials. See like Yeah I suppose it depends on whether that

SPEAKER 1
relationship has been.

SPEAKER 3
That's what I was thinking. Thematic transformation. Noted. the So he shouldn't work at the screen because you

SPEAKER 2
because you get um. And then OK. Because it's if our regret regresses or more numbers then

SPEAKER 1
I guess in real world you can probably I think that's.

SPEAKER 3
I guess the answer is the adjective transformation.

SPEAKER 1
Yeah, but I like my methodology here between the variables in general like it can work I think so turn

SPEAKER 2
it into a line. Yeah. And oftentimes you want to.

SPEAKER 3
Explicitly interested in the.

SPEAKER 1
I could, I could, you know, still get the same

SPEAKER 2
result. No that, um. I Yes. I Lungo-K. Uh Um. And All you C I just Yeah The. What This Yeah, it's it's still isn't it. So I That It's You guys Mhm I see. episode This. No. This one's This is a lot of risks that have been from. To Oh. And you already And wasn't your thoughts. but. It seems to suggest Oh. Yeah It's true, 15 year re takes 2. So that one yeah. The I can't remember.

SPEAKER 1
I just I couldn't.

SPEAKER 2
Well people immediately say that. So the I So Is this regression and regression. Yeah, one direction, uh. And the residual you mean is just. And this one is. Oh. So those are just Any observations. the Right. I I it's like a flip side of the same

SPEAKER 1
point.

SPEAKER 3
Yeah you're taking a shadow.

SPEAKER 2
If you just intuitively. It's like you can measure somebody's height by putting a

SPEAKER 1
little bit here or you can measure the shadow on the floor and they get the same.

SPEAKER 2
Just right.

SPEAKER 3
Well tomorrow so. OK. Oh Shall we resume? I haven't fixed this yet. Uh, I don't think the School of Economics has enough money to replace this after. Sellotape it, right? Let's run these regressions. And see what happens, what what should happen? The question is how do we interpret the regressions? What are we doing here? Oh, this is the linear log specification where the dependent variable was the original test score variable, but here we use a Pythonic way of including the logarithm, the natural log. You need to load or import NumPy. Lumpy, nay. OK. Um, so that Python can take logs and the natural log in Python is called LOG, not called LN, OK. Good to know. OK, so we're doing that. And so first you see, well, it's just another regression. And the interpretation now is this. So the number that we obtained is 36, but what's the interpretation? So the interpretation is the generic, 12 slides earlier, or 3 slides earlier is always what I wrote, wrote here at, at the bottom. Uh, independent vote goes up by one. The and the dependent variable changes on average by beta one. OK. So if in in in this regression, the independent variable goes up by 1 unit, but this is a logged independent variable, mapping this back to the original X variable, which is invariably what we do, means a 100% change in X. So if log X goes up by 1, that means X must have gone up by 100%. This is just a thought experiment. This is not really uh the percentage approximation that we want to use because that's too much of a jump. But we can interpret it that way and let's just read through this. So therefore, so a 1 unit increase in this regressor here increases Y by 36. That means mapping this to the original regressor X. A 100% increase in average income is associated with an increase in test scores by 36 points. But we like to keep our changes small, so we just break this down to a 1% change, so we divide everything by 100. So then the interpretation is, so every everything divided by 100, a 1% increase in average income will in this case increase. It's expected to increase test scores by 0.36. All good? You know, then for the rest, for the other. Combinations, it's not much more difficult. Let's run through them here we are regressing the log, the natural log of test scores on average income. When we do this, we get a coefficient, a slope coefficient of 0.0028. So the interpretation is here, the regressor is the original regressor. So one unit, in this case units were dollars, a $1 increase in average income will increase test scores by this number here, the estimate times 100. percent So 0.28%. And Then the last option or combination is everything is logged, and I'm getting a coefficient estimate of 0.055. 4 And then the interpretation is if my Regressor and changes by one. The regressor being the log of average income, that maps to a 100% change in the original regressor, average income. Then my dependent vari would go up by. Point 0554 times 100%. So 5.5%. Now this is interesting this specification because we're doing a lot of microeconomics today. That is the interpretation of an elasticity. Exchanges by 1%. Why changes by some percent. That's in elasticity, so the log log specification spits out in elasticity. So a 1% increase, so if I break it all down to a 1% increase, that's what we do with elasticities. A 1% increase in average income will increase test scores by 0.055. Pent in just a random reminder. Don't forget to say on average or say stuff like is expected to or tends to or is associated just to signal to me or to the markers of the final. I shouldn't do this distraction, but you understand that. It's on average, OK. That was just a detour making you nervous about something that is still weeks away. Any questions on this? Oh, cool. Too easy. Um. Which one do you choose here? This is more, it's less. A substantive specification, more like how you like to interpret your regressions. I, I would say these regressions offer you roughly the same information. It's just. The percentages or combinations of percentages. We saw this, was it last week we did the returns to schooling. We call it a return or rate of return, because we logged the dependent. And we could interpret this as a semi semi-elasticity anyway. Let's move on. We still have to To the next topic and then have a relatively small workshop session today. Are there, are there any questions on, on this, on logarithms? All right then, thank you. And let's move on to the last topic, interaction terms, interaction terms. Let's look at our model again this time regressing test scores on student teacher ratio and the English learner percentage. We have seen this data before. Remember this is the data set from California where many students are native Spanish speakers. Um, with the immigration background coming from Mexico or South America, um. And so you capture that through the English learner percentage real. Um, OK. We've run this regression before. This is just a reminder. We get a student-teacher ratio estimate of -1.1, English learner percentage of negative.6. Uh, this is all very interesting, but that's not the point, um. We do understand, do I say this all correctly here? Um, if District I could decrease the student-teacher ratio by 1 unit while holding English learner percentage constant, so this is important that we always say holding other stuff constant, the school district can expect an increase in average test scores of 1.1. OK, so that's using this qualifier is expected to increase. And then likewise, a thought experiment on if you somehow could change the English learner percentage, I don't know how that works. You just move students to other school districts. And anyway, this is the interpretation, but the segue is um As this now. So you're interested in studying the thought experiment in studying class size reductions. If you could somehow make your classes smaller, and the question is, maybe reduction, class size reduction or smaller student-teacher ratio is more effective in some circumstances than in others. And the circumstances here are described by the other variable in the model. So, for example, the effect of the student-teacher ratio on test scores varies with the percentage of English learners. The story is this, for example, if English learners, or native Spanish speakers in this case, benefit disproportionately from smaller class sizes. Maybe so making the story more complicated. Um, so more technically, The effect of a student teacher ratio change on test scores might itself depend on the other variable. In this case, English learner percentage. Or more generically in a diff in a generic model, that change of Y over the change in X1 might depend on the other regressor. And this is modelled. This possibility is modelled by so-called interactions between the two regresses. Let's have a look. I start with the simplest model, where both my included regressors are dummies, and I'll generalise this step by step to, do I make it two continuous ones? I think I do make it two continuous ones. We'll see. We start with, the two regressors are dummies. And If I give you two regressors that are dummies, and I say run the regression, and this is the model that you have in mind. But I want to allow for the effect of changing the first regressor, which is called D1 here, just to remind you that it's a dummy, to depend on the other regressor. So the way to do this is to multiply the two. And include that as a separate regressor. Then we have a multiple regression model with a 3rd coefficient, 3 slope coefficient. And the way I'm just, just putting this there is mechanically accepted. This is the 3rd regressor. The product between those two is not the same as as D1 or D2. The product between D1 and D2 is a completely new variable, the third variable, but it has this, it captures. This notion of How does the effect on one regressor depend on the value of the other regressor, and we'll see that in a second. OK, and so as soon as you include a product of two regressors, it's called an interaction term. So and you included as an extra variable here. Interpreting the coefficients, because it's a dummy model, it's relatively easy to make sense of. You can write down the expected values when both dummies. Take on particular values, and the way I'm framing it here, is I'm interested in in the effect of regressor one here called D1, on the dependent variable, so I'm interested in a unit change in D1. While the other regressor is fixed at some value. I could formulate it the other way around, but it's symmetric. So capturing or explaining it. This way is sufficient. OK. So the way I'm framing it is, I'm interested in a unit change in the first regressor. That's why here, I'm moving D1 from 0 to 1. Remember, these are dummies. So that's, that's the only thing that can happen here. While the other regressor is fixed at some value Q. There's not many options here either, because the second regress is also a dummy, but you can sense this will become more flexible as soon as I let them be continuous variables, which I will do in a second, but here, let's stick with the dummies. So I'll just write, write out the regression equation from the previous slide. So let's go to the previous slide. If I plug in 0 for D1 and the value Q Q is a generic placeholder for any value that the second regressor can take on, and I get. Beta knot plus 0. Plus Bayer 2 times Q.

SPEAKER 2
Plus 0 And I was studying the expected value so

SPEAKER 3
I don't get an error. So that's what I'm writing here, beta not plus beta 2 times q. If I tell you, oh, but the first regressor actually takes on value 1, you go back to the previous slide and what you obtain is beta 0 plus beta 1 plus beta 2Q plus beta 3Q. You compare the difference, what do you get? beta 1 plus beta 3Q. What's the difference? What changes between these two equations? Only D1. D1, when it changes from 0 to 1, Has the has the following effect on the dependent variable, beta 1 plus beta 3Q. Without an interaction term, it would just be beta one, the effect of the first regressor on the dependent variable. Now we allow the effect of the first regressor to depend on the value of the other regressor, which is, Captured or covered by. Because in this. Little model, the second regressor can also only take two values. Q is a placeholder for 0 or 1, so there's also not that many options. But we generalise this in a second. Um And then The interpretation that you see subsequently, and on this slide of beta 3 is the increment to the effect of the first regressor on the dependent variable, when the second regressor, in this case, takes on value 1. So beta 3 is an incremental effect. I think I've got an application for you. First, I need to go back to my original data and dummify it because there aren't actually dummies, but I can dummify it. I can make it so that there is a dummy variable that's called high student-teacher ratio if class size exceeds 19 or 20 and larger, or 0 otherwise, so that they've artificially created a dummy. And the same based on the English learner percentage. I looked in the data centre and I saw that So you can speak of a school district having a high English learner percentage if it is 10 or larger, this is in. That's just what the data told me. So I've created these two dummies and I'm running this regression. Here, how do I do this in Python? So I, I hear, I actually have hard code, this wasn't me, this was Alex who coded this, um, two dummy variables, added to the data frame based on the original data. But Never mind that, you run this regression, if you have 22 variables in your data frame, how do you interact with them? In Python, you just put a star in between them. What it, what it does then, it includes the dummies separately. Those are beta 1 and beta 2. But also their product. So you have 3 regresses. Here you see, you have the high student-teacher ratio, you have the high English learner percentage, and this year is their product. And I would say they've got still the expected signs. So high student-teacher ratio is not good for test scores, negative sign. High English learner percentage is not good for test scores, and the product we interpret now. So The way I'm framing this is in terms of the effect. Of the high student teacher ratio on test scores. If I tell you that high English learner percentage is zero, then this regression output tells me that the effect of high student-teacher ratio is -1.9. That's just this coefficient estimate that is beta one. But The effect of the high of a high student teacher ratio when the English learner percentage is high. Is the baseline 1.9 negative, -1.9, and you subtract of 3, because this is a negative, 3.49 as well. So that's the increment to the effect of the high student teacher ratio in school districts that also have a high English learner percentage. Does that make sense? So you get two slopes, if you will. A slope that applies to so excuse me, when I talk about slopes, now I'm the way I'm framing this, I'm discussing the effect of the student-teacher ratio on test scores. So in school districts that are. That do not have a high English learner percentage, the slope is -1.9. And in test scores who have a high English learner percentage, the slope is negative 5.4. This does seem to suggest. This was the thought experiment. Um, So the way I'm phrasing this is a policy experiment. One way to, to read this is, um, school districts with high English learner percentage are disproportionately, uh, disadvantaged. Um, No, I'm not phrasing as well. A high student teacher ratio has a worse effect in High English learner percentage school districts. OK, and as a policy experiment, if you could, if you could reduce class sizes, you would. Think that benefits school districts with a high English learner percentage more, OK. Um, All right. Any questions on, on this one? So this captures the main ideas, yeah. The interactions Oh yeah. It's not, but we, we ignore that here. But also because I just arbitrarily find these dummies. It's just a vehicle for me to present it. I don't want to delve too deeply into. Now The, the statistical significance of them. Um, we will move to to the fully continuous model, and then I think then we might actually look at that. Let's just have a, let's just, uh, let's move there slowly. Now the, the regressor that I'm framing this around is continuous. The other regressor is a dummy. I'm still including the product here as the interaction term. And the interpretation, if I if I write this out here I'll actually have this equation. So the thought experiment, since X now is continuous, you fix it at some value, little X. And add one to it. While the other regressor here, the dummy, is fixed at some value Q. Which could be 01 really only, OK? You work this out, but you get the same result. What is the effect of changing the first regressor by one unit? It is beta 1 plus beta 3 times Q. It depends on the other regressor. So that's really the same, and this line is really the same, really. Interpretation of beta 3 is the increment to the effect of X or Y when D is equal to 1. How do you run this? Oh, actually not quite there yet. Um, you could view these two cases as two different PRFs where the intercept is different and the slope is different. To see this, this was the interactive model. You just reorganise terms. Um, and that is Because the second regressor is a dummy, you can move beta 2D into the constant here. And here you have D times X, you move that into the slope coefficient. You go, if D can take on two values only, you get Two possible constant terms when D is equal to 0, you get only beta kno. When D is equal to 1, you get B plus B2 beta knot plus beta 2, and likewise for the slope. So depending on what D is doing, what what value D takes on. You have Different intercepts and different stops. So that's another way of interpreting that. And how do you run this technically? Well, we, we just include the interaction term this time between STI itself and the high English learner percentage. This is just the multiple regression model with 3 regressors where you have STR in there, you have the high English learner percentage, and the product between them. And the interpretation is you get a slope for the group in which high English learner is zero. That slope is. -0.97. And a slope of the for the student-teacher ratio when high English learner is equal to 1 is, well, you get the negative.97 and take off 1.2766 in addition, OK, it goes down further and you obtain a slope of -2.25. You get two different slopes. We've discussed this. One slope for the school districts where you have, we don't have high English percentage. And the slope for when the high percentage is one. So it's Um, shows you yet again that um the, the effect of the student teacher ratio in both cases is negative. It's more negative, it's worse in school districts where people speak more Spanish. OK. But so what you're doing here is you're modelling the effect of the student teacher ratio as a function in the other in the other regressor. Here the story is, maybe, um, people with a migration background disproportionately suffer from uh from a high student teacher ratio, and that's what's going on here, at least that's suggestive of that. Um, now, you can just use, um, our statistical tests from the previous weeks to, to test whether these two regression functions are actually different. So, we said we can write this out as two regression functions, one that applies to the zero group and one that applies to the group where the dummy is equal to 1. OK? And that, and your comparison is, are the two intercepts the same? Are the two slopes the same? And what else can we do? To Yeah, right. Are they both simultaneously the same, or is, is it separate? So in the first instance, we're asking ourselves, Are these two regressions identical, which boils down to the question, is beta 2 equal to 0 and beta 3 equal to 0 at the same time. So that's a joint test, F test. That's what I'm running here. And so I'm asking. is the coefficient beta 2. So that's the one that belongs to the high English learner percentage, and is the coefficient beta 3 equal to 0. Um, so that this year is the interaction term, and this is the high insurer percentage, and this is rejected. So they are not the same. Then you can still ask yourself, well, maybe beta 2 is 0. why am I doing this? Let's, let's go with beta 3 1st. We test that beta 3 is equal to 0. You can just go into this regression table and look at the TSA. OK, it's too close to 0. We cannot distinguish it from 0, so we do not reject. And for the other one, for the um. So this is what I'm saying here. T statistic is -1.32, we don't reject that one. And for beta 2 coefficient on the high English learner percentage, TSA is also. Too close to 0 to distinguish it from 0. So we do not reject these null hypothesis, but we do reject the joint hypothesis that both of them are zero. Just playing around with statistical tests in the setting and then. generalising this once more, now for allowing for the 2nd regressor to be continuous as well. So you have the product between them in the regression. And this is really the same exercise and it still boils down to if you change the first regressor by one unit, holding the other regressor fixed at some value Q, where now the value Q is a little bit more interesting. It cannot just be 0 or 1 because X2 is allowed to be continuous, it could be some other value. Then The formula is still the same. You get beta 1 plus beta 3 times Q where Q can now vary continuously wherever the second variable takes on values. OK, and it's still the increment to the effect of the first regressor on the dependent variable when the second regressor is held at some value Q. How do you run this regression with the original data, you just include the product and it adds this as a regressor here. Now, how do you make sense of that? This is actually the, it's not almost the last slide. How do you make sense of that? So what we What we Um, here is the effect of the student-teacher ratio to be a function of the value of the other variable. So then I went into the data and looked what values can this variable take on, this variable being the English learner percentage. I looked in the data and I saw the 25th percentile was 1.94%. So 25% of school districts. Um, Have an English learner percentage of 1.94 and lower. And the median is 8.85%, so half of them, or the middle school district has 8.85% of English learners. This is just what I found in the data and then the table reports other values to you. So some school districts have more than 44% English learners, and that is 10% of school districts have that. OK. Now, these are values that X2 can take on. So what I can do is I can stick this into my regression. Have I written this down anywhere? And Um, so this is, these are, this is the evaluation of the PRF, the estimated PRF. So, let me go back to the, um, previous regression output, and let's plug or pick out an English learner percentage of 8.85. Going back to this regression, what I want, what do I want? Sorry, come back here. Um, oh yeah, I'm just interested in the slope of the STR. So, the slope of the SDR is 1.1170, but also plus. 0.0012, the interaction term, multiplied by the value of the English learner percentage that I'm interested in, was based on the table, as I said, the median was 8.85. OK, so it's so multiply the, the beta 3 estimate by 8.85 because you see, This estimate is quite small. If I multiply it by 8.85 and add that number to -1.17, I get -1.11. It doesn't actually do anything. Can everybody follow my calculation or who wants me to explain more? So likewise, I can evaluate the student sorry the English learner percentage at other values. For example, this is almost 44, so I go -1.1170 plus 44 times 0.0012 is the effect of the student-teacher ratio in school districts at the 90th percentile of the English learner percentage. The bottom line is, here there's not a lot of action because the interaction term is estimated to be quite small. And when it hits a number like 44, not not not much changes, not much happens, OK. And so I get, I get this variation here, um. But what I do get is that the effect of the student teacher ratio is decreasing in the English learner percentage and in absolute value here. Differences don't seem sort of meaningful to to waste too much time, OK? But this is just how they work work out in this example. And then just playing around with statistical tests, um, I can look individually at the interaction term and at the STR coefficient in this table, um. Mm, the interaction term is not statistically significant. The student-teacher ratio term also not strictly speaking, so too close to zero to distinguish them from zero, but you can do a joint test. Where it tests the interaction term. Add the slope term that belongs to the student-teacher ratio for zeroness and I'm rejecting. Is that true? Yeah, I'm rejecting. Um, So this is yet another example where the individual hypothesis, you um You do not reject, but jointly you do reject. What can you do? But the, the bigger topic is In actions Allow you to model the effect. Of the change that one variable has on the outcome in a more flexible way. And in a way that. Makes it depend on the value of the other variable. Um Are there any questions? And we can see this now. In another example, it won't take us very long to talk about it, which is the question that I posted. Uh, for the workshop. Exercise. Let's have a look at that. There you go. This one, this one in the book. So, there's data on houses and house prices. I don't know where this probably data from the US. If I zoom out. Uh 11 more step. Another step. What am I doing? Can you guys see this? You can say no. Can you see it? Oh Where money Is it too small? OK. I don't, can you guys see it? Yeah, good. Um, all right, so. House price data This is probably from the earth, it's from the textbook. So what, what you see in every column here, where's my pointer? I A regression in column one. Column one. What you're doing is you're regressing the log. Of a house price but a house sold for on its characteristics in the economics is sometimes called a hedonic regression or yeah that that's what it is. So we go what are attributes worth to you? Like is it good to have a large house that's its size to have um. That's the number of bedrooms here. Or a pool or a good view, the interaction between them condition meaning is it in a good condition or not? And then there's an intercept. So in the first in the first regression, the first column here, what's happening is we're regressing the log of the house price on its size. Size is reported, this is in the US in square feet. Don't even know what that means. OK. But it's the size of the house. OK. Larger size means higher price. At least that's what this coefficient estimate suggests. So the coefficient estimate is even I got this 0 point, is it 400 or 300 and then 42. Um, what else is included in this regression? Whether the house has a pool or not, it's a dummy for having a pool or not. A dummy for having a good view or not, where I don't even know where this comes from, and who can objectively measure this. Let's say the agents because they're so objective, but let's just accept it. It has a good view. OK? Um, so you see, having a pool is a good thing, positive sign. For the house price. It increases the price of the house. Having a good view has a positive effect on the price of the house. And condition, what is condition? A binary variable, yeah, if, if the real estate agent reports that the housing is in excellent condition. OK. So that's a good thing too. Now, I don't know what I'm asking down there, but let's first, so this is one regression, and we talk about the other columns in a second, but let me ask you this. If I tell you my house has a pool, what is the How do you interpret this particular coefficient estimate given that the dependent variable is. A log Who wants to have a go at it. So the number is 0.082. What's the interpretation?

SPEAKER 4
That Holding all good. The house has Then the low price. Will be 0.082. Greater so interpreting your price, you can phrase that as a. Yeah. So Um, 100 times the coefficient of the 8.

SPEAKER 3
Perfect. Cool. So, if the house has a pool, this is why this area is quite nice. It's a binary row. You have a pool. It increases house prices. I should be careful. It is expected or predicted to increase house prices by 8.2%. Holding everything else constant. OK. That's exactly what you said. So, cool. Thanks for that. And then likewise, for the, for having a good view. The return to having a good view is 3.7%. The return to having it in good condition is 13%. This is all on average everything is constant. OK. The exercise at the bottom asks part A, uh, I think you're doing a renovation and you're adding 1500 square feet. This is what I'm saying there. This is from the book. Yeah, so you're adding 1500 square feet. How does the house price increase? The expected increase, holding everything else constant, who wants to Work it out for me. I don't need to know the actual number. Just the, the way. So the coefficient estimate is 0. 0042. For the size of the house. So every square foot adds that to the dependent r. So if I tell you my renovation makes the house bigger by 1500 units. What happens to the house price? Goes up by how much? OK, because we have to move on with our lives. Um, take this number here. By 1500. Then by 100. And then put a percentage sign to it. I don't know what that works out to. All right. What happens in column 2? column 2 is just a different regression. You still have the same dependent variable. But now instead of the size of the house in square feet, you have it, you log that. Otherwise, you include the same variables. Pool view and condition. All you do is you change the The size variable from square foot to its log. So you get an estimate of 0.69 on the log. So this is now a log log model. Is, am I asking anything in part B on this? Yeah, right, exactly, it's about. Part B is exactly about how to interpret the coefficient of log size. Who wants to interpret it? You go elasticity. Yeah. So it's an elasticity, is it a big one or small one? So the interpretation, like the mechanical one, if I increase LN size by one, And LNY will go up by 0.69. So breaking this down or mapping this back to the underlying original variables, um, a 100% increase in size. Would increase um Price by 69%. But then we really don't think about it that way, so break it all down by, by a factor of 100. OK. So 1% increase in house size increases price by Point 0.69%. OK. Now, let's move, what else is happening here. The number of bedrooms. Let's, let's jump into the last column. Number 5, we have an interaction term. So we've got a lot going on. Looks and interactions in one picture. So what's, what's the, what's the effect? Of pool on house price. Now it depends on the view. You sit in your pool and you have a Wanted to use a swear word, a bad view. Then what's the value of the pool? Let's say you, you, you have, you look at the, uh, what can you look at? Nuclear power plant. Oh, not, not in Australia, um. Uh, this is US data, then having a pool is not such a good thing. OK. But the, the interpretation is this. The effect of a pool now in column 5, you still regress Ellen Price on the same things, but also on the interaction of pool and view. So it depends on whether or not you have a good view. If you tell me I have a pool but not a good view, then your pool effect is. Who tells me the number? 7.1%. Having a pool is expected to increase house prices by 7.1%, holding everything else constant. If you tell me, luckily I do have a nice view, then you go the benefit of a pool now is 7.1% and the increment 2. 2.2%. 0, 0.022. So it is 7. No, no, I'm bad at maths. 7.32%. What am I doing? I'm adding. The pool number and the interaction number together. So the increment. To the effect of having a pool is 0.0022. OK. And so it's the, it's the extra benefit of having a pool. It's extra good to have a pool if you have a nice view. I just, 11 last thing if we have a minute, I can confirm we do is. I can frame this. The other way around as well. I don't have to frame this as what's the added benefit of having a pool when view changes. I can ask symmetrically, what's the added benefit of a view when pool changes? OK. So, First, I can go having a good view. If I don't have a pool. Gives me a benefit of 2.7% on average. When you go, but I do have a pool, it gives you. 2.7% plus the 0.0022 in. OK. So 2.92%. Does that make sense? So the benefit of having a good view differs in pool likewise. It's sort of the flip side of the coin. The way I discussed it in my slides was I only always framed it as I change X one by one unit. And then the effect depends on the value of X2, but you could have discussed it the other way around as well, and here I'm just pointing it out in this table. You can go if if there's a positive effect of having a view, and that is allowed to differ on the presence of a pool. Those all make sense. Just check whether it's financially a good investment to have a pool. 7%, yeah, it sounds pretty good. OK. But uh in any case. This finishes mainstream. Regression analysis or regression, yeah, multiple regression. And next week, we'll start time series analysis. OK. Have a, have a good week and do not forget your, what is it, quiz, the week 9 quiz, all right. See you next week.

SPEAKER 2
I would because I think that she. Um, someone. What Thank you. You know, See you have a good week.

SPEAKER 3
See you To To pay you.
