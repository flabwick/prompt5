SPEAKER 0
This material has been reproduced and communicated to you by or on behalf of the Australian National University, in accordance with Section 113% of the Copyright Act 1968. The material in this communication may be subject to copyright under the Act. Any further reproduction or communication of this material by you must be consistent with the provisions of the Act. Do not reproduce this material. Do not remove this notice.

SPEAKER 1
OK. Good afternoon. Welcome to week 11. Um, the portable mic is out of battery, so I'll just stay here. Excuse me. Well, I'll charge it there and then we'll try again later. So I'll be stationary here. Which reminds me of the first slide, but before we talk about the first slide, uh, any questions on course material? Course logistics. Is there a deadline today? Can anyone confirm? I chose my deadline quite poorly, like it's uh, is it at 5 today? Who here has not submitted yet? Who's brave enough to OK, never mind. Um, how was it? The, this is the assignment, right? OK. Yeah, OK. As you know, we, we mark in a friendly way. Um, So well. Uh, do more time series today and we'll wrap, we'll wrap up next week, but we won't eat the entire hour, two hours next week, but I'll, I'll work through. Uh, a practise test. I think that it is already on the website, so I'll, I'll work through that. To demonstrate to you. What level of detail we expect um in your answers. So that's that's next week. What else? There's another quiz next week. Uh, yeah, anyway, any, any questions? OK then, let's do more time series. Is this oh wait just a second. That's working, um. Stationary and trends is today's topic. It's quite important to cover this because that is. Behind or this this explains why we can run auto regression, why it's defensible or justifiable to run auto regressions. So let's have a look stationary. We, and we've, we've seen this, uh, I think last week already in the workshop session, we called or defined a time series to be stationary if it's probability distribution does not change over time. And then If it's not stationary, we call it non-stationary. Um, so what we exploited last week. When we did we study the mean of a time series, then the mean is independent, doesn't have a T subscript. If the time series is stationary, the same for the variants, and the assumption of stationarity entered our definition of the. A correlation. Um, just, just a random note here, but, um, what, what I'm saying here is that we want to work with stationary time series, and I'll show you now why that is. OK, that's the idea for the next few slides. Intuitively, so this definition sounds quite soft, but it's relatively rigorous that the probability distribution doesn't change over time. What that means so it's like a statistical definition. It's about the probability distribution, but intuitively. It means that the The future is Distributed similar to the past. That's like a statistical definition, but what's intuitive perhaps is that you would want that requirement to be able to make useful forecasts. If I tell you my time series for inflation, we'll have a big structural break tomorrow, then you can use as much data as you like from the past. It doesn't help you. Make a forecast for tomorrow if you know there is some sort of irregularity happening tomorrow. Um, At the same time, It's it's a restrictive assumption. I think about. Um, the GFC, we saw that in the data we had this big, big drop in inflation or deflation. Um, that seemed to be a situation where the assumption of stationarity did not apply and just. Put yourself in in the position of a forecaster in 2007. Before the GFC And they can make their forecasts based on based on the assumption of stationarity. They give you a good forecast and then the GFC happens, which you can sort of see as a structural break like a like a non-stationary event or an event that that makes your time series non-stationary. So the forecast that's based on previous observations, it's not completely useless, but you would, you would sense that it's it's. It would be poor. OK, it's very reasonable. OK, so, to deal with time series. That allow us to make good forecasts. We employ this concept of stationarity, which technically means that the distribution of the time series doesn't change in time. Um, if we believe the future to be fundamentally different from the past, then using past data to make forecasts for the future may be unreliable. I think that's quite clear. So we would like our time seems to be stationary. There is one particular threat to stationery, and those are trends. There's two types of trends, but before I mention those, a trend is a persistent long term movement or tendency. OK, this is a bit soft or unclear, but I'll I'll explain more in the next few slides. There are two types of trends will be mostly preoccupied with. The second type. The first type is called a deterministic trend. They are non-random functions over time. And Relatively easy to deal with because you just can can throw in. A flexible function of time on the right hand side, but I just looked it up again in our textbook. They actually are. Or go as far as saying. That time series econometricians, which I'm not. That's why I have to open the textbook. Um, time series econometricians. Don't really think that they are deterministic trends in time in in econ economic time series. Because this is like saying, for example, think of GDP. Why would GDP follow a pre-described trend? That's just, I mean, you can throw it in as a possibility, like a function of time on the right hand side of your auto regression, and we will do that. But I, I do want sort of to take your focus away. I mention the existence of deterministic trend, but economist econometricians, time series econometricians don't really think. That they feature prominently in time series. The only example that I could come up with are season seasonalities. By seasons I mean the 4 seasons of the year. So if you think of construction in the building industry, for example, at least I do remember this from Europe, where you have a real winter, you see construction varies by season. There's less building construction in winter. And more in, in the warmer months, something like that. And you can think of this as a deterministic trend. Um, but if I think about GDP or inflation, I don't see any reason why there should be like why time should play a deterministic role in explaining these things. As opposed to stochastic trends, that is the type of trend that really causes our problems here, um, that econometricians think could be present in data, in time series data, but I will tell you how to deal with them. They are random functions of time and do require econometric testing, which I'll cover in the next few slides. And we say that time series with stochastic trends are non-stationary. I'll, I'll prove that in a second. So first, deterministic trends that Here are 3 models that Give you a 3 time series X, Y, and T that that are not really auto regressions at all. They just have on the right hand side deterministic functions of time. And that's basically um what are they called polynomial functions, power functions, linear function, quadratic one, a cubic one. You could have any function of time, but um What you can do in auto regressions relatively easily like we did also when we studied in week 9 functional form, we considered throwing in a regressor and it's squared or it's cubic. That's what we do here where that regressor is now time. Um So these are 3 time series. Which follow a deterministic trend. Liar, quadratic, or cubic. And if you were given the time series Y, the middle one, and somebody told you this follows a quadratic trend, what all you need to do is throw in T and T square on the right-hand side as a regressor. So that when T increases by One every period. Um, now, if, if you don't know that. Oh what exactly the, the Deterministic trend is it doesn't hurt. Throwing in a few orders of tea and let the regression determine. Um, how important the time trend is. So you can always estimate the coefficients, and we will do that uh when we study more general AI models in a second. And a picture here of a deterministic linear trend, as you would expect it. So I generated this on the computer. Um, I gave it the Intercept 200 just for fun. And um Gave it What do we do? Oh, the linear time trend is slope is 85 here and as T increases, the time series goes up on average by 85, and then it fluctuates around that slope, um, because there's an error. That is very linear looking. Here's a quadratic looking one. And then, oh, I don't have a cubic one. I thought I had a cubic one for you, but you know, what's the point? It's, it's just a deterministic function, but what I'm trying to say here is We don't want such sim or we don't want to think of our time series as simplistic objects like that. Um If they have a component. That varies in time. We can throw that in like a higher order term, like a linear term, a kinetic term, but the most important component that we want to deal with are the stochastic trends. Um, so here, let's talk about stochastic trends. So they create bigger problems. And a simple example is our AR one model, like, like I've written it up there, where the auto-regressive coefficient is beta one. And It is equal to One So, this model that I've written up there, as soon as I tell you that beta 1 is equal to 1, and that the error has a zero mean, then that's called a random walk model. In most general with drift, the drift component is the beta knot term. Uh, beta not doesn't have to be 0. It could be some number and, yeah, some 0 number. And that's just called the drift. Um, Yeah, right. And when I said beta not to 0, I mean, I'm not saying I set it equal to 0. If you have a time series where beta node is 0, then it just boils down to the little line there at the bottom. Y, the current realisation is basically last periods realisation plus an error. There's no drift in this in this model. Why is it called a random walk? It doesn't really matter, but if you must know, if I take. The previous model where I didn't have a drift and I just subtract Y T minus 1 to the other side. All I have on the right-hand side is the error. So that just means. My change is completely random. As I move from 11 quarter to the next, or one period to the next, is that movement is completely random. It doesn't um involve the previous periods observation at all. Does it help you understanding why it's called random walk. It's just randomly walking around, OK? And you don't know where you end up. Um Yeah, and as you can see, so here in the middle. The expected value of the change is 0. Um, so while on average, we think we're, we're not making a change, realistically, you're making changes as you walk from period to period and they They can add up. Why not? So the expected period to period change is 0, which doesn't mean that the realised change is 0, But what that means statistically is that past observations don't play a role in predicting current changes in the time series. So if you go. If I told you inflation was a random walk, which I'm not telling you it is just a. Just um As an example, if it followed a random walk and you ask yourself what's inflation next period, and your best guess is there's no change. OK, over and above what inflation is today, like how would you change it? So your best prediction to walk myself back is to take current periods, inflation as a predictor for next period. Um, touch time series where, um, the past observations don't play a role in predicting current changes are also called martingales, but. Uh, just a random word for you. Here's a realisation of a random walk. I picked this one, so this is strategic. It's like how you can see bias in action. I chose this picture to demonstrate to you that a random walk, although its name suggests it should be quite random, it doesn't have like a direction or whatever, but it can accumulate up to something that looks very trending. It doesn't have to. That's why I mean I'm showing you a picture that is very striking, but if I show you 100 realisation of a random walk, I can't tell you how many of them will look like this. So this happened to be looking very trending, um. Which is just to justify why this thing is called a stochastic trend, but at the same time, I want to clarify that Not all stochastic trends look like a trend. In fact, most of them don't. Like, if I tell you, oh, if I plot you another random walk. Um, an example of a random walk, it might look very non-trending. It's still called a stochastic trend. Um, it's just terminology, I guess, but um here. I've created this time series. I guess the drift helps here. We've got a drift of 85 and then we add past observation plus an error, and that's the realisation here. It's a completely made up time series made up on the computer. OK. Any questions before we? Continue So random walks up. Particular of random walk models or particular AR one models, particular in that the auto regressive coefficient is exactly equal to one. OK. And now, I'm turning to the discussion of What problems they create or what problems these models have or pose for estimation. Um, so, so far no problems, but Two key features of the random walk model is the forecast for the next period is today's observation. I said that already. And if it's a model with drift, then you add the drift as well. So if the drift was 0, be or not, if that was 0, then all you have there is that your forecast. Based on all the data that you've got, it's really just current period observation. Now What I'm working out then here for the variances where for simplicity I assume that the time series started in. Period 0 With the realisation of 0 so that I don't have to go back to negative infinity. I just started at 0. And I asked myself, What's the variance? Of the time series at different points in time. And the big picture is, if you believe my calculation. The variant of YT is Little T times sigma square, where sigma squared is the variance of the error term. So what that means is that the variance is a function in time. Quite, quite obviously, there's a little T in there. It doesn't just say sigma squared. It says T times sigma squared. Of which you see immediately that this cannot be a stationary time series. Because by definition, a stationary time series should have variances that don't change in time. Because the definition of stationarity is that the distribution doesn't change in time. Here, the variance of the time series changes in time. So that suggests that you don't need to know anything about. The shape of the distribution, but its variance changes. It increases in time, OK. So by definition, uh, the random walk. Can be a stationary time. So that's the discovery here. So the AR one model, where the auto regressive coefficient is equal to one. Is a time series that is non-stationary. And the calculation here is relatively straightforward, so. Have a look for yourself. Um, Good. That, so that was an important observation. We are dealing here with a time series that's non-stationary. Now, unit routes. Going back to the AR one model. We now Uh, generalise we go well this generalised we consider two cases where the auto regressive coefficient is 1. Or in absolute value less than one. And depending on these two cases, we call it. Stationary or non-stationary, we discovered that when beta one is equal to one, that's the non-stationary case. And then we go for the case where beta one is less than one in absolute value, we call it stationary. And there could be another case where the auto regressive coefficient exceeds one in absolute value and. We don't consider cases like that. Such a time series would experience explosive behaviour which. You don't see even from stuff like GDP, right, and definitely not inflation. Or exchange rates, things like that. Well, I'm saying maybe maybe in Germany in the 1920s there was hyperinflation and maybe that was such a time series, but then I doubt that you could control and explain it with an auto aggressive regressive coefficient that exceeds one that we just. Don't include that in our discussion of the AR model. So when we talk about the AR one model. We're considering two cases, the stationary model where the auto regressive coefficient is smaller than 1 in absolute value. That's the stationary case and the non-stationary case where it's equal to one. The non-station areas. Another The term or terminology that we use interchangeably when a time series is non-stationary is we say that it has a unit route. Or a stochastic trend. So that's a bit unfortunate, but I'll have to mention that because you'll see it in books or the literature. Uh, you got these different labels for similar situations and we use them interchangeably. So is it three terms that we have. It could be a non-stationary time series. It could have a unit route. And it could have a stochastic trend. So these are interchangeable terms. That create problems. Um, so you could be a non-stationary time series, the time series could follow a stochastic trend or contain a unit route. These are all the same idea. OK, just unfortunately 3 different terms, but you, you get used to them. But mathematically we have defined it as an auto regressive coefficient that is equal to one in the AR1 model. In the ARP model, or generalising even further, how would that map or translate into the ARP model where you have um Higher order Um, auto-regressive coefficients. So it's not as easy as saying all the betas are equal to one. That's not the definition. Um, but turns out, It's a bit more complicated. It's also at the same time, nothing that we deal with mathematically because it gets too complicated, but the definition is this. So on my slide you have. So this is the ARP model. You have got auto regressive terms and you you call or you say this thing has a unit route or is non-stationary or follows a stochastic trend. If, if this polynomial equation here that is equal to 0 has solutions, uh, let me see. So we, we're solving this for that. So the, the betas are given to us, and you're, you're looking for the solutions to this polynomial equations in terms of Z, which value of Z. And make this make the left hand side equal to 0. Given particular values for the betas. So then we say the model contains a unit root if at least one of these solutions is equal to one. In absolute value. Let's do a plausibility check for the case where P is equal to 1. I'm doing this down there. The definition coincides with some here at the bottom now with 1 minus beta 1 Z. We're looking for the Z that makes that equal to 0. So solving for Z. Uh, gives you, if that is equal to 1 over beta one. And that is a root or a solution to the equation, OK? And then according to that definition there in the middle, uh, what am I saying? If one of the solutions is equal to one in absolute value, then there's a unit root. So how would that be equal to one if beta one was one? And Yeah, and now you could convince yourself that if if it's a second order term or higher order term, that is the equivalent to deal with or to define unit routes for higher order or regressive models. Um Any questions? OK Now, what problems do unit roots create? And again, when I say unit roots, I could say non-stationary, T, or stochastic trends. So unit routes, stochastic trends, and non-stationary create three problems. First, and maybe one that doesn't seem too big a problem is the auto regressive coefficients are biassed towards zero. So, Let's say you have an AR one model that contains a unit route, and it's been worked out the sort of Is being offered to us as a free lunch um that in that. Model. So if, if it does contain a unit roots, if the auto regressive coefficient in fact is one. You could one just, you go and estimate this model. You just run an auto regression. The OS estimator for beta one. So that as estimated beta one hat will be biassed. Time series ticians have worked out this bias, and the bias is this. It's it's very peculiar. It is one. So, you would think it should be near one, right, because this is, if this is a unit model beta one should be equal to one. So estimator hopefully is equal equal close to one, but I guess the suggestion here is that it's close to one, but there's a systematic bias and the bias is one. I guess the bias is really. The the second bit here, -5.3 over T. And T is the sample size. So what what waste last week? I think we had 80 years of quarterly data, so about 300, 300 training observations, and you know, it's not really a large bias, but it's a bias nevertheless. Exactly Uh, no, but uh it's, it's chopped off. Um, so you, you could approximate this through computer simulations. Also, for my taste. Unless I have a very small time series, even if I have 30 observations, I feel like this is not a large bias. Um, so it's a bit of a lazy argument against the A one model with a unit route, but it is a random. Fact that is useful to useful to know. Um Forecasts based on this. Could be biassed and then, yeah, OK, let's, let's not say more, but this, this is one problem. It might not be For applications like a very serious problem. Again, if T is a large number. 5.3 divided by 300 is the buyers. Um A bigger problem is the second one, and that's what we will deal with that the asymptotic distribution of the estimator is not normal anymore. When beta one is one. Uh, that is not easy to see. That requires some heavy lifting, mathematically speaking. Um, but let's appreciate what that means. You estimate your, you apply OLS here and Python will spit out standard errors, but where do these standard errors come from? From the assumption that the estimator has a normal distribution. But this estimator here doesn't have a normal distribution. When the underlying model is an AR1 model. With the unit route. So where beta one is equal to one. So, That's why we also didn't bother too much last week to read through the standard errors because we couldn't really know whether this was a stationary time series or not. Um So we cannot simply use the standard errors or TA or P values or the confidence intervals that Python or data or whatever software package you use spit out because they always are based on the assumption that your estimator has a normal distribution approximately in large samples. Um So in other words, in these cases, standard statistical inference will not work and then you would need to know well what is the distribution and that that's not so easy. There's an easier fix to that and that's where we work towards. And the fix is the following we test for stationary or non-stationary. If it's non-stationary, we first difference and then we will arrive at a time series that is stationary and problem solved. So rather than working out what the asymptotic distribution precisely is, we just manipulate the time series and turn it into a stationary one. For which we then can use. The standard errors that Python gives us. And another one. That It won't matter to us a lot, but it is a serious problem in more complicated models, models where you want to expand two time series against each other, so, so. Never mind the example I'm giving you here, but let's say you want to explain. Uh, inflation, Australia's inflation in Australia through, well, through its past, all the observations of inflation, but also what interest rates, exchange rates. Trade measures stuff like that, so where you have more complicated regression models that are not purely auto regressions, but you have other variables on the right hand side as well, um, then that can then non-stationary can create problems and the example that I'm giving you here is from the textbook where you have two time series that are not obviously linked. sort of structurally there shouldn't be a relationship, but econometrically, statistically, I should say there is a relationship just because they're individually containing a stochastic trend. Like Any two trending time series might occur or might appear to be correlated with each other, although they don't have anything to do with each other. So that's the example here from the from the books. The example is the unemployment rate in the US. And Industrial production in Japan. where you would go, what's the association? I mean, there is no association, but why would there be an association? It's not a very clear link between these two. But at the same time, you can see that both of them contain a unit route. When I say you can see, I mean, it requires some testing. So just trust me or trust the textbook. These are time series that that have been for which unit routes have been confirmed. And so when you regress them against each other, And this was now for the years 1962 to 1985, you find an association between the two. And again, don't want to read it necessarily in a causal way, but the, the coefficient is 2.22. It's not zero. And What we would say here is that you detect an association between these two time series that are not related to each other, really. There's no sort of. econometric model that links the two together, but they will be correlated with each other. I mean, we don't have to run a regression, we can just correlate them and you find a non-zero correlation when You find that there there's no dependency really between these two, but they are statistically dependent just because individually as time series they contain unit roots. Does that make sense? Now you can say how does this how does this create a problem for me here in this course where we only do auto regressions. It doesn't, but if we did fancier models where we try to. Explain inflation not only based on its on itself on its past but also on on other variables then you have to be careful that. All the regresses that you include on the right hand side are themselves stationary time series. And then the story continues, yada yada so it's basically a story again that you have correlation but not a relation. I, I'm purposefully not using or avoiding the word causation here because as I mentioned last week, in time series analysis we're not really thinking so much about causation more. Uh, well, our goal is to forecast, right, and, uh, and, and maybe create sort of standard errors, so we, we do want to deal with time series. that are well behaved in the sense that we can trust the estimators and interpret the standard errors properly. So we need stationary time series. Everyone convinced. Yeah.

SPEAKER 2
With these sorts of I, I would say oftentimes it's

SPEAKER 1
agnostic in the sense that you just, what's called the kitchen sink on the right hand side.

SPEAKER 2
the theory to make OK.

SPEAKER 1
Yeah, I mean, as e econometricians, look, I'm not a time series person, but that's my, my main main problem with time series is that they don't. Often use a structural model, they just throw stuff on the right hand side, but I don't want to sort of uh trivialise it. Uh, they know what they're doing. In this, in that day, you can throw stuff on the right-hand side that is stationary. So yes, I mean, you can throw Japanese industrial production on the right hand side. Just make sure you throw a version of it on the right hand side that is stationary. Here, you're requesting two non-stationary time series against each other. Time series connotationss wouldn't do that. Other than that, yeah, I would say it's agnostic in that I don't really know what I should include on the right hand side. This is a very crude simplification because I'm in the other camp, but uh. You use essentially best practises, like if you work on a research topic in time series, you're not starting from zero, right? You, you model your work on other people's work. And then consider extensions perhaps um. Yeah. Long story short, um, what you want to avoid is Uh Running regressions for time series. That are um non-stationary. Does that answer your question. No, didn't. He's like, stop talking. Continue already. Um, alright. Now, having convinced you know that unit roots pose a problem. How can we be sure that we're dealing with a time series that doesn't contain a unit route that is stationary? So because of these 3 problems, we would like to Estimate AR models for Time series that do not. Contain Unit routes. Um So how can we figure this out? And there's a famous test developed by Dicky and Fuller. It's a bunch of alternative tests, but it's, it's still the one to use. It's relatively easy to implement. Let's have a look. For our AR1 model, and I'll generalise to the ARP in a second. What we're doing is we are, well, we're taking the general AR model, AR one model, subtract. On both sides, Y minus 1. Then we arrived here at this equation here, where little delta is the original beta 1 minus 1. That's, that's a simple change, right? You can do that. That's what they've suggested. Just bear with me. So you want to know in a in a way where the beta one is equal to one. Uh, translating this to the little delta guy here, the little delta, not the big delta, this little one here. We want to see. Oh, what we, what we want is a time series where beta one is smaller than one in absolute value, right? That would be a stationary one. So the little Delta here ideally should be a negative number. Everyone with me? And that's what we want for or that's, that's what a stationary time series would have. Because a stationary time series is one where beta one is a number smaller than one in absolute value, so between -1 and 1 in between strictly. So the little delta. In a in a in a stationary time series would be a negative number. In a non-stationary time series, it would be 0. So that's what the tests then. Asks we want to test. For non-stationary. With the non the non-hypothesis is Non-stationary or unit route, which is the same, right? So, therefore, translated to that the little delta is 0. Against the alternative analysis, maybe the only time this semester, I don't know where we do a one-sided test because it can't be positive. Because we've excluded the explosive case. And that's just not the time series that we're dealing with, so the alternative is that little delta is. Strictly residents here. But one sided test. If we Statistically conclude that little delta is 0, then we map this back to the original beta one and conclude that the That, not little, but beta one is one, and therefore we have a unit route. That was a relatively easy argument. So what do you do to conduct this test, you would simply regress, but change of YT on YT 1 because going back to the previous slide, that's using this transformed model. OK. Where you go run the change of the time series against YT 1. You get a coefficient estimate for delta, and you ask yourself, is that delta? Too close to 0. And hopefully it's a negative number, right? for well-behaved time series. The delta estimate that we're getting, the little delta should be a negative number. The question is, is it negative enough? Is it sufficiently far away from zero? If so, we tend to believe that the time series is stationary. If it's too close to 0. It's nonstationary contains a unit. Let, let me talk through my points here. So you cannot, what am I what am I saying here? Complication. What is the complication? I, right. So yeah, you do run the auto regression as well, the transformed auto regression of. The change of YT, so that's the capital delta there against YT minus 1. It's just yet another simple regression. That's just based on transforming the AR1 model. And you get a coefficient estimate for delta. Call it little delta head. Um, problem there is, what's the problems? We, we want to do a significance test, right? We want to know is Is that little delta. Indistinguishable from zero. The complication that I'm pointing out here is we've learned that in the case of a unit root, which is the non hypothesis here, you can't just trust the standard errors that Python will spit out because they are based on the normal distribution, but we have just learned. That Under a unit route, There is no normal distribution, so you can't use. You can't use those standard errors or the TSA or the P-value, or the confidence internal. Luckily, and that's what Dicky and Fuller have worked out for us, you take that coefficient estimate, delta hat. And Uh, let me think here. Don't use the standard error that Python spits out or the TSA and compare that to 1.96. You compare it to, oh, is that a question? Yeah, I don't know if it's fair enough.

SPEAKER 3
Uh, my question is This is the process for the testing for a unit. And we discover that in fact there is no youth group, could we. And so oh we could use this.

SPEAKER 1
Yeah, yeah, yeah, and that's what we will do. Yeah. But first we have to convince ourselves that we reject the union route. So we want to reject the hypothesis here. I'll tell you, it will have a distribution. The delta delta, it's just not normal, and that's the contribution of Dicky and Fuller to have worked it out. But one way to think about it is, look, you can derive it mathematically. It's very difficult, but computers do it for us. They they create simulated distributions and this is in fact what Python does for us depending on the exact model that you're running, it spits out a simulated critical value. Let me not get into how how it does the simulation, but. You can think of two ways of figuring out the correct critical value, an analytical way where you have to work out analytically what is the normal uh sorry, what is the approximate or yeah, approximate distribution of this estimator here, or you let the computer simulate it in the background for you. The analytical result and the simulated result are quite similar, OK? And so Dickie and Fuller have worked this out for us. What are the critical values? They depend on on the exact model that you're running. Since we're not sort of considering too many complicated models here, there's not that many critical values that we have to consider. So it really boils down, I think to. Yeah, right, to 2 models here. And that that now comes the deterministic trend and that's a bit annoying that that that starts featuring again. And let's uh Let's first go. With the model that, let me go two slides earlier, that we literally saw here, where there is no deterministic trend. You've transformed it, you estimated your little delta. And what you do with that little delta, it will be some negative number, and the question is, is it too close to zero or not? And the decision rule is the following. So you don't compare it to -1.96 or something like that. You compare it to the critical value that Dickey and Fuller provided to us by figuring out what the analytical or analytically what the approximate distribution of this guy is, and it turns out that the critical value that you need to compare. Your estimate too is -2.86. In the, in the AR model. Where you haven't included a deterministic trend. Which I would say is what we do most of the time, but just to be in line with what the textbook recommends, it's cheap enough to throw in a deterministic trend as well. But then the asymptotic distribution is slightly different, but we have a critical value in that case as well. Just a second. So if you have Uh, if you think of the AI model as one that also includes a deterministic trend, and in addition, you want to consider the possibility that there's a stochastic trend. You still estimate your little delta and you compare it to the critical value of -3.41. And the decision rule is as follows. If you are between 0 and negative, let's say 2.86 in the model without a deterministic trend, then you're too close to 0. That means You can't make that call that there's no unit route or you put it differently. There could be a unit route, and you have to pretend like there is one. Although we haven't, uh, yeah. Let's go, go with that one. But our goal is really to reject, I mean it's our goal to reject. The unit route. Um, so I'm talking too much, just a second. I'll answer your question and then we take it to compare to this critical. So yeah, so you run an auto regression and I will run it in a second of delta on YT minus 1. You get you get an estimate for the little delta. So for example, I get -5 and I go -5 is smaller than -2.86. Therefore, I conclude that my, I reject the null hypothesis of unit route. And yeah, so. Yeah, not the test statistic. Yeah, that's right. So, let me think about it in the break and I'll, I'll say it. I'll say it again. So we've got a 9 minute break.

SPEAKER 4
Just Yes We. All the presentation it's a lot. I would assume that it's not to say. Yes. OK Perhaps because I. to the everyone. Ma. I would like those. I mean it's just a. Then That's We. that's I don't think it's like what I said. That's not, but you should look at the gene from here so we. It 2.86 or people. or not we agreed to try to. Yeah, they, they were only coming back to the possibility. Re Um, so I assume there's gonna be another number. If you want Yeah, and so. In Python sation of how you, how you run the test in Python you would. And I'll, I'll show you And what it does, it

SPEAKER 1
will actually be I I really I think the sthetic

SPEAKER 4
one this allows for that. I I just actually. Yes, and but and you do find it also reports the Instead of you need to know. that is So Yeah, it's like, because if you just

SPEAKER 2
take like, you know, a standard Phillips curve model, like, you know, the reason why it works because we have like, you know, a clear reason as to why, right? Because if we've got a tight labour market, then, you know, we expect, like, you know, inflation to like follow with it. Whereas, you know, if you were just, you know, being completely agnostic to it, just throwing more but if you

SPEAKER 1
really agnostic, you have to throw in a lot. Large models.

SPEAKER 2
And, and the other thing is, well, like just taking this to look like a very basic symbo isn't, um, what's it called like unemployment also not.

SPEAKER 1
This is, I think what I said to you, but um, I, I would have thought the times that it's that it is stationary, but I think I'm saying in the US data. Did we have unemployment here? We did have unemployment because it. They continued it with. Yeah, I mean, this suggests that unemployment is very good, yeah, so it's small station.

SPEAKER 4
So I, I, I mostly think of stuff like GDP

SPEAKER 1
or CPI, something that's like really deterministic almost deterministically trending it's containing which, which goes to show that you have to test the time series and the AD. A test.

SPEAKER 2
Well, I mean Yeah for me like it's not the concern is that if the data like. because it just seems to be standard practise accept if you. we can sort of use unemployment to forecast inflation and even. Unemployment even if it's a non-station if it still gives us like you know if it still gives us a good forecast, do we really have to worry about spurious regression because if you just have a good structural explanation. Then this is not as good.

SPEAKER 1
Yeah, if you're really only after the best forecast. Statistical Just Statistical problems Um Yeah, I agree, but with so with with this model, you have in mind to compare unemployment to what you think is some sort of natural rate of unemployment. That's a function of time. but uh It's it's a fascinating sort of tension between. How much structure do you see and how agnostic can be and what's what's really your goals for like.

SPEAKER 2
prediction, because they're like, oh, you, you know, that's like, what is it? It's like. Adaptive expectations or whatever and that's all you know it's expectations so like time series is isn't that.

SPEAKER 1
Yeah, I can tell you what for example they use at central banks. I could imagine that they still must be. Different approaches, what approach dominates, I'm not sure.

SPEAKER 2
fully structured model. And so I, you know, you just created like, you know, a very good quebec model. In the data from this time period.

SPEAKER 1
And of course can be criticised as well because obviously having something out and to your liking and your abilities, why didn't you do this and that and it's uh. And. It's a fascinating problem, but also to the wrong person to do.

SPEAKER 4
She. there's more. Yeah T I So you understand. Hello Uh. I And It What I mean, so I just. I Yeah. All right.

SPEAKER 1
So I need to correct myself. I. I told you you estimate your delta, the little delta. And compare that. To those values that's not correct. The way the way this is implemented is you take the T statistic that Python spits out and compared to these numbers, so it's in a way a corrected critical value for tea that they provide which is convenient that's sort of the selling point is. You, you just run your oral regression of the change in the time series on its past. Python will give you a standard output. And you can grab the TSA and compare it to these critical values. So let's, let's work through this. Here's our CPI time series. Does it contain a unit route? It doesn't look obviously trending. But that's not proof. So here is, so going back 2 slides, 3 slides. Now, these days. So, here is Actually I'm using the 2nd 1. I'm running this auto regression. And that is the change of YT on a constant, on a deterministic time trend, and on YT minus 1. Um, I'm including a deterministic time trend because it's cheap to do so. I don't think it makes a big difference because this doesn't contain any. It's not a proof, but just look at this picture. OK. But again, it's easy, and I'll show you here how to include it as a regressor. As simple as that, um, I add, when I say I, it was um Alex who did this, a new variable called trent, which is just a number that runs from 1 to 320 or how many, however many observations we have got. And all you need to do is include that here as a regressor. What are we regressing? We're not regressing. Inul against What was called fill, the lag of infill, but fi, the change of inflation on lyfill, the lag of inflation, right? That's, that's what the suggestion is. That gives us an estimate of little delta, and that that estimate. is here, -2794. That is a small number, but is it small enough? Is it far enough? Sufficiently far away from zero or not. The decision rule, and so again, correcting myself from earlier is not to compare that number. To the critical values, but the way that um. Provided these critical values, they do allow you to go to the TTA, which is -7 here, that you compare.

SPEAKER 3
Yeah. So is this, is this proof that there exists or does not exist in today, or is this just at the 95% level of confidence that probably isn't.

SPEAKER 1
Yeah, so that it's a, it's a statistical test, so it's just with 95% confidence. So you conclude that there's probably not a unit route here. The now hypothesis is there is a unit route. Here we are sufficiently far away from zero because -7. Um, is smaller than -3.41. That's The critical value that we have to compare it to. If you're smaller than this number here. Then you're sufficiently far away from 0 to conclude. to reject the null hypothesis. You just have to remember what the null was. The null was there is a unit route. Now, this is also the reason why we used this data last week. So this is a well-behaved stationary time series. We just tested this. If I, so this is the inflation data. This means not that the change of inflation. Contains a unit, it means that inflation contains a unit. OK. You've run this test using the change of inflation against the lag of inflation, but the conclusion here is that we reject the null of a unit route and therefore conclude that inflation is a stationary time series. That is why we worked with inflation data last week. We didn't work with CPI data last week. I mean, I took it down a level. First Differentiated it. To implement the test, these numbers, these critical values are based on the homo scale lasticity. I wanted to say robust, but that's not the right word. On, on the standard errors based on homo scalasticity. So here on the, on the previous slide, you do not robustify for heteroskelasticity, OK? Then the, the TSA is, is not the right one to be compared. To the Tekifula critical values. Um, So the T statistic will always be negative, and the key question is, is it sufficiently negative? Is it sufficiently small to distinguish it from zero? Here we conclude yes. Here is the inbuilt way of running the same tests. You wouldn't actually run the auto regression that way. You would just use the A fuller method that um what's the package called uh stats model time series sub package allows you to use. And it just goes, um, A Fuller on. Info and you have to drop the missing observations in the beginning because of Uh, creating in the inflation time series of the CPI that was a point missing at the beginning. And here we tell it. Max lag is, we just want an AR one model and include a What is it? a linear time trend and then you could include these options here for other time trends. It will create this calculate the T statistic here, -7. And it will give you the critical value here. Which might not exactly be the same because it uses the simulated distribution, but it's it's practically the same, but you don't have to do the comparison to the critical value. You can also just look at the p value here which is tiny, so you come to the same conclusion, you reject the null hypothesis. That inflation contains a unit route. All right, now, a higher order test if you go, but I don't think my model is AR one. And again, we don't really touch on what is the right model, but I'm just demonstrating how you could generalise this to an ARP model. Um, so what you would do instead is you, you would run the regression of delta YT on, uh, I think, on, on this. ARP model where you have Um, I think what's going on here. The model in first differences or you subtract, uh, let me see. Y T minus 1. Still, so you have Delta YT on the left hand side and the little delta YT minus 1 there. Oh yeah right. So the test works again just on the little delta and supposedly if there is a unit route, these time series here are stationary because they are first difference already. So this test boils down to running this regression, and here's the time trend deterministic one of delta delta YT on YT minus 1 and the changed. The lag changes NYT, OK. And for an AR4 model in Python. You create these for. Lagged dimils. OK. So, so, going, so including 4 of these, that's what I'm doing here. And Uh, what else? So what, what are we including? We're regressing dfa on a deterministic trend, on the lag of inflation, and the four lags of the change in inflation. That's just sort of the generalisation of the Dicky Fuller test in for the AR4 model here. And you look at the TE that belongs to the auto regressive term, which is -4. Interestingly, you still compare that to the same critical value. Um Because the distribution of delta is actually the same. Under the unit, under the hypothesis of a unit route, the other regressors are well behaved and don't change the distribution. OK. So no adjustment, no other adjustment required. And the, the one we compare it to here, the critical value, the one that Tiki Fuller report for the model with the deterministic trend, which was 3.4. So here, We get a TA of -4.4, which is smaller than -3.4, so we still conclude low unit route for inflation, even when we're using an AR4. And then the Python way of doing this is to set the max lag parameter equal to 4, which gives you the same number. And that was the last slide. Any questions?

SPEAKER 4
Yeah. Just to ask, um, in the model that we looked

SPEAKER 3
at before, little Delta was just a coefficient of one of. In the AR4 model, it's just the efficient, is that that roam around? Could it be the, the coefficients a different regressor?

SPEAKER 1
Um, no, no, it, it's for the, for the auto regressive coefficient of order one. Let me think, how do I explain this? It's it's hard to say how this relates to this earlier observation where we want the, the polynomial equation to have roots. Where one route is equal to one or something like that, it translates to just testing the auto regressive coefficient on the. On the Unwey minus 1. Um, and in that model, these guys here are included, but they are under the null hypothesis of a unit route. Because they are first difference versions of the original time series, of the, in the past, are stationary. Um, long story short, it, it still only translates into. Testing this which which makes it a quite convenient and simple test. So We've discovered that Unit roots Non-stationary Add stochastic trends all the same thing. Cause problems for estimation, but we figured out how to deal, how to detect whether a time series contains a unit route. Um, that's why we worked last week with inflation, because it was a time series that didn't contain a unit route. We saw that here. OK. Um, and the general guidance is, and I will give you more next week, but if you, if you happen to detect a unit route, You take the first difference of that time series, and then the unit route likely goes away. But you can test that then too. This is what we did. Because I have tested the original CPI data contained to unit route, so I first differenced it and I obtained the inflation data that didn't contain. All right, um. That was the last slide. So next week, uh, I'll wrap that all up together, all up and give you some practical guidelines of how you would move forward. Um, if given a time series and how you would create forecasts, you do that also this week and next week in the computer labs. It will not take more than half an hour next week, but then I'll spend the rest of next week working through the practise final exam. Or any other questions that you have. Um, now I would just answer the workshop questions, uh, but before I do so, are there any other questions about. Lecture material. Good, then I'll just get started with the workshop. Will you give me a number. What's happening? Come on, give me a number. What I'm a bit clueless what's going on here. Great What's going on. Don't know. Don't know what to do. Might use the do can. There's nothing here. I can't see anything. All right, um. Should we use the dock cam? I'll, the dock is actually on here. Um, if I put the iPad under the dock cam, does that work? Have we tried this? Let's see how that looks. Ah, OK, that's not so good. It's glaring too much. Would, uh, maybe it will work. Let's try, if not. Can people get paper ready? It's possible. Um, let's see how we go. If I zoom, does that make things work? Can you see this? But tolerable? OK. Could be worse I feel my re I All right, so All we're combining here old fashioned technology and new technology that doesn't work so. What have we got here? We've got a stationary time series. I could say it's stationary. Because the auto regressive coefficient is 0.7. OK. Um, What is going on, we've got a zero mean for the error term and a variance of 9. And then it says compute the mean and the variance of. YT OK, let's see what happens when I write. Does it work? So. Part A. it looks pretty good. So the mean. Uh, compute the mean, the mean of. YT Well, what you would do is just plug in. 2.5 plus 0.7. YT minus 1. Oops. Plus UT, right? I just plugged in the right hand side here. And then I can break this up into. 3 terms. Obviously 2.5. Plus 0.7, the expected value of YT minus 1 plus. The expected value of UT, of which I know immediately that's 0, right? What's the next step? So I'll do copy and paste. Of this bit here. Uh, let's move one down. What do I change now to work towards the goal? Oh. I use stationarity now oops. Stationary And go, I can erase the -1 in the subscript. So I go Like this, this was very subtle. I'll just highlight. Here, I erased the -1 by stationarity, right? And then I mean The rest is relatively easy. You subtract 0.7 times the mean to the left, so on the left you have. 0.3 times the mean. And then if you divide by 0.3, you get 2.5 over 1 minus 0.7. Everyone see that? I think I did something similar last week. All right. That was easy. No? Good. All right, variances. The variants of YT? Oh yeah, thanks, sorry about that. Uh, you just have to remind me, I'm sorry. So the variance of YT is the variance. Off, so I can just go copy and paste the stuff here. Oops, I'm not a bit more precise. Right. That is equal to. Duplicate again And go, all right, the 2.5 doesn't contribute. To the variants That was easy to do. And then The variance of a sum is the sum of the variances. And then we talk about the covariances in a second. I should probably. Oh, right. Why is this there? What happened? is over yet. So That is equal to uh. The variance of 0.7 times. My, I'm going slow, my, my minus 1, plus. The variance Of UT. And then I should consider the covariances, shouldn't I? Between 1 and UT, but they are 0. So that's why the statement here is correct. Then Then I get here, that is 0.7 squared times the variance of YT, right? Because of stationarity. I don't need a YT minus 1 there. And this year was 9, I think, in the question. So then Moving 0.7 squared times the variance to the left, and then dividing by 1 minus 0.7 squared gives us 9/1 minus 0. 7 squa. You can see the general formula really is this. Sigma squared is a sigma U. Over 1 minus beta 1 squad. Any questions? Co Um Here the errors are IID. So they're not related to the wires. It's just noise that you're adding. Not systematically related to the Y. So it's not like saying, as inflation grows, your error grows. No, the error is always randomly added. And what's next? That was Part A, Part B, compute the first two order covariances. P So covariance of YT With YT 1, that's the the first order or covariance. That's what we want. So let's do it. So what I do is I plug in the right-hand side for YT. 2.5 Plus 0.7 YT minus 1 plus. You Y T minus 1. Should we write it like this? Like with the expected value. The covariant covariances, it's not really the right way. So we've got three terms here could individually consider their covariances with this term there, OK, so. Let's see if this is a good way. So we get the covariance of 2.5 with YT minus 1. And we're guessing that it should be zero because 2.5 doesn't really vary, so we cannot covary. And plus the covariance. Of 0.7 Y T minus 1. With YT -1. Plus the covariants. Of UT. With IT men as well. So we go, this one here is 0. 2.5 is not a random variable that varies. What's the covariance of, or what's the second term equal to? So the covariance of 0.7 YT minus 1 with YT minus 1.

SPEAKER 4
If you pull the 0.07 out and turn it into a barrier.

SPEAKER 1
Yeah. That's a good question. OK, let's, let's go, um, what do we do? My suggestion is this. Let's take the 0.7 out and worry about whether we should square it or not. In a second. I'll give you the right answer straight away. And You go, so what's left is this. OK. I'll, I'll discuss this in a second, a little bit more, but let's just go with this. What do we do with the third term now, the covariants of UT and YT minus 1. That's a quick 10. Everyone happy with the zero then by uh no independence, that's right. So let's just finish this and then revisit that question of why, why this is a 0.7 here, not a 0.7 squared. Unless you feel like it is obvious to me, let's move on. All right. So we go, well, what is, what is that equal to? 0.7. What's the covariance of a random variable with itself? Has another word, variance. OK. Um, so that's what you said. Um, so that's just this. I'm very slow here and then I go, oh wait, that's equal 2. Now, we could plug in the result from part A, but who cares, but we, we all know what's going on here. So that That is the covariance. OK. Let's see. Now comes an aside. To justify what we did on there to justify what we did, um, so let me just copy this from up here, so this covariance. What I need space. I'll start an equality in the second line. What's the covariance of 0.7 YT 1 with YT 1? By the definition of the covariance, it's gonna be an ugly one. It's the expected value. Big parenthesis of. The first random error, which is 0.7 times Y minus 1. Subtract of the meaning of that random verbal. And now I put that in brackets. So the generic formula for the covariances. First random variable minus its mean, times the 2nd random variable minus its mean in expected values. We just took care of the first random variable. Now the 2nd 1. Is YT minus 1 minus it's mean. And then like this That is just by definition of the covariance. Uh, all right. Now, this 0.7 can just be pulled in front of the largest expectation. So, uh, what do I do? Because I can, I just copy and paste. I like copying, copying and pasting. That's all on there and I go. Remove the 0.7 here. And put it in front of the outer expectation. I know it's a bit messy, but. We've learned in in week one that you can put a factor or a factor or whatever it's called in front of the expected value, right? What is left Here Is The covariants Of YT minus 1 with YT 1 right? So Altogether we get 0.7 times the covariance of YT 1 YT 1. So that just justifies this movement up earlier. So it's not squaring the 0.7 that we should do, it's just like that. Oh yeah, sorry about that. Of course I can. You just must remind me. I'm sorry. I can't see it here. I can't see it on the screen, but I'm not good at multitasking. All right, so Similar calculation now for the 2nd. Or auto correlation covariance. Mm Yeah, let's do it so. So the covariance. Of white tea with IT 2 is what we're after here now. So, what do we do? What do we do? Can I just recycle the formula from upstairs? Yeah. Let's use this one here. And it doesn't really save me a lot of time. And what I have to fix here is put a 2 here, right? No That's my eraser. And then I need to lag this guy here yet another period, so that I can compare it to YT minus 2. So I go. Do one more step. That is The covariants of I guess I can, I could drop to 2.5, but anyways, let's write it plus 0.7. Now I lag this another period. So that is 2.5 plus 0.7 YT minus 2 plus UT minus 1, right? Plus UT UT minus 2. Everyone OK with that? Now Can we be quick and go therefore we get. So we can go yada. That's a technical term. So if you are in the situation in an exam, you get full marks if you write that.

SPEAKER 3
is that, is that? Any questions?

SPEAKER 1
Yada yada yada. Yeah, that's why what I should do in the lecture also. Yada yada yada. But what, what follows? What, what is the result? So we sense that we've got this guy here and that guy there and we get. 0.7. I hope I'm not making a mistake now. I go Covariants of That may not be so so so. Variances Of YT. Similar to what we did earlier, right? Because everyone, so. I'm only doing the, this, this move because of time constraints. I, in an exam, I would want you to, to work through this. Obviously Is this correct? We could vote by majority vote correct. Now the next question asks us to turn this into an auto correlation I think. Yeah. But we, we have the variances already. Oh sorry, we don't need the variances. So, uh, so it's quite easy actually. So This was the first result here. How do I turn this into, sorry, this is an auto covariance. How do I turn it into an auto correlation? I divide by the variance. That was the trick for the auto correlation. Well, what do you get if you divide by the variance? 0.7 Right? So let me just go uh copy this. So this was part See That we, I don't know let's, let's call this, I don't know what we call the autocorrelation. Y Y T minus 1 is equal to the covariances, the auto covariance. Why did I copy this over the variance. We saw this formula last week, so that it's just 0.7. So big discovery here. What's the general result? I won The auto regressive coefficient is the auto correlation. The first order auto correlation. Now, then similarly, So this is the 2 here. Sorry, can you see this? This is a 2. Is equal to 0.7 squared. So what's the peace order? 0.7 to the peak. All right. Was there any anything else here? Any questions? Moving on, oh yeah, question then you have to also.

SPEAKER 4
Yeah, yeah why is it just As a Well, um,

SPEAKER 1
because the covariance is defined as an expected value, so it, it just. That's the quick answer. Um Let let's leave it at that. I'm happy to talk after sort of time pressed. Yeah. It's, we define it what is an expected value and the uh variance is explicitly a quadratic expected or the expected value of a quadratic expression. I think there was another question here, uh, right. OK, we can do this party. We can just do quickly. Uh, what do you want to do? So part D What should it be? Why T +1, given that we have data all the way to T. If it's defined Like, like it's defined here. It is 2.5. Plus 0.7 Y T. Plus, oh I can't write that. Move it over, uh, not the not the whole picture. Uh Ah, I'm moving the whole picture. Oh, I guess it works. plus Uh, let me write a UT here. So what I'm getting is 2.5 plus 0.7. YT I plug in now was 120.3, and the expected value of UT is zero, so that answers that question. Sorry for rushing this. It's as easy as going we'll plug 10 2.3. In Right. Since it's an AR one model, that's all you have to plug in. So that's a very simple forecasting exercise. So this year is a 0 point. 7, sorry about that. Should we move on to the 2nd? Exercise Everyone is nodding, very good. All right. Sorry for pushing it or rushing it. And this, this is a very easy one. So we're given Uh, an AR one model for the change in YT. And question A asks us to turn this into an AR model for the original time series. This is like saying. Moving back from inflation to CPI. And so we have. We have this time series here. I'm just copying. And now I go, uh, the question asked, turn this into a time series for YT. Um, so it's as easy as writing out the changes. So that is equivalent to, on the left-hand side, YT minus YT -1. On the right hand side, beta not plus beta 1. Y T minus 1. Y minus 2. Plus UT still. And then isolating YT on the left hand side. And I get a better not, better not. I get a Y minus 10 sorry. I should also have it like this. A Y T minus 1. How many YT1s have I got on the right hand side? Um, if I move this guy over to the right hand side, I get One plus beta one. By 1. And I get minus. Beta one IT. -2 plus the error. Um That's it, really. That's the AR. That that's an AR2 model. And then Part B is almost redundant. I mean, you can give these guys new names. Gamma knot plus gamma 1 YT minus 1 minus gamma 2. Those are just new names, right? But what you could do now is you run a regression of YT on YT minus 1 and YT 2. You get your and if it's a stationary time series, you get an estimate for, for the gammas, and you can relate them back to the betas quite easily. That's all. And that this is sort of what we did last week. Um The times that I just wrote down is for CPI that was actually non-stationary. And we took first differences, and that was inflation. And we can So what we, what we usually do is not to estimate the model on YT because we suspect it is non-stationary. We take those differences, estimate that model, we obtain the betas. And then we can create forecasts either for delta YT or for YT, uh, what, whatever. We're interested in pursuing, OK? In, in any case, it was a relatively easy mechanical exercise. Are there any questions? Cool. So that was lecture 11, and then we'll meet each other again for the last time next week. Until then, have a good week and I'll see you then.

SPEAKER 4
I Come I So why do you just get the results. that that was the definition. Um, can I see the question, does it? from here The definition of Y2 + 1 give T

SPEAKER 1
is is is this conditional expectation. So all I did is plug in. YT +1. If this is the equation is equal to 2.5 plus 0.7 times y. Subca T sorry, this should be. OK. Right? So that it's the same. Y T +1, according to this equation here. It's equal to this this thing. And then 2.5 I can take out and here I get. Yeah, see you. I, um. Uh, a conditional expectation. I don't have space here to write, um. right Does it work the same? So it's actually given. Give them wide capital. Then can I just erase this? So you get 2.5 plus 0.7 of YT given your YT pedantic. I think you don't mind this, but this is the it's clear. So here. Then we offer this observation so you can just plug them in. Does it make sense?

SPEAKER 4
This Because I think they still But the the error

SPEAKER 1
term. So this is defined to be the expected value for the error term will play a role after the expected value. Yeah, so, yeah, that's, that's, that's what's written here. This thing here Its defined to be this expected value.
