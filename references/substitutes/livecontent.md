on

CHAPTER 4

The Buddy System

“Maybe you’re the problem, do you think?”

When Lauren Conrad, star of MTV’s The Hills, appeared on the Late Show with David Letterman in October 2008, her interview took an unexpected turn. The first minute was standard talk-show banter for a twenty-two-year-old star of a successful reality series: the amount of drama in her life. Less than a minute later, Conrad asked Letterman if he was calling her an idiot.

She started off the interview by discussing her ongoing feud with her by-then-former roommate Heidi Montag and Heidi’s boyfriend Spencer Pratt. In case you’re not familiar, here is the backstory: Lauren and Heidi’s friendship ended when they came to blows at a birthday party after Lauren accused Heidi and Spencer of starting rumors that she had made a sex tape. In addition, Lauren developed friendships with Stephanie (Spencer’s sister) and Holly (Heidi’s sister), complicating the social and family encounters of everyone involved. Lauren tried without success to strengthen the relationship between her roommates, Audrina and Lo. This strained Lauren’s friendship with Audrina, who reestablished her friendship with Heidi. Brody Jenner was also in the mix, dating Lauren, questioning her date with a Teen Vogue model, dating someone else himself, arguing with Spencer about his friendship with Lauren, getting accused of starting the rumors about Lauren’s sex tape, etc.

All of that drama happening in Conrad’s life is what David Letterman was referring to when he interjected, “That raises the question, maybe you’re the problem, do you think?” That quip sent an interview that was supposed to be a puffy, promotional chat into an uncomfortable tailspin.

Letterman immediately realized that he had taken the conversation into much deeper, more serious territory than either of them could have anticipated. He tried to soften the blow in a self-deprecating way, adding that he had done the same thing, for years refusing to close the learning loop by assuming everybody around him was an idiot.

“Let me give you an example from my own life. . . . For a long time . . . I thought, ‘Geez, people are idiots.’ Then it occurred to me, ‘Is it possible that everybody’s an idiot? Maybe I’m the idiot,’ and it turns out I am.”

Conrad clearly didn’t want to hear it, replying, “Does that make me an idiot, then?” Websites devoted to reality TV, gossip, media, and popular culture immortalized the moment, and that’s how they saw it: Letterman “basically calls Conrad an idiot” (Gawker.com), “ripped into” Conrad (Trendhunter.com), “makes fun of Lauren Conrad” (Starpulse.com).

Letterman’s comment was actually quite perceptive. His mistake was offering up the insight in an inappropriate forum to someone who hadn’t agreed to that kind of truthseeking exchange.

Conrad certainly had a lot of drama in her life: enough that MTV created two successive shows to document it. But, like most people do, she characterized the drama as a series of things happening to her. In other words, the drama was outside her control (luck). Letterman suggested some of it could be fielded into the skill bucket, a suggestion that might have been helpful to Conrad in the future if she had been receptive to it. Not surprisingly, she wasn’t.

Letterman had offered the helpful alternative hypothesis, unexpectedly, on a late-night talk show where the norm is fluff and PR. Perhaps Letterman’s approach would have been more appropriate in an Oprah Winfrey–style prime-time interview. Or on one of those reality therapy shows where reality stars agree to such an exchange. As it was, he violated the assumed social contract by challenging Conrad to bet on her outcome fielding when she hadn’t agreed to truthseek with him.

That exchange was similar to my interaction at the poker tournament with the six-seven of diamonds guy. I thought he was asking for my advice, so I responded by asking for more information to get an idea of whether he accurately fielded his losing outcome as luck. He was expecting me to adhere to the norm of being a sympathetic ear for a hard-luck story. When I attempted to delve into the details, I violated this implied contract. I Lettermanned him.

Such interactions are reminders that not all situations are appropriate for truthseeking, nor are all people interested in the pursuit. That being said, any of us who wants to get better at thinking in bets would benefit from having more David Lettermans in our lives. As the “original” Letterman learned from the awkward exchange with Lauren Conrad, Lettermanning needs agreement by both parties to be effective.

The red pill or the blue pill?

In the classic science-fiction film The Matrix, when Neo (played by Keanu Reeves) meets Morpheus (the hero-hacker played by Laurence Fishburne), Neo asks Morpheus to tell him what “the matrix” is. Morpheus offers to show Neo, giving him the choice between taking a blue pill and a red pill.

“You take the blue pill, the story ends. You wake up in your bed and believe whatever you want to believe. You take the red pill, you stay in Wonderland and I show you how deep the rabbit hole goes.”

As Neo reaches toward a pill, Morpheus reminds him, “Remember, all I am offering is the truth. Nothing more.”

Neo chooses to see the world as it really is. He takes the red pill and is pounded with a series of devastating truths. His comfortable world is a dream created by machines to enslave him as an energy source. His job and lifestyle, his clothes, his appearance, and the entire fabric of his life are an illusion implanted in his brain. In the actual world, taking the red pill causes his body to be unplugged from his feeding pod, flushed into a sewer, and picked up by Morpheus’s pirate ship, the Nebuchadnezzar. As rebels against the machines, Morpheus and his crew (and now Neo, due to his choice) live in cramped quarters, sleep in uncomfortable cells, eat gruel, and wear rags. Machines are out to destroy them.

The trade-off is that Neo sees the world as it actually is and, in the end, gets to defeat the machines that have enslaved humanity.

In the movie, the matrix was built to be a more comfortable version of the world. Our brains, likewise, have evolved to make our version of the world more comfortable: our beliefs are nearly always correct; favorable outcomes are the result of our skill; there are plausible reasons why unfavorable outcomes are beyond our control; and we compare favorably with our peers. We deny or at least dilute the most painful parts of the message.

Giving that up is not the easiest choice. Living in the matrix is comfortable. So is the natural way we process information to protect our self-image in the moment. By choosing to exit the matrix, we are asserting that striving for a more objective representation of the world, even if it is uncomfortable at times, will make us happier and more successful in the long run.

But it’s a trade-off that isn’t for everyone; it must be freely chosen to be productive and sustainable. Morpheus (unlike Letterman) didn’t just go around ripping people out of the matrix against their will. He asked Neo to make the choice and exit the matrix with him.

If you have gotten this far in this book, I’m guessing that you are choosing the red pill over the blue pill.

When I started playing poker, I chose truthseeking. Like Neo, I did it reluctantly and wasn’t sure what I was getting into. My brother took a blunt approach with me. My instinct was to complain about my bad luck and to marvel at how poorly others played, decrying the injustice of any hand I might have lost. He wanted to talk about where I had questions about my strategic decisions, where I felt I might have made mistakes, and where I was confused on what to do in a hand. I recognized he was passing along the approach he learned with his friends, a group of smart, analytical East Coast players, many of whom, like Erik Seidel,* were on their way to establishing themselves as legends at the game. In addition to introducing me to this approach, he also encouraged these phenomenal professionals to treat me as a peer when discussing poker.

I was lucky to have access at such an early stage in my career to this group of world-class players who became my learning pod in poker. And I was also lucky that if I wanted to engage that group about poker, I had to ask about my strategic decisions. I had to resist my urge to moan about my bad luck and focus instead on where I felt I might have made mistakes and where I was confused on what to do in a hand. Because I agreed to the group’s rules of engagement, I had to learn to focus on the things I could control (my own decisions), let go of the things I couldn’t (luck), and work to be able to accurately tell the difference between the two.

I learned from this experience that thinking in bets was easier if I had other people to help me. (Even Neo needed help to defeat the machines.) Remember the buddy system from school field trips or camp? Teachers or counselors would pair everybody up with a buddy. Our buddy was supposed to keep us from wandering off or getting into water too deep, and we did the same for our buddy. A good decision group is a grown-up version of the buddy system. To be sure, even with help, none of us will ever be able to perfectly overcome our natural biases in the way we process information; I certainly never have. But if we can find a few people to choose to form a truthseeking pod with us and help us do the hard work connected with it, it will move the needle—just a little bit, but with improvements that accumulate and compound over time. We will be more successful in fighting bias, seeing the world more objectively, and, as a result, we will make better decisions. Doing it on our own is just harder.

Members of our decision pod could be our friends, or members of our family, or an informal pod of coworkers, or an enterprise strategy group, or a professional organization where members can talk about their decision-making. Forming or joining a group where the focus is on thinking in bets means modifying the usual social contract. It means agreeing to be open-minded to those who disagree with us, giving credit where it’s due, and taking responsibility where it’s appropriate, even (and especially) when it makes us uncomfortable. That’s why, when we do it with others, we need to make it clear the social contract is being modified, or feelings will get hurt, defensiveness will rear its ugly head, and, just like Lauren Conrad, your audience won’t want to hear what you have to say. So, while we find some people to think in bets with us, with the rest of the world, it is generally better to observe the prevailing social contract and not go around saying, “Wanna bet?” willy-nilly. (That doesn’t mean we can’t ever engage in truthseeking outside of our group. Our approach just needs to be less head-on, less Letterman-like. More on that later, after we explore communications within the group.)

Out in the world, groups form all over the place because people recognize how others can help us; the concept of working together on our individual challenges is a familiar one. Having the help of others provides many decision-making benefits, but one of the most obvious is that other people can spot our errors better than we can. We can help others in our pod overcome their blind-spot bias and they can help us overcome the same.

Whatever the obstacles to recruiting people into a decision group (and this chapter points out several, along with strategies for overcoming them), it is worth it to get a buddy to watch your back—or your blind spot. The fortunate thing is that we need to find only a handful of people willing to do the exploratory thinking necessary for truthseeking. In fact, as long as there are three people in the group (two to disagree and one to referee*), the truthseeking group can be stable and productive.

It’s also helpful to recognize that people serve different purposes in our lives. Even if we place a high value on truthseeking, that doesn’t mean everyone in our lives has to adopt that or communicate with us in that way. Truthseeking isn’t a cult; we don’t have to cut off people who don’t share that commitment. Our Pilates friends or our football friends or any of our friends shouldn’t have to take the red pill to remain our friends. Different friends fill different needs and not all of them need to be cut from the same cloth. Those different groups can also provide much-needed balance in our lives. After all, it takes effort to acknowledge and explore our mistakes without feeling bad about ourselves, to forgo credit for a great result, and to realize, with an open mind, that not all our beliefs are true. Truthseeking flies in the face of a lot of comfortable behaviors; it’s hard work and we need breaks to replenish our willpower.

In fact, in my poker strategy group, we understood the need to occasionally opt out and off-load intense emotions before engaging in the work of accurately fielding an outcome. If, for example, one of us just got eliminated from a tournament, it was acceptable, every once in a while, to say, “For right now, I just need to moan about my bad luck.” The key was that when we did that, we recognized it was a temporary exception from the hard work we were doing together and to which we would return when the emotional rawness of the moment passed.

We know our decision-making can improve if we find other people to join us in truthseeking. And we know we need an agreement. What’s in that agreement? What are the features of a productive decision-making pod? The remainder of this chapter is devoted to offering answers to those questions. Chapter 5 builds on that by providing a blueprint for rules of engagement within truthseeking groups, how to keep the group from drifting off course, and the productive habits of mind the group can reinforce in each of us.

Not all groups are created equal

A well-chartered group can be particularly useful for habits that are difficult to break or change. This is not a crazy or even novel idea. We are all familiar with how the group approach can help with reshaping habits involving eating, consuming alcohol, and physical activity. The most well-known example of a productive group approach is Alcoholics Anonymous (AA).

The first of AA’s founders, Bill W., initially refrained from drinking through a difficult process that included years of failure, hopelessness, hospitalization, drugs, and a transformative religious experience. To maintain sobriety, however, he realized that he needed to talk to another alcoholic. Bill W. recruited Dr. Bob, the second founder of AA, on a trip to Akron, Ohio. Dr. Bob, considered by family and doctors to be a hopeless and incurable alcoholic, kept Bill W. from drinking on the trip. In turn, Bill W. eventually helped Dr. Bob give up drinking. AA has subsequently helped millions of people get and stay sober, and led to organizations trying the same approach with other difficult-to-tackle habits like narcotics abuse, smoking, unhealthy eating, and abusive relationships. That all sprang from the concept that we can do better with the help of others.

But, while a group can function to be better than the sum of the individuals, it doesn’t automatically turn out that way. Being in a group can improve our decision quality by exploring alternatives and recognizing where our thinking might be biased, but a group can also exacerbate our tendency to confirm what we already believe. Philip Tetlock and Jennifer Lerner, leaders in the science of group interaction, described the two kinds of group reasoning styles in an influential 2002 paper: “Whereas confirmatory thought involves a one-sided attempt to rationalize a particular point of view, exploratory thought involves even-handed consideration of alternative points of view.” In other words, confirmatory thought amplifies bias, promoting and encouraging motivated reasoning because its main purpose is justification. Confirmatory thought promotes a love and celebration of one’s own beliefs, distorting how the group processes information and works through decisions, the result of which can be groupthink. Exploratory thought, on the other hand, encourages an open-minded and objective consideration of alternative hypotheses and a tolerance of dissent to combat bias. Exploratory thought helps the members of a group reason toward a more accurate representation of the world.

Without an explicit charter for exploratory thought and accountability to that charter, our tendency when we interact with others follows our individual tendency, which is toward confirmation. The expression “echo chamber” instantly conjures up the image of what results from our natural drift toward confirmatory thought. That was the chorus I heard among some groups of players during breaks of poker tournaments. When one player brought up how unlucky they had gotten, another would nod in assent as a prelude to telling their own hard-luck story, which, in turn, would be nodded at and assented to by the group.

Lerner and Tetlock offer insight into what should be included in the group agreement to avoid confirmatory thought and promote exploratory thought. “Complex and open-minded thought is most likely to be activated when decision makers learn prior to forming any opinions that they will be accountable to an audience (a) whose views are unknown, (b) who is interested in accuracy, (c) who is reasonably well-informed, and (d) who has a legitimate reason for inquiring into the reasons behind participants’ judgments/choices.” Their 2002 paper was one of several they coauthored supporting the conclusion that groups can improve the thinking of individual decision-makers when the individuals are accountable to a group whose interest is in accuracy.

In addition to accountability and an interest in accuracy, the charter should also encourage and celebrate a diversity of perspectives to challenge biased thinking by individual members. Jonathan Haidt, a professor at New York University’s Stern School of Business, is a leading expert in exploring group thought in politics. Haidt, in his book The Righteous Mind: Why Good People Are Divided by Politics and Religion, built on Tetlock’s work, connecting it with the need for diversity. “If you put individuals together in the right way, such that some individuals can use their reasoning powers to disconfirm the claims of others, and all individuals feel some common bond or shared fate that allows them to interact civilly, you can create a group that ends up producing good reasoning as an emergent property of the social system. This is why it’s so important to have intellectual and ideological diversity within any group or institution whose goal is to find truth.”

In combination, the advice of these experts in group interaction adds up to a pretty good blueprint for a truthseeking charter:

A focus on accuracy (over confirmation), which includes rewarding truthseeking, objectivity, and open-mindedness within the group;

Accountability, for which members have advance notice; and

Openness to a diversity of ideas.

An agreement along these lines creates a common bond and shared fate among members, allowing the group to produce sound reasoning.

None of this should be surprising to anyone who recognizes the benefits of thinking in bets. We don’t win bets by being in love with our own ideas. We win bets by relentlessly striving to calibrate our beliefs and predictions about the future to more accurately represent the world. In the long run, the more objective person will win against the more biased person. In that way, betting is a form of accountability to accuracy. Calibration requires an open-minded consideration of diverse points of view and alternative hypotheses. Wrapping all that into your group’s charter makes a lot of sense.

The charter of the group must be communicated unambiguously, as Erik Seidel made clear to me. I had met Erik when I was a teenager, but when I started running into him at poker tournaments, it was the first time we were interacting in a business setting. Early on in my career, I saw Erik during a break in a tournament, and started moaning to him about my bad luck in losing a big hand. In three sentences, he laid out all the elements of a productive group charter. “I don’t want to hear it. I’m not trying to hurt your feelings, but if you have a question about a hand, you can ask me about strategy all day long. I just don’t think there’s much purpose in a poker story if the point is about something you had no control over, like bad luck.”

When you think about a charter for truthseeking interactions, Erik Seidel pretty much nailed it. He told me the rules of being in a pod with him. He discouraged me from confirmatory or biased thought like “I got unlucky.” He encouraged me to find things I might have control over and how to improve decisions about those. I knew he would hold me accountable to these things in future interactions. We would explore diverse ideas because he insisted that be the focus of our interactions.

Because I was lucky enough to be part of a group with a truthseeking charter, there was no question that my poker decision-making improved. When I could consult them on in-progress decisions, like whether to move up in stakes or bankroll management or game selection, their advice reduced the number of errors I was making. Likewise, access to their range of strategies and experiences improved the quality of my thinking and decisions on a continuing basis. When I had questions or didn’t understand why something happened, they would see things I didn’t. When they had questions or needed advice on a hand, I wasn’t just helping them work through a decision they made but would often get insights into my own game. Those interactions led to improvements in my game I would have overlooked or, at best, figured out on my own only after making a lot of costly errors.

Even better, interacting with similarly motivated people improves the ability to combat bias not just during direct interactions but when we are making and analyzing decisions on our own. The group gets into our head—in a good way—reshaping our decision habits.

The group rewards focus on accuracy

We all want to be thought well of, especially by people we respect. Lerner and Tetlock recognized that our craving for approval is incredibly strong and incentivizing. In most laboratory situations, they noted, study participants expected to explain their actions to someone they’d never met and never expected to meet again. “What is remarkable about this literature is that—despite the prevalence of these minimalist manipulations—participants still reliably respond as if audience approval matters.” It’s great to get approval from people we respect, but we crave approval so badly, we’ll still work to get it from a stranger. A productive decision group can harness this desire by rewarding accuracy and intellectual honesty with social approval.

Motivated reasoning and self-serving bias are two habits of mind that are deeply rooted in how our brains work. We have a huge investment in confirmatory thought, and we fall into these biases all the time without even knowing it. Confirmatory thought is hard to spot, hard to change, and, if we do try changing it, hard to self-reinforce. It is one thing to commit to rewarding ourselves for thinking in bets, but it is a lot easier if we get others to do the work of rewarding us.

Groups like AA demonstrate how a supportive group can provide the reward for doing the hard work of changing a habit routine, just by its approval. For engaging in the difficult work involved in sobriety, local AA groups give tokens or chips celebrating the length of individual members’ sobriety. The tokens (which members often carry or customize as jewelry) are a tangible reminder that others acknowledge you are accomplishing something difficult. There are chips for marking one to sixty-five years of sobriety. There are also chips given for every month of sobriety in the first year. There is even a chip given for being sober for twenty-four hours.

I experienced firsthand the power of a group’s approval to reshape individual thinking habits. I got my fix by trying to be the best credit-giver, the best mistake-admitter, and the best finder-of-mistakes-in-good-outcomes. The reward was their enthusiastic engagement and deep dives introducing me to the nuances of poker strategy. It was also rewarding to have these intelligent, successful players take my questions seriously and increasingly ask for my opinions. In contrast, I felt disapproval from them when I acted against the charter and complained about my bad luck, or expected them to confirm how great I played simply because I was winning.

While I never got close to attaining the goal of a pure focus on accuracy, my group helped me to give a little more credit than I otherwise would have, to spot a few more mistakes than I would have spotted on my own, to be more open-minded to strategic choices that I disagreed with. That moved me, even if just a little bit at a time, toward my goal of getting closer to the objective truth. And that little bit had a huge long-run impact on my success.

When I started playing poker, “discussing hands” consisted mostly of my complaining about bad luck when I lost. My brother quickly got sick of my moaning. He laid down the law and said I was only allowed to ask him about hands that I had won. If I wanted him to engage with me, I had to identify some point in those hands where I might have made a mistake.

Talking about winning (even if we are identifying mistakes along the way to a win) is less painful than talking about losing, allowing new habits to be more easily trained. Identifying mistakes in hands I won reinforced the separation between outcomes and decision quality. These discussions also made me feel good about analyzing and questioning my decisions because of the approval I got from Howard and the players I looked up to. I used that approval as evidence that I understood the game and had promise as a player. When they complimented me for finding alternative approaches in my winning hands or understanding the contribution of luck, that felt terrific. In time, I could expand this approach to identifying learning opportunities in any hand I played, not just the winning ones.

Once we are in a group that regularly reinforces exploratory thought, the routine becomes reflexive, running on its own. Exploratory thought becomes a new habit of mind, the new routine, and one that is self-reinforced. In a Pavlovian way, after enough approval from the group for doing the hard work of thinking in bets, we get the same good feeling from focusing on accuracy on our own. We internalize the group’s approval, and, as a matter of habit, we begin to do the kind of things that would earn it when we are away from the group (which is, after all, most of the time).

“One Hundred White Castles . . . and a large chocolate shake”: how accountability improves decision-making

David Grey is a high-stakes poker player and professional gambler, and a good friend. After a night at a racetrack and a bowling alley in New Jersey, David and a bunch of other bettors were hungry. It was late. Someone suggested White Castle. A discussion broke out about how many burgers the biggest eater in the group, Ira the Whale, could eat.

When they got Ira the Whale to say he could eat 100 burgers (remember, White Castle burgers are small), most of the group, not surprisingly, wanted to bet against him. David was an exception. “I was a young guy, just getting started. Fifty dollars was a big win or loss for me. There was about $2,000 out against Ira the Whale. I bet $200 on him because I thought he could do it.”

When they got to White Castle, Ira the Whale decided to order the burgers twenty at a time. David knew he was a lock to win as soon as Ira the Whale ordered the first twenty, because Ira the Whale also ordered a milkshake and fries.

After finishing the 100 burgers and after he and David collected their bets, Ira the Whale ordered another twenty burgers to go, “for Mrs. Whale.”

Accountability is a willingness or obligation to answer for our actions or beliefs to others. A bet is a form of accountability. If we’re in love with our own opinions, it can cost us in a bet. Ira the Whale held the other gamblers accountable for their beliefs about whether he could eat 100 White Castle burgers. Accountability is why John Hennigan (briefly) moved to Des Moines. After spending time in that kind of environment, you become hypervigilant about your level of confidence in your beliefs. No one is forced to make or take such bets, but the prospect is a reminder that you can always be held accountable for the accuracy of what you believe and say. It is truly putting your money where your mouth is.

Being in an environment where the challenge of a bet is always looming works to reduce motivated reasoning. Such an environment changes the frame through which we view disconfirming information, reinforcing the frame change that our truthseeking group rewards. Evidence that might contradict a belief we hold is no longer viewed through as hurtful a frame. Rather, it is viewed as helpful because it can improve our chances of making a better bet. And winning a bet triggers a reinforcing positive update.

Accountability, like reinforcement of accuracy, also improves our decision-making and information processing when we are away from the group because we know in advance that we will have to answer to the group for our decisions. Early in my poker career, my poker group recommended that a way to avoid the effects of self-serving bias when I was losing was to have a preset “loss limit”—if I lost $600 at the stakes I was playing, I would leave the game. The smart, experienced players advising me knew that in the moment of losing, I might not be my most rational self in assessing whether I was losing because I was getting unlucky or losing because I was playing poorly. A predetermined loss limit acts as a check against irrationally chasing losses, but self-enforcement is a problem. If you have more money in your pocket, you might still take it out. If you’re out of money, casinos have ATMs and machines that let you get cash advances on your credit cards. Poker players are also pretty liberal about lending money to losing players.

I was much less likely to break a loss limit because I knew I was accountable to my pod. If I reached my loss limit and my inner voice said, “This game is so good that I should put up more money and keep playing,” it also reminded me I’d have to answer for the decision to a group of players I respected. Accountability made me run that conversation in my head, in which I started explaining how I was just getting unlucky and they would expose why I was likely biased in my assessment, helping me resist the urge to buy more chips. And, after leaving a losing game and going home, I could offset some of the sting of losing by running the conversation where my pod would approve of my decision to quit the game when I told them about it.

Imagining how the discussion will go helps us to spot more errors on our own and catch them more quickly.

The group ideally exposes us to a diversity of viewpoints

John Stuart Mill is one of the heroes of thinking in bets. More than one hundred and fifty years after writing On Liberty, his thinking on social and political philosophy remains startlingly current. One of the frequent themes in On Liberty is the importance of diversity of opinion. Diversity and dissent are not only checks on fallibility, but the only means of testing the ultimate truth of an opinion: “The only way in which a human being can make some approach to knowing the whole of a subject, is by hearing what can be said about it by persons of every variety of opinion, and studying all modes in which it can be looked at by every character of mind. No wise man ever acquired his wisdom in any mode but this; nor is it in the nature of human intellect to become wise in any other manner.”

There is a simple beauty in Mill’s insight. On our own, we have just one viewpoint. That’s our limitation as humans. But if we take a bunch of people with that limitation and put them together in a group, we get exposed to diverse opinions, can test alternative hypotheses, and move toward accuracy. It is almost impossible for us, on our own, to get the diversity of viewpoints provided by the combined manpower of a well-formed decision pod. To get a more objective view of the world, we need an environment that exposes us to alternate hypotheses and different perspectives. That doesn’t apply only to the world around us: to view ourselves in a more realistic way, we need other people to fill in our blind spots.

A group with diverse viewpoints can help us by sharing the work suggested in the previous two chapters to combat motivated reasoning about beliefs and biased outcome fielding. When we think in bets, we run through a series of questions to examine the accuracy of our beliefs. For example:

Why might my belief not be true?

What other evidence might be out there bearing on my belief?

Are there similar areas I can look toward to gauge whether similar beliefs to mine are true?

What sources of information could I have missed or minimized on the way to reaching my belief?

What are the reasons someone else could have a different belief, what’s their support, and why might they be right instead of me?

What other perspectives are there as to why things turned out the way they did?

Just by asking ourselves these questions, we are taking a big step toward calibration. But there is only so much we can do to answer these questions on our own. We only get exposed to the information we have been exposed to, only live the experiences we have experienced, only think of the hypotheses that we can conceive of. It’s hard to know what reasons someone else could have for believing something different. We aren’t them. We haven’t had their experiences. We don’t know what different information they have. But they do.

Much of our biased information processing stems from the amount of rope that uncertainty affords us. Well-deployed diversity of viewpoints in a group can reduce uncertainty due to incomplete information by filling in the gaps in what we know, making life start to fit more neatly on a chessboard.

Others aren’t wrapped up in preserving our narrative, anchored by our biases. It is a lot easier to have someone else offer their perspective than for you to imagine you’re another person and think about what their perspective might be. A diverse group can do some of the heavy lifting of de-biasing for us. A poker table is a naturally diverse setting because we generally don’t select who we play with for their opinions. Even better, when there is disagreement stemming from the diverse opinions represented at a poker table, the discussion may naturally progress toward betting on it. These are ideal circumstances for promoting accuracy.

Numerous groups have recognized the need to engineer the kind of diversity and encouragement of dissent that naturally occurs at a poker table. The State Department, since the Vietnam War, has had a formal Dissent Channel, where employees can have their dissenting views heard and addressed without fear of penalty. The American Foreign Service Association, the professional organization of foreign-service employees, has four separate awards it gives annually to members “to recognize and encourage constructive dissent and risk-taking in the Foreign Service.” The Dissent Channel has been credited with a policy change that helped end the genocidal war in Bosnia. In June 2016, fifty-one State Department employees signed a memo calling for President Obama to strengthen American military efforts in Syria. In late January 2017, approximately one thousand employees signed a dissent cable in response to President Trump’s executive order suspending immigration from seven Muslim-majority countries. The Dissent Channel represents something hopeful in our nation’s decision-making process. In an environment of increased polarization, foreign-service employees can make their voices heard about policies with which they disagree, and do it regardless of whether the administration is Democrat or Republican. Allowing dissent has a value that transcends party politics.

After September 11, the CIA created “red teams” that, according to Georgetown law professor Neal Katyal in a New York Times op-ed, “are dedicated to arguing against the intelligence community’s conventional wisdom and spotting flaws in logic and analysis.” Senior Obama administration officials, following the raid that killed Osama bin Laden, mentioned red-team analysis among the methods used to measure the degree of confidence that bin Laden, in the absence of visual or auditory confirmation, was in the compound subject to the raid.

Dissent channels and red teams are a beautiful implementation of Mill’s bedrock principle that we can’t know the truth of a matter without hearing the other side. This commitment to diversity of opinion is something that we would be wise to apply to our own decision groups. For example, if a corporate strategy group is figuring out how to integrate operations following a merger, someone who initially opposed the merger would be good to have as part of the group. Perhaps they have reasons why the two sales departments won’t mesh—whatever their reasons, they could help the majority move forward with a wiser approach by taking those reasons into account.

Diversity is the foundation of productive group decision-making, but we can’t underestimate how hard it is to maintain. We all tend to gravitate toward people who are near clones of us. After all, it feels good to hear our ideas echoed back to us. If there is any doubt about how easy it can be to fall into this confirmatory drift, we can even see this tendency in groups we consider some of the most dedicated to truthseeking: judges and scientists.

Federal judges: drift happens

Cass Sunstein, now a Harvard law professor, conducted a massive study with colleagues when he was on the faculty at the University of Chicago Law School, on ideological diversity in federal judicial panels. Sunstein recognized at the outset that the U.S. Courts of Appeals are “an extraordinary and longstanding natural experiment” in diversity. Appellate court panels are composed of three judges randomly drawn from that circuit’s pool. Each circuit’s pool includes life-tenured judges chosen (when an opening occurs or Congress recognizes the need for additional judges) by the sitting president. In any particular appeal, you could get a panel of three Democrat appointees, three Republican appointees, or a two-to-one mix in either direction.

The study, encompassing over 6,000 federal appeals and nearly 20,000 individual votes, found, not surprisingly, that judicial voting generally followed political lines. Pure, unaided open-mindedness, even by life-tenured judges sworn to uphold the law, is hard.

When there was political diversity on the panels, the researchers found several areas where that diversity improved the panel’s work. Even though, in most cases, two politically similar judges could dictate the panel’s outcome, there were significant differences between heterogeneous and homogeneous panels. A single panelist from the other party had “a large disciplining effect.”

They found, for example, “strong evidence of ideological dampening” in environmental cases. Democrat appointees, who overall voted for plaintiffs 43% of the time, voted for plaintiffs just 10% of the time when sitting with two Republican appointees. Republican appointees, who overall voted for plaintiffs 20% of the time, voted for plaintiffs 42% of the time when seated with two Democrat appointees. This held up across most of the twenty-five categories of cases in which they had a sufficiently large sample to reach a conclusion.

The authors concluded that the result endorsed the importance of exposure to diverse viewpoints: “What is necessary is reasonable diversity, or diversity of reasonable views . . . and that it is important to ensure that judges, no less than anyone else, are exposed to it, and not merely through the arguments of advocates.”

Sunstein’s group found that federal appellate judges need the diverse viewpoint of an opposing-party appointee. Judges, they found, followed the human instinct of succumbing to groupthink. “Our data provide strong evidence that like-minded judges also go to extremes: the probability that a judge will vote in one or another direction is greatly increased by the presence of judges appointed by the president of the same political party. In short, we claim to show both strong conformity effects and group polarization within federal courts of appeals.”

The growing polarization of the Supreme Court is a case in point. Each justice now has four clerks, all of whom have similar credentials: top-of-the-class graduates of top law schools, law review editors, and clerkships with federal appeals court judges. The clerks, over the years, have played an increasingly important role in helping the justices with their intellectual workload, discussing details of cases and drafting initial versions of opinions.

Prior to the appointment of Chief Justice Roberts in 2005, it was an informal badge of honor, especially among some of the conservative members of the court, that they hired clerks with ideological backgrounds that differed from theirs. Bob Woodward and Scott Armstrong, in The Brethren, described how Justice Powell “prided himself on hiring liberal clerks. He would tell his clerks that the conservative side of the issues came to him naturally. Their job was to present the other side, to challenge him. He would rather encounter a compelling argument for another position in the privacy of his own chambers, than to meet it unexpectedly at conference or in a dissent.”

Chief Justice Burger hired equally from the ranks of former clerks of Democrat- and Republican-appointed judges. Chief Justice Rehnquist, who served on the court with Burger and succeeded him, arrived at the court suspicious of the role liberal clerks could have in influencing his opinion. According to The Brethren, however, that attitude disappeared almost immediately. Rehnquist believed “the legal and moral interchanges that liberal clerks thrived on were good for the Justices and for the Court.” Justice Scalia, when he served on the D.C. circuit and in his early years on the Supreme Court, was known for seeking out clerks with liberal ideologies.

As the Supreme Court has become more divided, this practice has all but ceased. According to a New York Times article in 2010, only Justice Breyer regularly employed clerks who had worked for circuit judges appointed by presidents of both parties. Since 2005, Scalia had hired no clerks with experience working for Democrat-appointed judges. In light of the shift in hiring practices, it should not be so surprising that the court has become more polarized. The justices are in the process of creating their own echo chambers.

Justice Thomas, from 1986 to the time the article was written, was 84-for-84 in hiring clerks who had worked for Republican-appointed judges. Not surprisingly, according to data compiled from the Journal of Law, Economics, and Organization, he is the justice furthest from the ideological center of the court, much further right than the most liberal-leaning justice (Sotomayor) is left.

Thomas once said, “I won’t hire clerks who have profound disagreements with me. It’s like trying to train a pig. It wastes your time, and it aggravates the pig.”* That makes sense only if you believe the goal of a decision group is to train people to agree with you. But if your goal is to develop the best decision process, that is an odd sentiment indeed.

This polarization warns against forming a decision group that is a collection of clones who share the same opinions and knowledge sources we do. The more homogeneous we get, the more the group will promote and amplify confirmatory thought. Sadly, that’s exactly what we drift toward. Even Supreme Court justices do that. We are all familiar with this tendency in politics; it’s the complaint on both sides of the political aisle. Conservatives complain that liberals live in an echo chamber where they just repeat and confirm their point of view. They aren’t open to new information or ideas that don’t fit what they already believe. That’s the exact same criticism liberals have of conservatives.

Although the Internet and the breadth of multimedia news outlets provide us with limitless access to diverse opinions, they also give us an unprecedented opportunity to descend into a bubble, getting our information from sources we know will share our view of the world. We often don’t even realize when we are in the echo chamber ourselves, because we’re so in love with our own ideas that it all just sounds sensible and right. In political discourse, virtually everyone, even those familiar with groupthink, will assert, “I’m in the rational group exchanging ideas and thinking these things through. The people on the other side, though, are in an echo chamber.”

We must be vigilant about this drift in our groups and be prepared to fight it. Whether it is the forming of a group of friends or a pod at work—or hiring for diversity of viewpoint and tolerance for dissent when you are able to guide an enterprise’s culture toward accuracy—we should guard against gravitating toward clones of ourselves. We should also recognize that it’s really hard: the norm is toward homogeneity; we’re all guilty of it; and we don’t even notice that we’re doing it.

Social psychologists: confirmatory drift and Heterodox Academy

In 2011, Jon Haidt, speaking to an audience of 1,000 social psychologists, noted the lack of viewpoint diversity in their field. He reported that he could identify only one conservative social psychologist with any degree of field-wide recognition.

Surveys of sociologists’ professional organizations have found that 85%–96% of members responding self-identified as left of center, voted for Obama in 2012, or scored left of center on a questionnaire of political views. (Most of the remaining 4%–15% identified as centrist or moderate rather than conservative.) The trend has a long tail, but it has been accelerating. In the 1990s, liberals among social psychologists outnumbered conservatives 4-to-1. More recent surveys show that the ratio has grown to greater than 10-to-1, sometimes far greater. A tendency to hire for a conforming worldview combined with the discouraging aspects of being so decisively outnumbered ideologically suggests that, unchecked, this situation won’t get better. According to the surveys establishing this trend toward homogeneity, about 10% of faculty respondents identified as conservative, compared with just 2% of grad students and postdoctoral candidates.

Haidt, along with Philip Tetlock and four others (social psychologists José Duarte, Jarret Crawford, and Lee Jussim, and sociologist Charlotta Stern) founded an organization called Heterodox Academy, to fight this drift toward homogeneity of thought in science and academics as a whole. In 2015, they published their findings in the journal Behavioral and Brain Sciences (BBS), along with thirty-three pieces of open peer commentary. The BBS paper explained and documented the political imbalance in social psychology, how it reduces the quality of science, and what can be done to improve the situation.

Social psychology is particularly vulnerable to the effects of political imbalance. Social psychologists are researching many of the hot-button issues dividing the political Left and Right: racism, sexism, stereotypes, and responses to power and authority. Coming from a community composed almost entirely of liberal-leaning scientists, the quality and impact of research can suffer.

The authors identified instances in which political values became “embedded into research questions in ways that make some constructs unobservable and unmeasurable, thereby invalidating attempts at hypothesis testing.” This occurred in several experiments involving attitudes on environmental issues and attempts to link ideology to unethical behavior. They also identified the risk of researchers concentrating on topics that validated their shared narrative and avoiding topics that contested that narrative, such as stereotype accuracy and the scope and direction of prejudice. Finally, they pointed to the obvious problem inherent in the legitimacy of research characterizing conservatives as dogmatic and intolerant done by a discipline that is over 10-to-1 liberal leaning.

First, the Heterodox Academy effort shows that there is a natural drift toward homogeneity and confirmatory thought. We all experience this gravitation toward people who think like we do. Scientists, overwhelmingly trained and chartered toward truthseeking, aren’t immune. As the authors of the BBS paper recognized, “Even research communities of highly intelligent and well-meaning individuals can fall prey to confirmation bias, as IQ is positively correlated with the number of reasons people find to support their own side in an argument.” That’s how robust these biases are. We see that even judges and scientists succumb to these biases. We shouldn’t feel bad, whatever our situation, about admitting that we also need help.

Second, groups with diverse viewpoints are the best protection against confirmatory thought. Peer review, the gold standard that epitomizes the open-mindedness and hypothesis testing of the scientific method, “offers much less protection against error when the community of peers is politically homogeneous.” In other words, the opinions of group members aren’t much help if it is a group of clones. Experimental studies cited in the BBS paper found that confirmation bias led reviewers “to work extra hard to find flaws with papers whose conclusions they dislike, and to be more permissive about methodological issues when they endorse the conclusions.” The authors of the BBS paper concluded that “[n]obody has found a way to eradicate confirmation bias in individuals, but we can diversify the field to the point to where individual viewpoint biases begin to cancel out each other.”

The BBS paper, and the continuing work of Heterodox Academy, includes specific recommendations geared toward encouraging diversity and dissenting opinions. I encourage you to read the specific recommendations, which include things like a stated antidiscrimination policy (against opposing viewpoints), developing ways to encourage people with contrary viewpoints to join the group and engage in the process, and surveying to gauge the actual heterogeneity or homogeneity of opinion in the group. These are exactly the kinds of things we would do well to adopt (and, where necessary, adapt) for groups in our personal lives and in the workplace.

Even among those who are committed to truthseeking, judges and academics, we can see how strong the tendency is to seek out confirmation of our beliefs. If you have any doubt this is true for all of us, put this book down for a moment and check your Twitter feed for whom you follow. It’s a pretty safe bet that the bulk of them are ideologically aligned with you. If that’s the case, start following some people from the other side of the aisle.

Wanna bet (on science)?

If thinking in bets helps us de-bias, couldn’t we apply it to help solve the Heterodox Academy problem? One might guess that scientists would be more accurate if they had to bet on the likelihood that results would replicate as compared to traditional peer review, which can be vulnerable to viewpoint bias. Especially in an anonymous betting market, confirming the strength of your pre-existing ideology or betting solely on the basis that replication of a study confirms your own work or beliefs counts for nothing. The way a scientist would be “right” in such a betting market is by using their skill in a superior way to make the most objective bets on whether results would or would not replicate. Researchers who knew in advance their work would be subject to a market test would also face an additional form of accountability that would likely modulate their reporting of results.

At least one study has found that, yes, a betting market where scientists wager on the likelihood of experimental results replicating was more accurate than expert opinion alone. In psychology, there has been a controversy over the last decade about a potentially large number of published studies with results subsequent researchers could not replicate. The Reproducibility Project: Psychology has been working on replicating studies from top psychology journals. Anna Dreber, a behavioral economist at the Stockholm School of Economics, with several colleagues set up a betting market based on these replication attempts. They recruited a bunch of experts in the relevant fields and asked their opinions on the likelihood the Reproducibility Project would replicate the results of forty-four studies. They then gave those experts money to bet on each study’s replication in a prediction market.

Experts engaging in traditional peer review, providing their opinion on whether an experimental result would replicate, were right 58% of the time. A betting market in which the traders were the exact same experts and those experts had money on the line predicted correctly 71% of the time.

A lot of people were surprised to learn that the expert opinion expressed as a bet was more accurate than expert opinion expressed through peer review, since peer review is considered a rock-solid foundation of the scientific method. Of course, this result shouldn’t be surprising to readers of this book. We know that scientists are dedicated to truthseeking and take peer review seriously. Arguably, there is already an implied betting element in the scientific process, in that researchers and peer reviewers have a reputational stake in the quality of their review. But we know that scientists, like judges—and like us—are human and subject to these patterns of confirmatory thought. Making the risk explicit rather than implicit refocuses us all to be more objective.

A growing number of businesses are, in fact, implementing betting markets to solve for the difficulties in getting and encouraging contrary opinions. Companies implementing prediction markets to test decisions include Google, Microsoft, General Electric, Eli Lilly, Pfizer, and Siemens. People are more willing to offer their opinion when the goal is to win a bet rather than get along with people in a room.

Accuracy, accountability, and diversity wrapped into a group’s charter all contribute to better decision-making, especially if the group promotes thinking in bets. Now that we understand the elements of a good charter, we move on to the rules of engagement for a productive decision group, how to most effectively communicate with one another. A pioneering sociologist actually designed a set of truthseeking norms for a group (scientists) that form a pretty good blueprint for engagement. I don’t know if he was a bettor, but he was influenced by something very relevant to thinking about bias, rationality, and the potential gulf between perception and reality: he was a magician.