SPEAKER 0
This material has been reproduced and communicated to you by or on behalf of the Australian National University, in accordance with Section 113% of the Copyright Act 1968. The material in this communication may be subject to copyright under the Act. Any further reproduction or communication of this material by you must be consistent with the provisions of the Act. Do not reproduce this material. Do not remove this notice.

SPEAKER 1
I Until Yeah So Please Yes. And then I don't want it, but. Yeah. Yeah 2 days. Yeah I'm not You guys said it was your son Yeah Your I Oh I The Exc I Um. Yeah OK. Yeah I I what No. They OK dokes.

SPEAKER 2
Good morning, good morning, good afternoon, everyone. How are you guys? All right, so, so it's week 4 of the semester. Everything should be going more or less smoothly, I hope, um. So, just uh one housekeeping item today. Uh, after this week. You are in a good position to work on the first assignment, so everything that we do. In the assignment. Covers material up to including week 4, in particular, the computer lab of week 4. So, therefore, this slide here where I encourage you to start working on your assignments. As soon as you're done with your computer lab this week. Um, the assignment is due, I think in 2 weeks on the Wednesday, and you have Oh, so you've got 2 weeks from now. What am I saying there, it should take you maybe 3 hours, maximum of 5 hours working on this. So, Start early, submit next week. How about that in week 5 and uh. Like, like that, you can make sure that there's no last minute stress. Regarding the deadline. I'll, I'll send reminders throughout the not the semester, the next two weeks, reminding you. Um, of the deadline and to start early and not to leave it to the last minute, yada yada yada, but, um, so it's, it's totally doable if you've been to your computers in particular this week'st, um, and then it shouldn't be. Too burdensome. I hope you'll find that. Are there any questions about the Class, the assignment, yeah.

SPEAKER 3
I ask a question, but having a look at the assignment, it seems pretty straightforward like as long as you work through the steps you do the same. How will it just be all the percentage if you do it right, is there differentiation, do we have to do something more right than just.

SPEAKER 2
I think there's a point allocation, some something like maybe there's 10 questions and you can get 10 points. So I think maybe there's a question that counts more but but it's indicated in the question, but um I would say I think the raw marks we give on this is 10 and then we break it or we deflate it to 7.5 I think that was the rule. Um, but I think the median student would get 8 or 9, probably. Does that answer your question? So like the quiz, the The assessments in the semester are Very friendly, which is not to say that the one at the end of the semester isn't friendly, right? But the lowest hanging fruit happens in the semester, so you don't wanna, don't want to miss those. Any other questions? It definitely is straightforward if you, if you've worked through your week for computer lab, because it, it's really out of that lab that this assignment. Is based on OK. Then let's do the stuff here. And the, the portable mic isn't working, so I'll just stand here. Well, it's, it's out of battery. So for the first hour, I'll just stand here and talk to this mic. Um, And I can read off my screen here. So what do we do? Where were, where were we at? We were doing By various stats last week and then we transitioned into the um Simple linear regression model and today We want to finally have a look at how to obtain these. These two parameters that is the intercept and the slope when you're given a scatter plot and you want to fit a line. OK, so specification of the model. First, this is the population linear regression model. Uh, we've seen this last week, I believe, OK. And Y is the dependent variable X is the independent variable or explanatory variable or the regressor. Sometimes they're called covariants, whatever. Um, but they are the right-hand side variables. U is the error term. which captures, so if you read the, the blurb at the bottom, the error term captures all factors that could explain why that are not captured by Xs. That's why last week I called it an umbrella term, or you can think of it as a container, uh, that Could be. Only indirectly specified. In every application you have to ask yourself what could be Part of you, what could be contained in it. OK. And the generic statement is always everything that influences Y that isn't X. But then in, in an application, the question would specifically be, well, give me examples of What other variables you would want to include to explain why, and we will See many examples throughout the semester. Um, beta is the intercept and beta one is the slope. These betas are the population betas, or the. The numbers that we're after, but we don't know. OK, so last week I used my device called the Oracle that told me the numbers. Um, today we Typically do not know the values of data and not me one. The question is, how can we estimate them from data. So our main interest is to learn about the expected effect on the dependent variable of a unit change in the independent variable. Um Courageously we could call this a causal effect. In, in this semester, I'd be very careful with interpreting. Whichever estimates we obtain as causal effects. But we, we will look into that as well. When can you interpret stuff in a causal way? Not today, but as we go along. And in a picture, the causal effect is the slope of the line that we're fitting. So the question that we turn to today is given a scatter plot. How can we fit a line? OK, so that turned. So, this is um what am I saying here, fitting a line boils down here. It boils down to finding estimates for beta not and beta one. Um, those are really the only two parameters. That for this model here pins down the line. Um, statistically we want to know how to estimate beta 1 and beta one. We will use estimation. That's the topic of today. And then next week we turn to hypothesis testing of the unknown beta knots and beta ones, OK, given that we have got estimates in hand for them. So we will have all estimators for beta not and beta one. The question is, can we use our tools from early in the semester where we learned how to construct confidence intervals? Um, in a similar way in this situation here, and yes, we can, and we'll study that next week and we will then also construct confidence intervals for the unobserved population betas, so beta not beta one. So with with the example that the book. Gives us, we had the test score equation. which we want to explain by the student-teacher ratio. Um, what I'm writing down there at the top is the PRF. You go, isn't there an error missing? Well, because I'm saying the PRF is, that is really the PRF. The PRF is the, the line that runs the population line that we would like to know that runs through the data, the scatter plot, OK. That doesn't have an error. Um, Beta one is the slope. Uh, we saw that last week, we can interpret that as the derivative of the test score function. And what we would like to know, I guess I'm repeating myself, uh, beta and beta one. And then later, next week, we want to do statistical inference. At this stage, I'm also pointing out that what we're really interested in is beta one. Beta not is just a byproduct. Wow, it's happening here. Um Uh Mm Sounds like a hair dryer or something. Um, so what we really care about is beta one. The slope we interpret that fingers crossed as some sort of structural slash causal effect. Beta knot is just a device that moves the line into the middle of the scatter plot. Beta one is the thing that angles the line correctly, and that's the thing that we're after, but we will always estimate both of them. So we saw that scatter plot maybe last week or even 2 weeks ago. The question is, how do you, well, how does the estimated PRF look like? And so what we're learning today is where that blue line is coming from. So this is the. The line that we would obtain after applying OS estimation, which we will define in a second, and what we get is an estimate for the intercept 698.9 and for the slope -2.28. Um, those are our best guesses for Betano and beta one. Well, I'm saying our best guesses because there is some sort of Gauz-M Markov result for OLS estimation as well, and that's justify my use of the label best best guess, OK, um, but we'll also investigate that maybe next week or next week. So the estimated PFF is here at the bottom. And it's 698.9 minus 2.28 times the student-teacher ratio. You can plug in values for STR. So, for example, 20, you get a specific number. That specific number is your Value Um, your predicted test score or expected test score for a person who had 20 classmates. OK. And that's an estimate because it was based on the estimated beta or beta, beta not hat and beta one hat. And So here I'm just repeating what I said. And the sign of course is As expected, we would expect a negative sign. Um, the number itself could be interesting, um, how the STR uh changes in in So in size or if one student increase in class size decreases expected test score by 2.28. OK, that's a specific number. What's interesting also is you could quite literally interpret the intercept. As the predicted test score for students, what am I saying? Who didn't have any classmates, right? Because if you stick in zero for SDR, which you can. Then you get 698.9. So the the literal interpretation would be that's the wrong one, that if you are a student who didn't have any classmates. Um, I predict. A test score of 698.9, but if you look at the scatter plot, how many students in the data have zero classmates. So you look here on the left, it starts at 10, and there are no dots. The smallest class, class size supported by the data is, what is it? 14 maybe. OK. So that you could plug in, but you wouldn't go lower because you can argue, I mean, even though the line, if you see the line of course can run outside of the scatter plot, just mechanically can do that. The question is always, doesn't make sense. And you can convince yourself that it doesn't make sense because you can run the line into the negative numbers also. What, what would that mean? So you always have to put common sense in your interpretation as well. Is that a question?

SPEAKER 1
I guess it's a good linear regression. Use like the sample With this one, if you're going like something less than. That's an out of so and isn't that Say that

SPEAKER 2
again. Oh, so yes, so, um. Yeah, you can say maybe you have knowledge that there are classrooms, and this was in California that are smaller than 14, just your sample didn't include them. You could make an out of sample. Prediction, right, but the question is, given that particular sample, how confident are you? How far would you go for class sizes of 13, I would still be relatively happy to do that, right? But then that's why I mean you use some common sense or reality check, you wouldn't go to 543, or something like that. But it is technically an out of sample prediction which you can do. But then the question is um How useful is that? We will talk about more about this when we do time series at the end of the semester where we definitely want to do out of sample forecasts. So for example, we are currently in I have to think we are in March, so quarter 1 of this year, and we want an hour of sample forecast for quarter 2 of inflation, for example, because who cares about last year's inflation? I mean, only to the extent that You use it as an input to to make a forecast model. So out of sample is of interest, but when we go out of sample, we always have to ask ourselves how confident we are. Was there a question here?

SPEAKER 1
Does that mean like Vigorous way to Common sense is

SPEAKER 2
never Rigorously Um, no, no. Um Yeah, uh, let me note, uh. Digress too far, but um. I picked extreme cases which clearly don't seem commonsensical, like 0 or -7. But how about 9 or 8? Then I can't tell you that that's a bit of a blurry, blurry area. And I've got no, no guidelines for you. It really depends on the application and the data. And your understanding of How representative your sample is. If our sample is representative, then we would have all the small classrooms in here as well. OK, moving on, where were we? OK, so we still. Yeah, I think I had that Yeah So, we want to investigate how we get that line, OK? That is, we want to learn about estimation. Now, I'm using this trick again where I attentively assume that I do know the truth. I do know the betas. When I say betas, I didn't say beta hat, I mean the betas, the population betas, the true intercept and the true slope. The estimators of them will be called the beta hats. And on the previous slide we obtained the estimated PIF right that was the line that I could put into the scatter plot after using the data to come up with estimates for betano and beta one. Those estimates were beta not hat. And be one hat. OK. Here, as a generic picture. I'll give you a scatter plot, the red dots. And all I'm trying to say, so the solid line is the PIF. That is based on the population datas, OK? And the estimated PFF is the one where, where I use my fancy method called OLS to obtain. Estimates Good guesses for beta kno and beta one, those estimates being beta not hat and beta one hat. Once I obtain them, I can put that line into the picture. This picture suggests that you obtain a different line just because your beta not hat. Will not magically be equal to bait or not. And your beta one hat will not be magically equal to beta one. They're estimators. They're hopefully close. That's why the dash line is sort of similar to the solid line, but this picture is already telling you the estimated line that we're putting in there. Is itself only a guess, OK. Makes sense. Now, what I'm doing here. This is the picture that you will see at the end of the day. You don't get to see the solid line. So one note, this is it's a bit of a sorry or sad picture because the estimated line looks like a very bad approximation of the scatter. That is just my poor skills of exposition. OK, a second, the estimated line that we put. Usually, you can convince yourself in your computer labs, it's visually much nicer than the one that you're seeing here. OK. What was your question?

SPEAKER 3
the uncertainty between the estimated supply and the population. Isn't that we're not good at figuring out what line fits to those that group the dots, it's that when we add all the rest of the dots to the population that line. No, no, we, we do have a very good way

SPEAKER 2
of getting that dashed line. The uncertainty, I think you expressed this quite well, is if I gave you twice as much data, the line, this line, the estimated line would adjust properly and go back to the previous picture and asymptotically, meaning for large samples, approach the solid line. We have a good procedures estimator to obtain the dashed line, and that estimator will have the property that if I give you a very large data set, it will be very close to the solid line. And here I'm just telling you in this picture, I'm just reminding you there is actually no oracle that tells you the values of beta not and beta one. We have to use the data to estimate them and then you can plot this picture. OK. But that's a good situation to be in, in particular, after what I just told you that the estimator, the OLS estimator, that these two numbers, beta not hat and beta 1 hat, have this property that they get very close to the original betas, which means in the picture that the dash line moves very close to the solid line if your data set is large. Now, terminology. So here I'm comparing these two states of the world again, we're in the left picture, you are in the Position or we do see the solid line, but that's just for me to point out terminology. The right picture is the estimated PF. That's the one that we get to see at the end of the day. But why I'm sort of comparing them here is to point out different terminologies and in particular, Can I I'll use my laser pointer maybe, um, in the left picture, um. What am I, so I'm picking a value for X here, and you go up to the line. That is from last week, the sorry, not the estimated, the, the conditional expectation of Y given X. All That's the um what do we call it, the, that's the point on the population regression function that tells you for this particular X, we expect this particular value of Y. OK. And so the dot, the point on the line, the dot above here, as the actual realisation. So there's a combination of Xi and Yi that gives you this dot here, and the vertical difference of that dot to the line is the error. Contrast that to its equivalent in this picture here. It's the same dot, right? But the line is different. This is the estimated line. So the same dot here, the vertical distance here is called you hat. That's the residual. Which is not the same as the error. So the vertical difference. Between YI and the line is either the error when the line is the POF or the residual when the line is the estimated POF. Does that make sense? So this picture makes that distinction very clear, um. So try to. Try to internalise this. There's a difference between errors and residuals. It's a matter of definition. The, the difference between a realisation and the PIF is the error, and the estimated PIF is the residual. OK. What else? I think that's that's the point of this slide. Are there any questions on this? OK, good. So this is sort of a A visual or graphical definition of error and residual. Here's the Mathematical definition. So first, the predicted value of YI is called YI hat to indicate it's an estimator for YI. It's simply the estimated PAF, which is beta not hat plus beta 1 XI. So in the previous picture, the predicted value of YI is this guy here on the Estimated PAF, the point here, where you you plug in this particular value for XI. The predicted value of the outcome for someone, if it's a person whose value of X is this XI is this value of Yiha and the actual realisation for this person is higher here for this person, but there's another person somewhere lower here that has a negative residual and like that. They vertically differ um around The estimated PF. And then the error. is given by the difference between the realisation YI and the PIF. And the residual is the difference between. The realisation and the Uh, estimated PFF. So here, instead of writing Y I hat, I could write what I define upstairs, beta not hat. Plus beta1 had XI subtracting that off here and it looks symmetric to this one. So you get the error is the difference between YI and beta not minus beta 1 XI and the residual is oops. Why I minus beta not hacked. Minus beta 1 hat XI, which is this guy here. Makes sense? OK, then what am I saying here? I yeah. So you can map to the original observation by taking your predicted value of YI and adding the residual. So that's what the first line says, really. Let me go back to To the picture two slides earlier, this is just saying you can go here to this point by taking your predicted value Y hat. And adding the residual you have takes you from here up there. That's what that corollary says. And likewise, you could take the PIF version of that, meaning find the point on the PIF and add the error, you also end up. In YI. That's what the second result says. And then down there I'm just stating the obvious that. We haven't even defined where the beta hats come from, but our knowledge from estimation of in the 1st 2 or 3 weeks or so, you wouldn't be too surprised that that if you obtain an estimate for beta or not, that estimate is beta not had that that has a very low probability of coinciding with beta not and likewise for beta one. The estimates are never. Exactly equal to the things that you're after. OK. That's a relatively Fair statement. Once I know that the estimates have a normal distribution and a normal distribution is a continuous distribution. Every point on the real line has a zero probability of occurring. That makes that statement true. But of course, this looks defeatist, this result here. OK. Of course. The estimator that we're proposing is of good quality, by which I mean has good properties, and we have a chance of being close to the truth, and close is good enough. I mean, that's that's as much as we can. Hope for Now the question, where do the beta hats come from both of them. So we're returning to the square's estimation now. So what we do, what we're proposing is a bit arbitrary but it works out well and so we looked at it last week in the workshop session in the univariate. case, we minimise in some sense the difference between the estimated population regression function and the observations. So we're going to put a line in there that minimises in some sense all the individual distances that you're getting. OK. So all the vertical distances. That you're getting Between the line that you're putting into this picture and the and the dots that are given to you, you minimise that. Then Um, and you can point out, well, some points are above the line, others are below the line. Don't they cancel each other and are zero? Yeah, that is actually true. That's why we turn to a quadratic criteria. We square them. OK. We will prove later that yes, they actually average out to zero, the above the line dots and the below the line. That's when I'm talking about this, I'm, I'm thinking about this picture. Uh, let's go with this picture here. This is, we are free to put the dashed line in. How do we want to put it in? Uh, this, like I said, this one looks unfortunate. Definitely, you wouldn't put one here at the top. You wouldn't put it above the scatter plot. You wouldn't put it below the scatter plot. You wouldn't put it somehow inside the scatter plot. And what you want to do is roughly put it through the middle of the dots, OK. We define this now rigorously, mathematically as The least square squared criteria criterion where you want all the vertical differences that you're getting. So you've got all your dots here 100 dots. If you have a larger data set, you have 10,000 dots. You have 10,000 vertical differences to the line that you are free to put in there, and you choose the line, the one line, and that's the cool thing. There's really only one that minimises the quadratic. The sum of the quadratic differences. OK. And so going back to my definition. And the trick is to look. At this criterion here where we are free to choose B and B1, they're not called beta here on purpose. Currently there are just generic arguments which we are free to choose with the objective to minimise the function that's given here at the bottom, which is take. All the resulting vertical differences that you're getting. Square them And choose the Bs, be not and B1 in such a way to minimise this. That's The arbitrary criterion that we're choosing. And you could be clever and say why squared, why not take absolute value differences. That can also be done. That's a different estimator. It's called the least absolute deviations estimator. That's a non-differentiable function which Jose has taught you about. I trust. Um, you don't want to deal with that, OK? We want to take derivatives. So this is a very well behaved objective function here at the bottom. It looks ugly. The main piece that's ugly here is the summation operator, but really, if you think about it, that's just the summation. And the squaring here doesn't cause any troubles, and it's actually a functional form that behaves well for the purpose of taking derivatives. Any questions? Have you guys seen this Motivation before these squares. I don't know. I'm not feeding you guys. So you have, OK, if not, I mean, here it is. OK. There you go. You can accept it. It's in a way what I told you a few weeks ago, for any estimation problem, there's always infinitely many estimators. So another objective function would be 42, and it's not an objective function. OK. Just choose 42 as values for beta not and beta1, OK. But that's nonsensical. So here, this sort of Just drops on you and you have to accept it and trust me it will have nice properties, but the motivation. It doesn't seem far-fetched or it seems intuitive. I, I hope at least I conveyed that. OK. So, that's the plan to To choose B not and B1. minimising this objective at the bottom. That puts a unique line in the picture. That looks Representative of the scatter plot. OK. So the set of solutions, so our goal is now to choose particular values for the bees and call them beta not hat and beta one hat, and those are the unique minimizers of this function. Turns out there is a minimizer and it is unique. It's not like if you programme this up in Python or whatever, uh Python will say, oh, I've got 500 different combinations of betano and beta one. So combinations of intercept and slope. But describe your sketch up plot. No, there's only one. That's cool. Um And it's also easy to compute. So here's the mathematical definition of obtaining beta not hat and beta one hat. You want to find the arguments that minimise this function. The arguments here are pick B not and B1 so to minimise this quadratic sum. So we look at the whole right hand side, so after the arc bin as a function in B and B1, because YI and XI is given to us. That's the data. I'll give you an Excel spreadsheet or whatever CSV file with the data. Um, That's just get upload. You want to find, or you read this here as a function in two variables, B and B1. Um, it's a quadratic function because we have the square here. And our goal is to find the values of B and B1 that minimise this function. Um, the values that minimise that function are called the solution, and we give them a particular name, beta knot hat and beta one hat, and we call these our OLS estimators for beta knot and beta one. Um, the geometry of the minimization problem. is you have a quadratic function in Uh, 2 dimensions or 3 dimensions, whichever way you want to look at it. This is a pseudo 3 dimensional picture. It is Reminiscent of, so if when I take my son to the arboretum, have you been to the arboretum? There's this. This, I don't know, how do you call this shape, um, this inverted. Mountain. The valley, valley, there you go. There's a little valley, OK, where he can take his balance bike and just roll down. Where does he end up? At the OLS estimator. He doesn't know this, but, so if you put a marble up here and you ask, let it go down, where does it end up? At the bottom. That's what you're after and that's what we're doing mathematically. OK. The, the real life application is at the arboretum. There's this valley that looks like this near the visitor centre. Um, you see the kids rolling rolling downhill there all the time. And um the nerd that I am, it reminds me of this picture here. Uh it's a bit sad, I know. But uh um so we are finding this thing here at the, this unique minimizer. But what what you're seeing here is at the bottom, it's not flat. Right? Uh, it's not like my, my bottle here, which has many minimizers here at the bottom. You see, because it's flat, OK. That's, you get the idea. This one doesn't have that problem. There is a unique minimizer at the bottom. And it's easy to obtain for computer software and mathematically for us now in the next 10 minutes, we can work it out. And Yeah, right. And then in multiple regression analysis, you would have the K dimensional equivalent of that, which you can then not visualise anymore, but mathematically, it behaves the same. Any questions? Cool. Well, let's do the heavy lifting now, the maths of, of that. It's relatively easy, if I say. Because you took EE 1001, here I'm, I'm mentioning that with Jose. Um, or to, I don't know. Um, you have, you would have done two dimensional, what's this called, uh, when you take derivatives, calculus. Have you done that? Don't say no, please. OK. All right, so. You're going back 2 slides, 3 slides. This is our objective function to an economic student, if you see that, you should start getting sort of Pavlovian reflexes where you go, you start drooling and you go, I know what to do. Let me off the chain. You take derivatives, OK? You take derivatives. You don't think like a mathematician. You just take derivatives and fingers crossed it works, and actually that's what will happen here. OK. So we take that objective function and Go differentiate. That's the first step. Differentiate, um, so I'm differentiating here. And we've done this for a slightly simpler function last week. And so let's read through this. We look at this as a function in, in two arguments, B and B1. So we take derivatives with these two. Arguments with respect. Uh, are there any questions on, on this here? Um, so it's um, it's called chain rule. The outer function is the quadratic function. Here, this two appears up here. Why is there a negative in front of both derivatives? Where does that come from? Um Yeah, right. The inner function is I minus minus B1 XI. So if I take derivatives with respect to B, I've got a negative B, so that's that negative here. The summation behaves. Very easily, right? You just carry it along. OK. It's almost like it wasn't there. It is there and it's important, but it doesn't really cause any troubles for taking derivatives. And then this XI here is because you have B1 times XI. So you're taking derivatives not with respect to X, right? You're taking it with respect to B1. In the 2nd, the 2nd first derivative that we're taking here. So we get this XI here. OK. So We've done that, that step, then you go good oh yeah with with respect to.

SPEAKER 1
Ah, yeah.

SPEAKER 2
So anything that doesn't carry an I sub script. Can go in front of the summation, it doesn't have to, but it makes it look cleaner, but when you have an I subscript, you can't just take it out of the sum, unfortunately, because. Yeah, that's right, yeah. And the negative could could go in. Only the two can go in, negative 2 can go in, but anything that has an I sub script does not leave the. Summation operator. So what we have now is two derivatives. What's the next step that Jose taught you? Uh, set equal to 0, OK? We've got, then we look at this in two equations and two unknowns. I go, fingers crossed this should work out and here it will. And at the setting equal to zero step, this step here, that's where I turn, I give my, my generic bees, I give them specific names. I call them beta not hat and beta 1 hat because that's where, OK, going back to this picture, that's where your pin fix this point down here. Where generally B1 varies here on this line and B not on that line, but where you Where you enforce the first order condition, you set the derivatives equal to 0, that's where you go, that's where you make your attempt to find this spot here, which is represented by these two points, beta not hat and beta 1 hat. So that's where I now call these guys beta not hat and beta on hat, OK? Because I'm setting equal to 0, that pins down the location. And the rest here is just Sit down and solve, OK? And it's it's a bit ugly. And we're going through this here. Um, 3rd step. Here, what I do is use the 1st, 1st order condition. And so for beta not hat as a function in beta one hat. So I'm just copying and pasting from the previous slide, the first order condition, and then Do you see that in the second line, I'm dividing both sides by 2. And in the third line, I'm breaking up. The sum of the differences is the the differences of the sums or whatever. OK. To ask Jose. And uh in the fourth line, What's happening In the 4th line, um, what's happening here with this guy here? What I have here is the sum running from 1 tonne of beta not hat. beta not hat. Is some number that we're adding to itself. So that is equal to n times that number, right? And what happens with the third term here? Well, this beta 1 hat doesn't have an I subscript, so I can take it out of the sun. And then I'm rearranging. I wanted to isolate beta not hat, so I'm getting this result and then copying and pasting. Now I divide both sides by N. So what I have obtained is if you read through this. The OLS estimator of beta node. Called beta not hat is Y bar minus beta on hat X bar. That's a simple looking result. We know these guys, these objects Y bar and X bar, they are the sample averages of your dependent variable and your independent variable. They are easy to obtain. Now, if I told you, or if I gave you a data set and I asked you, use this formula and give me beta not hat, the only ingredient that's missing here is beta one hat. But luckily, We have another degree of freedom. We have another equation to use, the 2nd 1st order condition that will help us pin down beta one hat. So that's the next exercise. Are there any questions on this? Good. Super. Um, yeah. So this is, this is something to, to memorise. OK. Um, and I think you'll get used to it. You won't have to memorise it. I think you'll get used to it because you'll, you'll see it lots throughout the semester. So the intercept estimate, or the OLS estimate for the intercept is simply Y minus beta 1 hat. Expa. Now This looks ugly, but we, we can do it, OK? And I, I think all I'm saying here is Uh, we have another first order condition available. So what we will do is we use that result that we just obtained for Bay and not have, and plug it into our 2nd 1st order condition, which I'm repeating here. Um, And then we'll read through the rest of it. In 10 minutes. Now it's time to take a break, yeah. Could Yes Yes No.

SPEAKER 1
Oh yeah. I I Thank. Wait so.

SPEAKER 2
How does this happen? And so here you have the sum of beta not had. So I said be not had some number that you're adding 10 times to itself. You go not terms. So that's 10 times.

SPEAKER 3
And for this. the submission for the 01.

SPEAKER 2
Oh, that, that's as usual and something doesn't have an I sub script, you can take it out of something. Oh, I see. OK, so that's more, yeah, yeah. So anything that is inside the sun, like there's two. Any number that you can, you don't have to, but in this case it helps us.

SPEAKER 1
this is like to be great. Yeah you know I Right. And If you So. So I get my name.

SPEAKER 3
I My question.

SPEAKER 1
I am most 2 One You OK I can do this.

SPEAKER 3
I don't know I.

SPEAKER 1
this I Just I think it's time to ask this question. I I Oh Yeah, sorry.

SPEAKER 2
But you'll, um, you'll get a better sense of that. Uh, as we go through the semester, definitely the things that we're doing here. And you get a practise test, maybe 1 or 2, and you, you'll see to what, but it's not that the exam is full of proofs. You'll, you'll you'll you'll see. I'll try to give you a better idea as as we go along.

SPEAKER 1
I'm Oops. S Yes. s right. Mm. Yeah. Uh. That what. just I I right It's. It Oh.

SPEAKER 2
I All right, let's um ease back into it, into this friendly looking slide. The, the mic works again, it's fully charged. Um, so, we're using the 2nd first order condition, the one where we differentiate it with respect to B1. And that's this one here. We saw that before. What's happening here? I'm dividing both sides by -2. What's happening here? What is happening here? I know, I'm sticking in, they are not hat. As we've just derived or obtained it. Here OK. And what is happening here? So line number 4. Um, I'm just combining here. That is not too fancy. Um, line 5. OK, this. What's happening there? OK, I'm multiplying out. This XI here pre-multiplies this big parenthesis here. This big parenthesis has really 3 terms in it. So I'm getting basically 3 terms there. Um, any questions on the 5th line? Oh, I know, I know, exciting stuff. Then, so we've got, I see this here as three terms. So, They are all inside the summation, so I'm not being too explicit. I could put big parenthesis around it, but it's implicitly clear because all these terms contain objects that have an I subscript, so they all belong to the summation, but you can also write that as separate summations. That's what's happening at the bottom here. And then I'm doing more. So for the first term, so at the very bottom, there's nothing fancy going on here, obviously. For the second term, I've got the negative here obviously, and then I've got Y XI. So and I have got that sum, so I get, I get this Y bar. I can take that to the front because Y bar doesn't have an I subscript. Things that don't have I subscript like the number -2 earlier, I can take to the front outside of the summation operator. That's how this Y bar shows up here, right? That's this Y bar here where I'm now. Applying the summation operator individually to all of these three terms, it doesn't change a thing. It just makes it. Look differently and taking this Y bar in front. So I get some XI. And likewise with this term here, negative beta 1 hat times XI2 minus XY XI is this term here where I take the Bhat one in front of the summation operator. Copy and paste from the previous slide and now rearranging. It looks very ugly, I understand that. But uh let's work through that. Um, so here, I'm just putting a big parenthesis around it, not because it's necessary, OK, just to sort of isolate it from the rest. And then what's happening here? How do I get N times X barx Y bar? I'm getting this obviously from this term here. But how do you, if you compare. I'm obtaining an N times X bar here. Where does that come from? Oops. The N by times X bar. I actually I'm writing it. And we will do this in the workshop. So let's not waste too much time now. But if you have a son. Over the XIs, you can also take the average X and times it by N. It's the same. OK. So, for example, I could ask everyone here, what's your height. I could add all your heights, then I get the The total height of all the students in this room. Or I could just get your average height and times that by, by the number of students in this room. Same number. OK. And then here nothing fancy happening. Yeah. But this looks ugly, and we must clean that up. That's what's happening in the mill. Um, how does that work? How does that work? Uh, sorry, um, I'm not cleaning up anything yet. I'm taking this line here. And I'm moving the first two terms. Actually, what I'm doing is I'm, I'm moving this term here, the, the last term, the negative beta one hat times the sum, moving it to the other side. So it pops up here, right? So I'm just moving the third term here. Over to the left hand side, then it ends up here. And these guys here are the same as these guys there. Now, I still have, like I said, I still have to clean this one up. I mean, what I could do now here, if you look at this, this achieves the result, because our goal is to get beta one hat. All I would need to do now in this ugly looking line. is divided by this big summation here. And then you get the vision by that on the right hand side. So then you have successfully isolated beta one hat. On the right hand side, all you have is data. That's our goal, but we have an extra goal. We want a nice, a nice looking result or um. A pretty result. OK. And so we do one mathematically unnecessary step, but aesthetically necessary. And that's what we're doing now. We make this thing here look better. And how do we make it look better? How we work it out here. We, we break up the two terms that are inside this parenthesis here, like so. And this X bar lands here in front of the sun, OK. And then using this trick here again, that the sum of the XI is the same as N times X bar, applying this here as well, you get X bar here. And N times X bar there, which is N X square. Does that make sense? It's all ugly, what is it? summation operator, so I'll ask, I'll send you Jose if you've got any questions. I'll outsource this. Not, not my, not my business. Woohoo. Um, so isolating now, beta one hat, we get this. So what have I done, previous slide, I've made this thing here look prettier. By simplifying it to this. So I can divide by this here to isolate beta one hat. So I've got beta one had left on the right hand, on the left hand side, this term divided by this term on the right hand side. That's what this is here. Is that right? Yeah, that's what I'm getting. And I, I go, OK, we could say, job done. What we wanted is an expression for beta one hat, which we have obtained on the right hand side. The only things that you can obtain from your sample data. If I give you an Excel spreadsheet, you can calculate the right hand side. That's what Python will do for you. But there is one extra step to make it look pretty, and that's this one. It's not mathematically necessary, but it makes the result more intuitive. Let's have a look. What is the, what is, what is it that we do, and we prove that in the workshop. So let's, let's not uh justify it now. Accept that and you can read it from the, from the right hand side to the left-hand side. Like if you have an expression like this, you might as well write it like that. We show this in just after we finish this discussion in the workshop bit. So if you eyeball. The numerator and the denominator, they have this structure here. So you can apply this result where you go, so the numerator then is this particular summation, which has exactly the structure here, so you can plug this in for the numerator. And if you think about, well, the denominator actually has the same structure where if you replace Y bar and Y by X and X bar, you, you get this. So there's a version of this for the denominator, which is The sum of XI minus X bar square. So We have this result, we made it look a little bit prettier here. And now I divide. Numerator and denominator by N, I obtained this result here. And why is this now more intuitive? Why is this useful? Because if you think about what we have just written down, upstairs, numerator, that's the sample covariance between Y and X. The nominator is the sample variance of X. That's why we had to do all these rearranging of the summations to obtain this more relatable result. The estimator for the slope is in words. The sample covariance between X and Y over the sample variance in X. This is something to internalise as well where I go. Just know this. By heart. Just I mean be able to derive this. And Also be able to just write it down as a result. Because it's so easy to remember also. The slope estimator is sample covariance over sample variance. And you just have to remind yourself what covariance, well, there's only two variables here between X and Y. What variance at the bottom of X. So the OLS estimator of the slope is equal to the ratio of sample covariance and sample variances, very simple result. So, last slide, collecting the results, we actually get very easy expressions for both. Um, I would say for the slope, it's almost easier than for the intercept, OK, but this is definitely something that you can memorise. Definitely something I would now expect you to be able to derive as well. Uh, enough time to practise this. Um, so on the right-hand side, you only have things that are computable of the sample data. Um, and yeah, Python will do this for you. If you, if you give it a spreadsheet, data will do it. What else? Excel will do it. Mini Temple, I don't know what else. JuchiT will do it for you. Um, any questions? I know this was a bit ugly. Um, But can't be avoided. We arrived at a relatively pretty result. So we set up this arbitrary objective function to put a line in a picture that minimises some of the square differences. And the points that achieve that are these guys here, which have a relatively intuitive. Um expression, OK. Are there any questions on that? OK, super. Oh. Now Let's do the workshop. Unfortunately, this year, this iPad still doesn't work, and they, they've tried to help me. I can make it work in, in the CBE precinct, but somehow not here. Usually I just hook it up and I can write on it, but it just doesn't work. Um, but I think this is not too bad, I hope, and so we'll keep going with old paper and pencil. Um, so let's turn on the dock cam. Um, which chair is good. Is this, I guess you can see, can zoom in. So it's mostly technical today, it's unfortunate, but uh let's do it. And anyway, So in exercise. One So we start with a very easy one. So That this is equal to N times X bar. So that's the claim. I'm too lazy to write the I subscript here. You may do that too in an exam situation. OK. Our our sums, I think always run from 1 tonne. So what's the point of writing it down 600 times. Um That we want to show now, OK. So I could write. Prove this. So how do we start? So we start by this. What, can, is this too small? Should I zoom in more? Nobody complains, you can see. Yeah, thanks. What do you do? This is a very easy one. What's the first manipulation? To you It's easier you're going in the other direction

SPEAKER 3
starting from the definition of exile could do that too,

SPEAKER 2
yeah. That's good, and I started this. Anyone, any takers? So how about I'll go to multiply by N and divide by N. Can you do that? Yeah, of course you can. You can multiply by 42 if you divide by it, right? And then, OK, I can now make it look. Like this, so that is 1 over N. And this year Is the definition of X bar, right? So that's how easy that is. And my My real life story was, I could ask everyone in this room, let's say, 40 students, how tall are you? And add all your heights. I don't know. If it's 40 students and you're all 2 metres tall, you're like, I get 80 metres. OK, it's a bit a bit much. But something between 50 and 70, right? metres is your total height. But I could also take the average student. I have no idea who this is, in terms of height, and multiply that by 40, I get the same number. And that's the mathematical statement of that. It's very, very relatable. OK. Alright, Part B that's it. I'm slightly trickier one. So I don't want to write down the whole thing. Um, but what we start with is Well, Yeah, I don't know. We start with this. OK, let me write it down. So I want to prove this. is equal to XIYI minus N X Y bar. And then just to be safe, we can put big parenthesis around because we're not suggesting that N times X bar Y bar is inside of the summation. So So here is my So I have to start with the left hand side. So here, I just use brute force. Multiply out, simplify, clean up. So you get, you get um 4 terms, right? So you get Let me put parenthesis uh X I Y I minus. XIY bar. Minus X Y I. Plus X bar. Why? Everyone happy with that? So we get Some of XI. Why I, let me be slow and go. I've got 4 terms here. So I've got 4 sums. I can go minus the sum of XIY bar. Minus the sum of X YI plus the sum of X Y bar. That expo is inside of the sun. Now, I can use that rule that things that don't have I subscripts can go in front of the sun. So like this guy here. Can go here And this chap here can go there. And these guys, well, we will deal with these guys in a second. So let's, let's do this. So with no change for the first term. My bar sum of Xi minus X bar sum of Yi. Plus, now let's, let's be brave. What do we do with the sum of X bar Y bar? You can use What, what can we use? And it's like you have You sum a constant times, you get N times the constant. What's the constant here? X bar times Y bar, right? So, I can say that is equal to N times X Y. Because it's summing X, the product of X Y N times. Whatever the values are, it doesn't really matter, but you get N times that. And now for, for these guys here in the middle. You use the rule from upstairs. You go, the sum of XI is N times X bar. So you get Some of XII minus. Y N X minus X N Y plus. N X Y bar. Then these guys kill each other. And we're done. Any questions? So yeah, it's boring him at one hour on. Working with sums, but that's That's what you need to do in regression analysis. That's, I guess that's why we're teaching it in World War One, because it's needed here. How do computers do this? If you code this up in C++ or C, I can see that as loops. Or if it's in Python. The way I would look at this is I turn this or transfer this or map this into linear algebra. And deal with vectors and matrices instead. But the foundation is this, OK? Any questions? Right. That was easy. Uh, where, what are the other questions? Right. Exercise 2 Some facts about OS, those are relatively important, so This is a meaningful exercise, unlike the first one. The first one is just gives you some tools for working with something that's helpful. But here, this is some deep results or some really crucial results. About ordinarily square estimation. So So facts about Or less So, oops, the first statement is this. 1 over N sum of UI hat is 0. Proof I don't. Good point. Yeah, good. Why do I write this down? If you multiply both sides by end. It's equivalent to saying. The residuals not average, some out to zero. That's what I said earlier. That The vertical distances. that are above the line, and the ones that are below the line cancel each other out. That's the result. The square differences don't. That's our objective function. This is not our objective function. This is just a trivial result of OLS. So how do we get this? So first, what we need for all of these exercises. Is a Recall That we obtained this result for the intercept estimator. It was Why bar Minus beta one hat. X bar. So that's something to remember. We, we derived this in the first hour. So why am I writing this here? Well, it helps us here proving this particular statement. So, you pointed out that the division by N here is redundant, so let me, let me take it away straight away. So let me just cross it out. We don't need the division by N. Um, so what is a residual? So to start the proof or the derivation, so, um, so I could say we want to Prove this, so let's start the proof. And I have my little lemma here that beta not had is Y minus beta 1 X. So we recall. Um, where do I start? I Y I minus Y I hat, do you remember that? The difference between the realisation, that's the Yi, and my prediction for it, that is based on the estimated PAF, that's why. Um, so what I want to do, yeah, so that is equal to. Why I minus or what is Y I had is It's The point, the prediction is the, the point on the dashed line, if you remember that, which was given by beta knot hat. And let's put parenthesis around it, plus beta one had XI. Right? This is the estimated PIF. That is what Y hat is, OK. And for a person. Um, whose value is XI. OK, I predict this person to have a Yi of beta not plus beta on XI. OK. So now I sub in my bed are not hat from upstairs. Um Yeah, that's right. Please let me know if I make any sign mistakes which I do. Quite a lot. Everything OK with my signs? Looks Looks good to me. You stop me if I'm wrong. So we go. Let's collect terms. Let's collect the YIs. So we get YI minus Y bar. Let's collect access so we get Let's put it like this, beta one hat. So what should I put? Here Beta one hat multiplies. It's 6. X minus XI. It doesn't look so symmetric, but it doesn't really matter. So I could put a minus sign here, and I get XI minus X bar. But it doesn't mean OK, why am I doing all of this? I want to, is it still on the same page? I want to study the sum of this now. So what I've written down here is another expression for you, I hat. So let's Put this to the sun now. So it follows. That some of That I don't need parenthesis here. So, so, so sum of you are hat is equal to the sum of YI oops, here we go, minus Y bar. Let's go fast plus beta 1 had sum of X minus XI. What's next? by symmetry, but you could just turn the sum yi

SPEAKER 3
minus y bar into. Uh, NY.

SPEAKER 2
Yeah. So you get Yeah, how slow must I go? Does everyone see that this is 0. No, should I go slow, we break it up, we go, some of why I. Minus sum of Y bar. Super slow. Plus beta one hat, um. Sum of X bar minus sum of XI. equal to, while the sum of YI was on the page. The sum of YI Is N times Y bar. We showed this earlier, so that. N Y bar. The sum of the Y bars is N times Y bar. And so same story here. So you get NX minus NX bar, which is 0. Using the same tricks over and over again, like dealing with summations and summing XI is equal to Nx X bar, summing X bar is equal to N times X. There's a pattern there. There's no, yeah, the question. Oh, did I? They're supposed to look like capital. Which one doesn't look like one, these guys? I mean make them look More important now. That's just Sorry? Yeah, well, yeah, it doesn't, but uh I I meant for all of them to be upper or capital. um. Yeah. I don't make a distinction between a random variable and this realisation. I just use capital letters. So what's the next question? Any, any questions on that? I know it's not exciting, but so. Uh, it is important though, let me just emphasise this again. What this means is we're putting our line into the scatter plot. In such a way that the residuals that we then obtain some out to zero. This is our free choice, but it happens to be the choice that we're making here. The OLS estimator, the OLS line that we're putting has this feature that I guess it put in this way, it puts the line through the middle. OK, in that sense, because they all average out to zero. When I said earlier, so a line like this. So if I, if I give you a scatter plot. So this, these are all supposed to be dots. So this here, would not be an OLS line. Because you have more positive residuals than negative residuals. They will not average out to zero. Does that makes sense. Now, but there is, I'm just making a conjecture, don't ask me to proof. Go ask Jose. Oh, it's really convenient to have him. Um, there's many lines that have that feature, that the, the residuals put some out to zero. Our line happens to have that feature too. OK. Our line has an additional feature that it minimises the sum of the squared residuals, which is a good feature to have. Next So part B. So Prove this One over Some of why I had is equal to Y bar. So we know if it didn't say why had, if it just said why I, then that would be definitionally true. Um, but it doesn't say why, it says why hat. And so here, Let's start our proof we recall. I had, didn't we write this down just now? Yeah. I Beta not hat plus beta one hat XI, right? And now I'm recycling my result from the previous page where we go for beta or not, we had Y bar. So now we get the relationship to Y bar. quite visibly, uh, but there was more minus beta one had X bar. What I've written down here now is just beta not hat. And then I still have to go plus beta 1 had XI. Let's clean this up. We go Y bar. Now that's right it likes. Yeah OK OK. That was just why hat. Now we're interested in averaging the Y hats. You can sort of see what happens. But let's not just sort of see it. Let's drive this home. So therefore, Uh, 1 over N. Some of what I had. equal to 1 over N sum of Y bar plus waiter 1 had Xi minus X bar. Break this up. We have basically 2 terms, one over n, sum of white bar is 1 term. The parentheses are not really required, but it looks better. Plus 1 over N, beta 1 hat. Sum of Xi minus X bar. What's the first term equal to? Well, the sum of Y bar is equal to N times Y bar. But we're dividing by end here. So we just get my bar. And I think we've seen before that this would be 0. So, um, that, that, that finishes this result. And what's the story here? What's the big picture? We still have time. That's good. What is the big picture? I guess it's a, it's a good feature for your predictor to have that it averages out. To the average of the thing that you want to predict. OK. All right. And then last, any questions on this? And now, I think you sort of see that we use the same tricks all the time. So part C, that's another important one for OLS is that this year is 0. So. Prove this. Statement is Oops, that the sum. Oh, the residuals times X is 0. So fancy way of saying this is that the Residuals are orthogonal to the axis, just a fancy way of saying this, OK. But the, the statement is interesting, um, namely that the residuals are in a, in a way our choice, like, But putting a specific line, we are creating the residuals, and they have got this mathematical feature that they The cross product with the regressors happened to be zero. Another way of reading this is if you divide both sides by n. You get 1 over N, some UI had times XI, that captures the flavour of the sample covariance between the residuals and the axis is zero. And again, the Xs are given to us. The residuals are our choice by putting a particular line in the picture. But we use this method called OLS that produces these residuals that have this feature. They happen to be having a zero sample covariance. With the regressors, that's how I read this statement. OK, now, let's establish this. OK, good. What did we do? What did we have? Yeah, that should help us. So where can I have this all in one. Picture. Yeah. Uh, where am I? Here you see a residual and here We've derived this for the residual. So, if I had my iPad, I could just copy this and plug this in here. That's what I'll do. So I'll go. So recall From earlier, UI hat is Y I minus Y bar. Plus, actually, I'm gonna go minus beta one, just to make it look nicer, Xi minus X bar. So. So Recall recall, recall. I'm using this result here, where instead of writing it plus beta 1 ha, X minus XI, I'm just reversing signs. So I get minus beta 1 hat, Xi minus X bar. That's really what I wanted here in the first place, but I didn't write it, but I'll write it up here. So this is a negative sign. So it's exactly the same as this one here. OK. So Recall that now, remember that? So what do we get? Oops, that's a clumsy looking some I go. So big parenthesis Y I minus Y, they're all capital letters minus beta 1 hat. X I minus X bar. And now I go, what do I do? Oh, I've got this XI here. And now I'm clever, I'll do this. And so, I, I don't have to put a bracket here, but I do this anyways. Why was I able to do this? Sorry, this is, sorry, it's supposed to be um. Expand here. Because This is the residual. Times X bar is 0. OK. By, by an earlier result. So this here is the residual. So this is justified. Because the sum of UI hat X bar is 0. Why is that? Because you can put the X bar in front. And here you've got the sum of the UI hats, which is 0. We just proved that. So this is an innocuous subtraction here. I know it's a bit annoying, all of this stuff, but it all works out. Who needs uh Extra help on this one here. This is the justification. Put the XR that's here in front of the sun, and you just have the sum of UI hat, which we've shown in the previous exercise is 0. So this is innocuous. So continuing. Um, I, I break this up into two terms, so we get It is equal to the sum of, so I take this XI here times this Yi here. So we get. XI minus X bar, Yi minus Y bar. So I've taken care of this guy here, multiplying that thing there. And now I still have minus beta 1 hat XI minus X bar, multiplying X X I minus X bar. Minus so it's. Right it like this, beta 1 had sum of XI minus X bar squared, right? That's this thing, these guys here. Now Go I don't have enough space, so I'll just continue down here. I go, I get some of X I minus X bar, Yi minus Y bar. So I'm just writing this here, minus. Beta one hat. We've obtained an explicit result for beta one hat earlier. It is this. From the lecture And then I still get Sum of XI minus X bar squad. So what I've written here. Is the OLS estimator for beta one as we derived it in the lecture. And now the rest Just happens to simplify quite magically. So this guy. Kills that kills that guy. Because this guy and that guy, OK, so we get. Is 0. Um That finishes that proof. So I say here, residuals and regressors are orthogonal, which I just take to mean this here, that the residuals are produced in such a way by OLS that they happen to have this feature. And in another way again of looking at it is that the sample covariance between residuals and axis is zero. OK, because if you divide both sides by N here, upstairs, then the left hand side is the sample covariance. Because the residuals have an average of 0 anyway, so then that is the sample covariance between these two anyway. I'll stop torturing you. I know this isn't too exciting, but we've established a lot of technical facts about OLS today. And I think it turns more conceptual and intuitive in the next few weeks. So we've We've reached the, the bottom. OK. So, anyways, have a good week and I'll see you again next week.

SPEAKER 1
Thank you Yeah, see you. Yeah So Yes. Yeah. Yeah. the. Yes B. So just continuing on and doing that, and you just know that that equals 0. And then put that in there. Oh, was it a girl? No. So what would you do well, does that equal.

SPEAKER 2
have to be summed could be let's say you are person I your height, and this is the average person's height. That's not necessarily 0 unless you are the average person, which I'm not suggesting what makes this 0 I mean this isn't. You have to run it over the sum and it's so. Yeah, this is just comparing a person you talk about people here, a person's x value relative to the average person that doesn't have to be zero, right? And this year is looking at the, well, this we have established that was part of what we did in the beginning so this this go everybody's, yeah, have a good week, everybody's X it's equal to. The average person's X times. I mean it's just the algebraic. I know it might be very intuitive, but the process of summing. Produces this this result that um.

SPEAKER 1
Yeah, you can't just do that. Yeah, that's important and I think it's relatable in that

SPEAKER 2
way that it's some person's realisation of an X variable, and that's just the average person, and there's no point, no reason why it has to be zero. It could be 0, but not generally. But as soon as you run it over the sum, it will algebraically have to be zero, which is like saying you take every person and sum all the Xs, like this was my example of.
